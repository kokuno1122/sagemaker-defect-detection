{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Visual Inspection Automation with Amazon SageMaker This solution detects product defects with an end-to-end Deep Learning workflow for quality control in manufacturing process. The solution takes input of product images and identifies defect regions with bounding boxes. In particular, this solution uses an implementation of An End-to-End Steel Surface Defect Detection on NEU surface defect database (see references ) in PyTorch using PyTorch-lightning . Background According to the Gartner , hyper-automation is the number one trend in 2020 and will continue advancing in future. When it comes to manufacturing, one of the main barriers to hyper-automation is in areas where Human involvements is still struggling to be reduced and intelligent systems have hard times to become on-par with Human visual recognition abilities and become mainstream, despite great advancement of Deep Learning in Computer Vision. This is mainly due to lack of enough annotated data (or when data is sparse) in areas such as Quality Control sections where trained Human eyes still dominates. What is Visual Inspection? The analysis of products on the production line for the purpose of Quality Control . Visual inspection can also be used for internal and external assessment of the various equipment in a production facility such as storage tanks, pressure vessels, piping, and other equipment ( source ) which expands to many industries from Electronics, Medical, Food and Raw Materials. What are the Problems? Some of the major problems are Human Visual Error Human visual inspection error is a major factor in this area. According to this report Most inspection tasks are much more complex and typically exhibit error rates of 20% to 30% (Drury & Fox, 1975) which directly translates to cost . Cost According to some estimate , a trained quality inspector salary varies between 26K (US) - 60K per year. Getting Started You will need an AWS account to use this solution. Sign up for an account here . The easiest is to click on the following button to create the AWS CloudFormation Stack required for this solution AWS Region AWS CloudFormation US West Oregon us-west-2 US East N. Virginia us-east-1 US East Ohio us-east-2 Then acknowledge adding the default AWS IAM policy or use your own policy Click on the Create Stack Once the stack was created, go to the Outputs tab and click on the NotebookInstance link to directly go to the created notebook instance To see the demo, click on 0_demo.ipynb and follow the instructions Checkout 1_finetune.ipynb to finetune / resume training of the provided pretrained checkpoint Checkout 2_detection_from_scratch.ipynb to train a detector from scratch Finally, 3_classification_from_scratch.ipynb notebook, if you just need an accurate image classifier What Does this Solution Offer? This solution offers an implementation of the state-of-the-art Deep Learning approach for automatic Steel Surface Defect Detection using Amazon SageMaker . The model enhances Faster RCNN and output possible defects in an image of surface of a steel. The NEU surface defect database (see references ), is a balanced dataset which contains Six kinds of typical surface defects of the hot-rolled steel strip are collected, i.e., rolled-in scale (RS), patches (Pa), crazing (Cr), pitted surface (PS), inclusion (In) and scratches (Sc). The database includes 1,800 grayscale images: 300 samples each of six different kinds of typical surface defects This solution trains a classifier on NEU-CLS dataset as well as a detector on NEU-DET dataset. Here is a sample images of the six classes NEU sample and here are the sample detection results Contents cloudformation/ defect-detection.yaml : The root cloudformation nested stack which creates the AWS stack for this solution defect-detection-sagemaker-notebook-instance.yaml : Creates SageMaker notebook instance defect-detection-permissions.yaml : Manages all the permission necessary to launch the stack defect-detection-endpoint.yaml : Creates demo endpoint using in 0_demo.ipynb solution-assistant : Deletes the created resources such as endpoint, S3 bucket etc. during cleanup src/ prepare_data/ : Data prepartion for NEU datasets sagemaker_defect_detection/ : Main package dataset : Contains NEU dataset handling models : Contains the MFN model code utils : Various utilities for visualization and coco evaluation classifier.py : For classification task detector.py : For detection task transforms.py : Contains the image transformations used in training notebooks/ : All the notebooks described above scripts/ : Various scripts for training and building Architecture Overview The project Needs access to Amazon S3 for storing data and training artifacts Provides interactive training , evaluation and visualiztions of the results in the provided notebooks using Amazon SageMaker Deplying and testing an HTTPS endpoint Monitoring the deployed model via Amazon CloudWatch Here is the visual architecture Cleaning up When you've finished with this solution, make sure that you delete all unwanted AWS resources. AWS CloudFormation can be used to automatically delete all standard resources that have been created by the solution and notebook. Go to the AWS CloudFormation Console, and delete the parent stack. Choosing to delete the parent stack will automatically delete the nested stacks. Caution: You need to manually delete any extra resources that you may have created in this notebook. Some examples include, extra Amazon S3 buckets (to the solution's default bucket), extra Amazon SageMaker endpoints (using a custom name). Customization For using your own data, make sure it is labeled and is a relatively balanced dataset. Useful Links Amazon SageMaker Getting Started Amazon SageMaker Developer Guide Amazon SageMaker Python SDK Documentation AWS CloudFormation User Guide References K. Song and Y. Yan, \u201cA noise robust method based on completed local binary patterns for hot-rolled steel strip surface defects,\u201d Applied Surface Science, vol. 285, pp. 858-864, Nov. 2013. Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. Hongwen Dong, Kechen Song, Yu He, Jing Xu, Yunhui Yan, Qinggang Meng, \u201cPGA-Net: Pyramid Feature Fusion and Global Context Attention Network for Automated Surface Defect Detection,\u201d IEEE Transactions on Industrial Informatics, 2020. Security See CONTRIBUTING for more information. License This project is licensed under the Apache-2.0 License.","title":"Home"},{"location":"#visual-inspection-automation-with-amazon-sagemaker","text":"This solution detects product defects with an end-to-end Deep Learning workflow for quality control in manufacturing process. The solution takes input of product images and identifies defect regions with bounding boxes. In particular, this solution uses an implementation of An End-to-End Steel Surface Defect Detection on NEU surface defect database (see references ) in PyTorch using PyTorch-lightning .","title":"Visual Inspection Automation with Amazon SageMaker"},{"location":"#background","text":"According to the Gartner , hyper-automation is the number one trend in 2020 and will continue advancing in future. When it comes to manufacturing, one of the main barriers to hyper-automation is in areas where Human involvements is still struggling to be reduced and intelligent systems have hard times to become on-par with Human visual recognition abilities and become mainstream, despite great advancement of Deep Learning in Computer Vision. This is mainly due to lack of enough annotated data (or when data is sparse) in areas such as Quality Control sections where trained Human eyes still dominates.","title":"Background"},{"location":"#what-is-visual-inspection","text":"The analysis of products on the production line for the purpose of Quality Control . Visual inspection can also be used for internal and external assessment of the various equipment in a production facility such as storage tanks, pressure vessels, piping, and other equipment ( source ) which expands to many industries from Electronics, Medical, Food and Raw Materials.","title":"What is Visual Inspection?"},{"location":"#what-are-the-problems","text":"Some of the major problems are","title":"What are the Problems?"},{"location":"#human-visual-error","text":"Human visual inspection error is a major factor in this area. According to this report Most inspection tasks are much more complex and typically exhibit error rates of 20% to 30% (Drury & Fox, 1975) which directly translates to cost .","title":"Human Visual Error"},{"location":"#cost","text":"According to some estimate , a trained quality inspector salary varies between 26K (US) - 60K per year.","title":"Cost"},{"location":"#getting-started","text":"You will need an AWS account to use this solution. Sign up for an account here . The easiest is to click on the following button to create the AWS CloudFormation Stack required for this solution AWS Region AWS CloudFormation US West Oregon us-west-2 US East N. Virginia us-east-1 US East Ohio us-east-2 Then acknowledge adding the default AWS IAM policy or use your own policy Click on the Create Stack Once the stack was created, go to the Outputs tab and click on the NotebookInstance link to directly go to the created notebook instance To see the demo, click on 0_demo.ipynb and follow the instructions Checkout 1_finetune.ipynb to finetune / resume training of the provided pretrained checkpoint Checkout 2_detection_from_scratch.ipynb to train a detector from scratch Finally, 3_classification_from_scratch.ipynb notebook, if you just need an accurate image classifier","title":"Getting Started"},{"location":"#what-does-this-solution-offer","text":"This solution offers an implementation of the state-of-the-art Deep Learning approach for automatic Steel Surface Defect Detection using Amazon SageMaker . The model enhances Faster RCNN and output possible defects in an image of surface of a steel. The NEU surface defect database (see references ), is a balanced dataset which contains Six kinds of typical surface defects of the hot-rolled steel strip are collected, i.e., rolled-in scale (RS), patches (Pa), crazing (Cr), pitted surface (PS), inclusion (In) and scratches (Sc). The database includes 1,800 grayscale images: 300 samples each of six different kinds of typical surface defects This solution trains a classifier on NEU-CLS dataset as well as a detector on NEU-DET dataset. Here is a sample images of the six classes NEU sample and here are the sample detection results","title":"What Does this Solution Offer?"},{"location":"#contents","text":"cloudformation/ defect-detection.yaml : The root cloudformation nested stack which creates the AWS stack for this solution defect-detection-sagemaker-notebook-instance.yaml : Creates SageMaker notebook instance defect-detection-permissions.yaml : Manages all the permission necessary to launch the stack defect-detection-endpoint.yaml : Creates demo endpoint using in 0_demo.ipynb solution-assistant : Deletes the created resources such as endpoint, S3 bucket etc. during cleanup src/ prepare_data/ : Data prepartion for NEU datasets sagemaker_defect_detection/ : Main package dataset : Contains NEU dataset handling models : Contains the MFN model code utils : Various utilities for visualization and coco evaluation classifier.py : For classification task detector.py : For detection task transforms.py : Contains the image transformations used in training notebooks/ : All the notebooks described above scripts/ : Various scripts for training and building","title":"Contents"},{"location":"#architecture-overview","text":"The project Needs access to Amazon S3 for storing data and training artifacts Provides interactive training , evaluation and visualiztions of the results in the provided notebooks using Amazon SageMaker Deplying and testing an HTTPS endpoint Monitoring the deployed model via Amazon CloudWatch Here is the visual architecture","title":"Architecture Overview"},{"location":"#cleaning-up","text":"When you've finished with this solution, make sure that you delete all unwanted AWS resources. AWS CloudFormation can be used to automatically delete all standard resources that have been created by the solution and notebook. Go to the AWS CloudFormation Console, and delete the parent stack. Choosing to delete the parent stack will automatically delete the nested stacks. Caution: You need to manually delete any extra resources that you may have created in this notebook. Some examples include, extra Amazon S3 buckets (to the solution's default bucket), extra Amazon SageMaker endpoints (using a custom name).","title":"Cleaning up"},{"location":"#customization","text":"For using your own data, make sure it is labeled and is a relatively balanced dataset.","title":"Customization"},{"location":"#useful-links","text":"Amazon SageMaker Getting Started Amazon SageMaker Developer Guide Amazon SageMaker Python SDK Documentation AWS CloudFormation User Guide","title":"Useful Links"},{"location":"#references","text":"K. Song and Y. Yan, \u201cA noise robust method based on completed local binary patterns for hot-rolled steel strip surface defects,\u201d Applied Surface Science, vol. 285, pp. 858-864, Nov. 2013. Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. Hongwen Dong, Kechen Song, Yu He, Jing Xu, Yunhui Yan, Qinggang Meng, \u201cPGA-Net: Pyramid Feature Fusion and Global Context Attention Network for Automated Surface Defect Detection,\u201d IEEE Transactions on Industrial Informatics, 2020.","title":"References"},{"location":"#security","text":"See CONTRIBUTING for more information.","title":"Security"},{"location":"#license","text":"This project is licensed under the Apache-2.0 License.","title":"License"},{"location":"CODE_OF_CONDUCT/","text":"Code of Conduct This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.","title":"Code Of Conduct"},{"location":"CODE_OF_CONDUCT/#code-of-conduct","text":"This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.","title":"Code of Conduct"},{"location":"CONTRIBUTING/","text":"Contributing Guidelines Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution. Reporting Bugs/Feature Requests We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment Contributing via Pull Requests Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: You are working against the latest source on the mainline branch. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request . Finding contributions to work on Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start. Code of Conduct This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments. Security issue notifications If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page . Please do not create a public github issue. Licensing See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution. We may ask you to sign a Contributor License Agreement (CLA) for larger changes.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing-guidelines","text":"Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.","title":"Contributing Guidelines"},{"location":"CONTRIBUTING/#reporting-bugsfeature-requests","text":"We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment","title":"Reporting Bugs/Feature Requests"},{"location":"CONTRIBUTING/#contributing-via-pull-requests","text":"Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: You are working against the latest source on the mainline branch. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request .","title":"Contributing via Pull Requests"},{"location":"CONTRIBUTING/#finding-contributions-to-work-on","text":"Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.","title":"Finding contributions to work on"},{"location":"CONTRIBUTING/#code-of-conduct","text":"This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.","title":"Code of Conduct"},{"location":"CONTRIBUTING/#security-issue-notifications","text":"If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page . Please do not create a public github issue.","title":"Security issue notifications"},{"location":"CONTRIBUTING/#licensing","text":"See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution. We may ask you to sign a Contributor License Agreement (CLA) for larger changes.","title":"Licensing"},{"location":"reference/neu/","text":"Module neu Dependencies: unzip unrar python -m pip install patool pyunpack View Source \"\"\" Dependencies: unzip unrar python -m pip install patool pyunpack \"\"\" from pathlib import Path import shutil import re import os try : from pyunpack import Archive except ModuleNotFoundError : print ( \"installing the dependencies `patool` and `pyunpack` for unzipping the data\" ) import subprocess subprocess . run ( \"python -m pip install patool==1.12 pyunpack==0.2.1 -q\" , shell = True ) from pyunpack import Archive CLASSES = { \"crazing\" : \"Cr\" , \"inclusion\" : \"In\" , \"pitted_surface\" : \"PS\" , \"patches\" : \"Pa\" , \"rolled-in_scale\" : \"RS\" , \"scratches\" : \"Sc\" , } def unpack ( path : str ) -> None : path = Path ( path ) Archive ( str ( path )) . extractall ( str ( path . parent )) return def cp_class_images ( data_path : Path , class_name : str , class_path_dest : Path ) -> None : lst = list ( data_path . rglob ( f \"{class_name}_*\" )) for img_file in lst : shutil . copy2 ( str ( img_file ), str ( class_path_dest / img_file . name )) assert len ( lst ) == len ( list ( class_path_dest . glob ( \"*\" ))) return def cp_image_annotation ( data_path : Path , class_name : str , image_path_dest : Path , annotation_path_dest : Path ) -> None : img_lst = sorted ( list (( data_path / \"IMAGES\" ) . rglob ( f \"{class_name}_*\" ))) ann_lst = sorted ( list (( data_path / \"ANNOTATIONS\" ) . rglob ( f \"{class_name}_*\" ))) assert len ( img_lst ) == len ( ann_lst ), f \"images count {len(img_lst)} does not match with annotations count {len(ann_lst)} for class {class_name}\" for ( img_file , ann_file ) in zip ( img_lst , ann_lst ): shutil . copy2 ( str ( img_file ), str ( image_path_dest / img_file . name )) shutil . copy2 ( str ( ann_file ), str ( annotation_path_dest / ann_file . name )) assert len ( list ( image_path_dest . glob ( \"*\" ))) == len ( list ( annotation_path_dest . glob ( \"*\" ))) return def main ( data_path : str , output_path : str , archived : bool = True ) -> None : \"\"\" Data preparation Parameters ---------- data_path : str Raw data path output_path : str Output data path archived: bool Whether the file is archived or not (for testing) Raises ------ ValueError If the packed data file is different from NEU-CLS or NEU-DET \"\"\" data_path = Path ( data_path ) if archived : unpack ( data_path ) data_path = data_path . parent / re . search ( r \"^[^.]*\" , str ( data_path . name )) . group ( 0 ) try : os . remove ( str ( data_path / \"Thumbs.db\" )) except FileNotFoundError : print ( f \"Thumbs.db is not found. Continuing ...\" ) pass except Exception as e : print ( f \"{e}: Unknown error!\" ) raise e output_path = Path ( output_path ) if data_path . name == \"NEU-CLS\" : for cls_ in CLASSES . values (): cls_path = output_path / cls_ cls_path . mkdir ( exist_ok = True ) cp_class_images ( data_path , cls_ , cls_path ) elif data_path . name == \"NEU-DET\" : for cls_ in CLASSES : cls_path = output_path / CLASSES [ cls_ ] image_path = cls_path / \"images\" image_path . mkdir ( parents = True , exist_ok = True ) annotation_path = cls_path / \"annotations\" annotation_path . mkdir ( exist_ok = True ) cp_image_annotation ( data_path , cls_ , image_path , annotation_path ) else : raise ValueError ( f \"Unknown data. Choose between `NEU-CLS` and `NEU-DET`. Given {data_path.name}\" ) return if __name__ == \"__main__\" : import sys if len ( sys . argv ) < 3 : print ( \"Provide `data_path` and `output_path`\" ) sys . exit ( 1 ) main ( sys . argv [ 1 ], sys . argv [ 2 ]) print ( \"Done\" ) Variables CLASSES Functions cp_class_images def cp_class_images ( data_path : pathlib . Path , class_name : str , class_path_dest : pathlib . Path ) -> None View Source def cp_class_images ( data_path : Path , class_name : str , class_path_dest : Path ) -> None : lst = list ( data_path . rglob ( f \"{class_name}_*\" )) for img_file in lst : shutil . copy2 ( str ( img_file ), str ( class_path_dest / img_file . name )) assert len ( lst ) == len ( list ( class_path_dest . glob ( \"*\" ))) return cp_image_annotation def cp_image_annotation ( data_path : pathlib . Path , class_name : str , image_path_dest : pathlib . Path , annotation_path_dest : pathlib . Path ) -> None View Source def cp_image_annotation ( data_path : Path , class_name : str , image_path_dest : Path , annotation_path_dest : Path ) -> None : img_lst = sorted ( list (( data_path / \"IMAGES\" ). rglob ( f \"{class_name}_*\" ))) ann_lst = sorted ( list (( data_path / \"ANNOTATIONS\" ). rglob ( f \"{class_name}_*\" ))) assert len ( img_lst ) == len ( ann_lst ), f \"images count {len(img_lst)} does not match with annotations count {len(ann_lst)} for class {class_name}\" for ( img_file , ann_file ) in zip ( img_lst , ann_lst ): shutil . copy2 ( str ( img_file ), str ( image_path_dest / img_file . name )) shutil . copy2 ( str ( ann_file ), str ( annotation_path_dest / ann_file . name )) assert len ( list ( image_path_dest . glob ( \"*\" ))) == len ( list ( annotation_path_dest . glob ( \"*\" ))) return main def main ( data_path : str , output_path : str , archived : bool = True ) -> None Data preparation Parameters data_path : str Raw data path output_path : str Output data path archived: bool Whether the file is archived or not (for testing) Raises ValueError If the packed data file is different from NEU-CLS or NEU-DET View Source def main ( data_path : str , output_path : str , archived : bool = True ) -> None : \"\"\" Data preparation Parameters ---------- data_path : str Raw data path output_path : str Output data path archived: bool Whether the file is archived or not (for testing) Raises ------ ValueError If the packed data file is different from NEU-CLS or NEU-DET \"\"\" data_path = Path ( data_path ) if archived : unpack ( data_path ) data_path = data_path . parent / re . search ( r \"^[^.]*\" , str ( data_path . name )). group ( 0 ) try : os . remove ( str ( data_path / \"Thumbs.db\" )) except FileNotFoundError : print ( f \"Thumbs.db is not found. Continuing ...\" ) pass except Exception as e : print ( f \"{e}: Unknown error!\" ) raise e output_path = Path ( output_path ) if data_path . name == \"NEU-CLS\" : for cls_ in CLASSES . values () : cls_path = output_path / cls_ cls_path . mkdir ( exist_ok = True ) cp_class_images ( data_path , cls_ , cls_path ) elif data_path . name == \"NEU-DET\" : for cls_ in CLASSES : cls_path = output_path / CLASSES [ cls_ ] image_path = cls_path / \"images\" image_path . mkdir ( parents = True , exist_ok = True ) annotation_path = cls_path / \"annotations\" annotation_path . mkdir ( exist_ok = True ) cp_image_annotation ( data_path , cls_ , image_path , annotation_path ) else : raise ValueError ( f \"Unknown data. Choose between `NEU-CLS` and `NEU-DET`. Given {data_path.name}\" ) return unpack def unpack ( path : str ) -> None View Source def unpack ( path : str ) -> None : path = Path ( path ) Archive ( str ( path )). extractall ( str ( path . parent )) return","title":"Neu"},{"location":"reference/neu/#module-neu","text":"Dependencies: unzip unrar python -m pip install patool pyunpack View Source \"\"\" Dependencies: unzip unrar python -m pip install patool pyunpack \"\"\" from pathlib import Path import shutil import re import os try : from pyunpack import Archive except ModuleNotFoundError : print ( \"installing the dependencies `patool` and `pyunpack` for unzipping the data\" ) import subprocess subprocess . run ( \"python -m pip install patool==1.12 pyunpack==0.2.1 -q\" , shell = True ) from pyunpack import Archive CLASSES = { \"crazing\" : \"Cr\" , \"inclusion\" : \"In\" , \"pitted_surface\" : \"PS\" , \"patches\" : \"Pa\" , \"rolled-in_scale\" : \"RS\" , \"scratches\" : \"Sc\" , } def unpack ( path : str ) -> None : path = Path ( path ) Archive ( str ( path )) . extractall ( str ( path . parent )) return def cp_class_images ( data_path : Path , class_name : str , class_path_dest : Path ) -> None : lst = list ( data_path . rglob ( f \"{class_name}_*\" )) for img_file in lst : shutil . copy2 ( str ( img_file ), str ( class_path_dest / img_file . name )) assert len ( lst ) == len ( list ( class_path_dest . glob ( \"*\" ))) return def cp_image_annotation ( data_path : Path , class_name : str , image_path_dest : Path , annotation_path_dest : Path ) -> None : img_lst = sorted ( list (( data_path / \"IMAGES\" ) . rglob ( f \"{class_name}_*\" ))) ann_lst = sorted ( list (( data_path / \"ANNOTATIONS\" ) . rglob ( f \"{class_name}_*\" ))) assert len ( img_lst ) == len ( ann_lst ), f \"images count {len(img_lst)} does not match with annotations count {len(ann_lst)} for class {class_name}\" for ( img_file , ann_file ) in zip ( img_lst , ann_lst ): shutil . copy2 ( str ( img_file ), str ( image_path_dest / img_file . name )) shutil . copy2 ( str ( ann_file ), str ( annotation_path_dest / ann_file . name )) assert len ( list ( image_path_dest . glob ( \"*\" ))) == len ( list ( annotation_path_dest . glob ( \"*\" ))) return def main ( data_path : str , output_path : str , archived : bool = True ) -> None : \"\"\" Data preparation Parameters ---------- data_path : str Raw data path output_path : str Output data path archived: bool Whether the file is archived or not (for testing) Raises ------ ValueError If the packed data file is different from NEU-CLS or NEU-DET \"\"\" data_path = Path ( data_path ) if archived : unpack ( data_path ) data_path = data_path . parent / re . search ( r \"^[^.]*\" , str ( data_path . name )) . group ( 0 ) try : os . remove ( str ( data_path / \"Thumbs.db\" )) except FileNotFoundError : print ( f \"Thumbs.db is not found. Continuing ...\" ) pass except Exception as e : print ( f \"{e}: Unknown error!\" ) raise e output_path = Path ( output_path ) if data_path . name == \"NEU-CLS\" : for cls_ in CLASSES . values (): cls_path = output_path / cls_ cls_path . mkdir ( exist_ok = True ) cp_class_images ( data_path , cls_ , cls_path ) elif data_path . name == \"NEU-DET\" : for cls_ in CLASSES : cls_path = output_path / CLASSES [ cls_ ] image_path = cls_path / \"images\" image_path . mkdir ( parents = True , exist_ok = True ) annotation_path = cls_path / \"annotations\" annotation_path . mkdir ( exist_ok = True ) cp_image_annotation ( data_path , cls_ , image_path , annotation_path ) else : raise ValueError ( f \"Unknown data. Choose between `NEU-CLS` and `NEU-DET`. Given {data_path.name}\" ) return if __name__ == \"__main__\" : import sys if len ( sys . argv ) < 3 : print ( \"Provide `data_path` and `output_path`\" ) sys . exit ( 1 ) main ( sys . argv [ 1 ], sys . argv [ 2 ]) print ( \"Done\" )","title":"Module neu"},{"location":"reference/neu/#variables","text":"CLASSES","title":"Variables"},{"location":"reference/neu/#functions","text":"","title":"Functions"},{"location":"reference/neu/#cp_class_images","text":"def cp_class_images ( data_path : pathlib . Path , class_name : str , class_path_dest : pathlib . Path ) -> None View Source def cp_class_images ( data_path : Path , class_name : str , class_path_dest : Path ) -> None : lst = list ( data_path . rglob ( f \"{class_name}_*\" )) for img_file in lst : shutil . copy2 ( str ( img_file ), str ( class_path_dest / img_file . name )) assert len ( lst ) == len ( list ( class_path_dest . glob ( \"*\" ))) return","title":"cp_class_images"},{"location":"reference/neu/#cp_image_annotation","text":"def cp_image_annotation ( data_path : pathlib . Path , class_name : str , image_path_dest : pathlib . Path , annotation_path_dest : pathlib . Path ) -> None View Source def cp_image_annotation ( data_path : Path , class_name : str , image_path_dest : Path , annotation_path_dest : Path ) -> None : img_lst = sorted ( list (( data_path / \"IMAGES\" ). rglob ( f \"{class_name}_*\" ))) ann_lst = sorted ( list (( data_path / \"ANNOTATIONS\" ). rglob ( f \"{class_name}_*\" ))) assert len ( img_lst ) == len ( ann_lst ), f \"images count {len(img_lst)} does not match with annotations count {len(ann_lst)} for class {class_name}\" for ( img_file , ann_file ) in zip ( img_lst , ann_lst ): shutil . copy2 ( str ( img_file ), str ( image_path_dest / img_file . name )) shutil . copy2 ( str ( ann_file ), str ( annotation_path_dest / ann_file . name )) assert len ( list ( image_path_dest . glob ( \"*\" ))) == len ( list ( annotation_path_dest . glob ( \"*\" ))) return","title":"cp_image_annotation"},{"location":"reference/neu/#main","text":"def main ( data_path : str , output_path : str , archived : bool = True ) -> None Data preparation","title":"main"},{"location":"reference/neu/#parameters","text":"data_path : str Raw data path output_path : str Output data path archived: bool Whether the file is archived or not (for testing)","title":"Parameters"},{"location":"reference/neu/#raises","text":"ValueError If the packed data file is different from NEU-CLS or NEU-DET View Source def main ( data_path : str , output_path : str , archived : bool = True ) -> None : \"\"\" Data preparation Parameters ---------- data_path : str Raw data path output_path : str Output data path archived: bool Whether the file is archived or not (for testing) Raises ------ ValueError If the packed data file is different from NEU-CLS or NEU-DET \"\"\" data_path = Path ( data_path ) if archived : unpack ( data_path ) data_path = data_path . parent / re . search ( r \"^[^.]*\" , str ( data_path . name )). group ( 0 ) try : os . remove ( str ( data_path / \"Thumbs.db\" )) except FileNotFoundError : print ( f \"Thumbs.db is not found. Continuing ...\" ) pass except Exception as e : print ( f \"{e}: Unknown error!\" ) raise e output_path = Path ( output_path ) if data_path . name == \"NEU-CLS\" : for cls_ in CLASSES . values () : cls_path = output_path / cls_ cls_path . mkdir ( exist_ok = True ) cp_class_images ( data_path , cls_ , cls_path ) elif data_path . name == \"NEU-DET\" : for cls_ in CLASSES : cls_path = output_path / CLASSES [ cls_ ] image_path = cls_path / \"images\" image_path . mkdir ( parents = True , exist_ok = True ) annotation_path = cls_path / \"annotations\" annotation_path . mkdir ( exist_ok = True ) cp_image_annotation ( data_path , cls_ , image_path , annotation_path ) else : raise ValueError ( f \"Unknown data. Choose between `NEU-CLS` and `NEU-DET`. Given {data_path.name}\" ) return","title":"Raises"},{"location":"reference/neu/#unpack","text":"def unpack ( path : str ) -> None View Source def unpack ( path : str ) -> None : path = Path ( path ) Archive ( str ( path )). extractall ( str ( path . parent )) return","title":"unpack"},{"location":"reference/sagemaker_defect_detection/","text":"Module sagemaker_defect_detection View Source try : import pytorch_lightning except ModuleNotFoundError : print ( \"installing the dependencies for sagemaker_defect_detection package ...\" ) import subprocess subprocess . run ( \"python -m pip install -q albumentations==0.4.6 pytorch_lightning==0.8.5 pycocotools==2.0.1\" , shell = True ) from sagemaker_defect_detection.models.ddn import Classification , Detection , RoI , RPN from sagemaker_defect_detection.dataset.neu import NEUCLS , NEUDET from sagemaker_defect_detection.transforms import get_transform , get_augmentation , get_preprocess __all__ = [ \"Classification\" , \"Detection\" , \"RoI\" , \"RPN\" , \"NEUCLS\" , \"NEUDET\" , \"get_transform\" , \"get_augmentation\" , \"get_preprocess\" , ] Sub-modules sagemaker_defect_detection.classifier sagemaker_defect_detection.dataset sagemaker_defect_detection.detector sagemaker_defect_detection.models sagemaker_defect_detection.transforms sagemaker_defect_detection.utils Functions get_augmentation def get_augmentation ( split : str ) -> Callable Obtains proper image augmentation in train split for detection task. We have splitted transformations done for detection task into augmentation and preprocessing for clarity Parameters split : str train or else Returns Callable Image augmentation function View Source def get_augmentation ( split : str ) -> Callable : \"\"\" Obtains proper image augmentation in train split for detection task. We have splitted transformations done for detection task into augmentation and preprocessing for clarity Parameters ---------- split : str train or else Returns ------- Callable Image augmentation function \"\"\" if split == \"train\" : return albu . Compose ( [ albu . Resize ( IMAGE_RESIZE_HEIGHT , IMAGE_RESIZE_WIDTH , always_apply = True ), albu . RandomCrop ( IMAGE_HEIGHT , IMAGE_WIDTH , always_apply = True ), albu . RandomRotate90 ( p = PROBABILITY ), albu . HorizontalFlip ( p = PROBABILITY ), albu . RandomBrightness ( p = PROBABILITY ), ], bbox_params = albu . BboxParams ( format = \"pascal_voc\" , label_fields = [ \"labels\" ], min_visibility = 0 . 2 , ), ) else : return albu . Compose ( [ albu . Resize ( IMAGE_HEIGHT , IMAGE_WIDTH )], bbox_params = albu . BboxParams ( format = \"pascal_voc\" , label_fields = [ \"labels\" ]), ) get_preprocess def get_preprocess ( ) -> Callable Image normalization using albumentation for detection task that aligns well with image augmentation Returns Callable Image normalization function View Source def get_preprocess () -> Callable : \"\"\" Image normalization using albumentation for detection task that aligns well with image augmentation Returns ------- Callable Image normalization function \"\"\" return albu . Compose ( [ albu . Normalize ( mean = [ MEAN_RED , MEAN_GREEN , MEAN_BLUE ], std = [ STD_RED , STD_GREEN , STD_BLUE ]), albu_transforms . ToTensorV2 (), ] ) get_transform def get_transform ( split : str ) -> Callable Image data transformations such as normalization for train split for classification task Parameters split : str train or else Returns Callable Image transformation function View Source def get_transform ( split : str ) -> Callable : \"\"\" Image data transformations such as normalization for train split for classification task Parameters ---------- split : str train or else Returns ------- Callable Image transformation function \"\"\" normalize = transforms . Normalize ( mean = [ MEAN_RED , MEAN_GREEN , MEAN_BLUE ], std = [ STD_RED , STD_GREEN , STD_BLUE ]) if split == \"train\" : return transforms . Compose ( [ transforms . RandomResizedCrop ( IMAGE_HEIGHT ), transforms . RandomRotation ( ROTATION_ANGLE ), transforms . RandomHorizontalFlip (), transforms . ToTensor (), normalize , ] ) else : return transforms . Compose ( [ transforms . Resize ( IMAGE_RESIZE_HEIGHT ), transforms . CenterCrop ( IMAGE_HEIGHT ), transforms . ToTensor (), normalize , ] ) Classes Classification class Classification ( backbone : str , num_classes : int ) Classification network Parameters backbone : str Either resnet34 or resnet50 num_classes : int Number of classes View Source class Classification ( nn . Module ) : \" \"\" Classification network Parameters ---------- backbone : str Either `resnet34` or `resnet50` num_classes : int Number of classes \"\" \" def __init__ ( self , backbone : str , num_classes : int ) -> None : super (). __init__ () self . mfn = MFN ( backbone ) self . flatten = nn . Flatten () self . fc = nn . Linear ( self . mfn . out_channels * 14 ** 2 , num_classes ) def forward ( self , x ) : return self . fc ( self . flatten ( self . mfn ( x ))) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module apply def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf children def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ()) cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device )) double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t ) eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False ) extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t ) forward def forward ( self , x ) View Source def forward ( self , x ): return self . fc ( self . flatten ( self . mfn ( x ))) half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t ) load_state_dict def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys ) modules def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param register_backward_hook def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_parameter def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self share_memory def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ()) state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert ) train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type )) zero_grad def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ () Detection class Detection ( mfn , rpn , roi ) Detection network as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. View Source class Detection ( GeneralizedRCNN ): \"\"\" Detection network as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. \"\"\" def __init__ ( self , mfn , rpn , roi ): dummy_transform = GeneralizedRCNNTransform ( 800 , 1333 , [ 00 .0 , 0.0 , 0.0 ], [ 1.0 , 1.0 , 1.0 ]) super (). __init__ ( mfn , rpn , roi , dummy_transform ) Ancestors (in MRO) torchvision.models.detection.generalized_rcnn.GeneralizedRCNN torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module apply def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf children def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ()) cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device )) double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t ) eager_outputs def eager_outputs ( self , losses , detections ) View Source @torch . jit . unused def eager_outputs ( self , losses , detections ) : # type : ( Dict [ str, Tensor ] , List [ Dict[str, Tensor ] ] ) -> Tuple [ Dict[str, Tensor ] , List [ Dict[str, Tensor ] ]] if self . training : return losses return detections eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False ) extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t ) forward def forward ( self , images , targets = None ) Arguments: images (list[Tensor]): images to be processed targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional) Returns: result (list[BoxList] or dict[Tensor]): the output from the model. During training, it returns a dict[Tensor] which contains the losses. During testing, it returns list[BoxList] contains additional fields like scores , labels and mask (for Mask R-CNN models). View Source def forward ( self , images , targets = None ) : # type : ( List [ Tensor ] , Optional [ List[Dict[str, Tensor ] ]] ) -> Tuple [ Dict[str, Tensor ] , List [ Dict[str, Tensor ] ]] \"\"\" Arguments: images (list[Tensor]): images to be processed targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional) Returns: result (list[BoxList] or dict[Tensor]): the output from the model. During training, it returns a dict[Tensor] which contains the losses. During testing, it returns list[BoxList] contains additional fields like `scores`, `labels` and `mask` (for Mask R-CNN models). \"\"\" if self . training and targets is None : raise ValueError ( \"In training mode, targets should be passed\" ) if self . training : assert targets is not None for target in targets : boxes = target [ \"boxes\" ] if isinstance ( boxes , torch . Tensor ) : if len ( boxes . shape ) != 2 or boxes . shape [ -1 ] != 4 : raise ValueError ( \"Expected target boxes to be a tensor\" \"of shape [N, 4], got {:}.\" . format ( boxes . shape )) else : raise ValueError ( \"Expected target boxes to be of type \" \"Tensor, got {:}.\" . format ( type ( boxes ))) original_image_sizes = torch . jit . annotate ( List [ Tuple[int, int ] ] , [] ) for img in images : val = img . shape [ -2: ] assert len ( val ) == 2 original_image_sizes . append (( val [ 0 ] , val [ 1 ] )) images , targets = self . transform ( images , targets ) # Check for degenerate boxes # TODO : Move this to a function if targets is not None : for target_idx , target in enumerate ( targets ) : boxes = target [ \"boxes\" ] degenerate_boxes = boxes [ :, 2: ] <= boxes [ :, :2 ] if degenerate_boxes . any () : # print the first degenrate box bb_idx = degenerate_boxes . any ( dim = 1 ). nonzero (). view ( - 1 ) [ 0 ] degen_bb : List [ float ] = boxes [ bb_idx ] . tolist () raise ValueError ( \"All bounding boxes should have positive height and width.\" \" Found invaid box {} for target at index {}.\" . format ( degen_bb , target_idx )) features = self . backbone ( images . tensors ) if isinstance ( features , torch . Tensor ) : features = OrderedDict ( [ ('0', features) ] ) proposals , proposal_losses = self . rpn ( images , features , targets ) detections , detector_losses = self . roi_heads ( features , proposals , images . image_sizes , targets ) detections = self . transform . postprocess ( detections , images . image_sizes , original_image_sizes ) losses = {} losses . update ( detector_losses ) losses . update ( proposal_losses ) if torch . jit . is_scripting () : if not self . _has_warned : warnings . warn ( \"RCNN always returns a (Losses, Detections) tuple in scripting\" ) self . _has_warned = True return ( losses , detections ) else : return self . eager_outputs ( losses , detections ) half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t ) load_state_dict def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys ) modules def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param register_backward_hook def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_parameter def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self share_memory def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ()) state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert ) train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type )) zero_grad def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ () NEUCLS class NEUCLS ( root : str , split : str , augmentation : Union [ Callable , NoneType ] = None , preprocessing : Union [ Callable , NoneType ] = None , seed : int = 123 , ** kwargs ) NEU-CLS dataset processing and loading View Source class NEUCLS ( ImageFolder ) : \"\"\" NEU-CLS dataset processing and loading \"\"\" def __init__ ( self , root : str , split : str , augmentation : Optional [ Callable ] = None , preprocessing : Optional [ Callable ] = None , seed : int = 123 , ** kwargs , ) -> None : \"\"\" NEU-CLS dataset Parameters ---------- root : str Dataset root path split : str Data split from train, val and test augmentation : Optional[Callable], optional Image augmentation function, by default None preprocess : Optional[Callable], optional Image preprocessing function, by default None seed : int, optional Random number generator seed, by default 123 Raises ------ ValueError If unsupported split is used \"\"\" super (). __init__ ( root , ** kwargs ) self . samples : List [ Tuple[str, int ] ] self . split = split self . augmentation = augmentation self . preprocessing = preprocessing n_items = len ( self . samples ) np . random . seed ( seed ) perm = np . random . permutation ( list ( range ( n_items ))) # TODO : add split ratios as parameters train_end = int ( 0.6 * n_items ) val_end = int ( 0.2 * n_items ) + train_end if split == \"train\" : self . samples = [ self.samples[i ] for i in perm [ :train_end ] ] elif split == \"val\" : self . samples = [ self.samples[i ] for i in perm [ train_end:val_end ] ] elif split == \"test\" : self . samples = [ self.samples[i ] for i in perm [ val_end: ] ] else : raise ValueError ( f \"Unknown split mode. Choose from `train`, `val` or `test`. Given {split}\" ) Ancestors (in MRO) torchvision.datasets.folder.ImageFolder torchvision.datasets.folder.DatasetFolder torchvision.datasets.vision.VisionDataset torch.utils.data.dataset.Dataset Methods extra_repr def extra_repr ( self ) View Source def extra_repr ( self ): return \"\" NEUDET class NEUDET ( root : str , split : str , augmentation : Union [ Callable , NoneType ] = None , preprocess : Union [ Callable , NoneType ] = None , seed : int = 123 ) NEU-DET dataset processing and loading View Source class NEUDET ( Dataset ): \"\"\" NEU-DET dataset processing and loading \"\"\" def __init__ ( self , root : str , split : str , augmentation : Optional [ Callable ] = None , preprocess : Optional [ Callable ] = None , seed : int = 123 , ): \"\"\" NEU-DET dataset Parameters ---------- root : str Dataset root path split : str Data split from train, val and test augmentation : Optional[Callable], optional Image augmentation function, by default None preprocess : Optional[Callable], optional Image preprocessing function, by default None seed : int, optional Random number generator seed, by default 123 Raises ------ ValueError If unsupported split is used \"\"\" super () . __init__ () self . root = Path ( root ) self . split = split self . classes , self . class_to_idx = self . _find_classes () self . samples : List [ DetectionSample ] = self . _make_dataset () self . augmentation = augmentation self . preprocess = preprocess n_items = len ( self . samples ) np . random . seed ( seed ) perm = np . random . permutation ( list ( range ( n_items ))) train_end = int ( 0.6 * n_items ) val_end = int ( 0.2 * n_items ) + train_end if split == \"train\" : self . samples = [ self . samples [ i ] for i in perm [: train_end ]] elif split == \"val\" : self . samples = [ self . samples [ i ] for i in perm [ train_end : val_end ]] elif split == \"test\" : self . samples = [ self . samples [ i ] for i in perm [ val_end :]] else : raise ValueError ( f \"Unknown split mode. Choose from `train`, `val` or `test`. Given {split}\" ) def _make_dataset ( self ) -> List [ DetectionSample ]: instances = [] base_dir = self . root . expanduser () for target_cls in sorted ( self . class_to_idx . keys ()): cls_idx = self . class_to_idx [ target_cls ] target_dir = base_dir / target_cls if not target_dir . is_dir (): continue images = sorted ( list (( target_dir / \"images\" ) . glob ( \"*.jpg\" ))) annotations = sorted ( list (( target_dir / \"annotations\" ) . glob ( \"*.xml\" ))) assert len ( images ) == len ( annotations ), f \"something is wrong. Mismatched number of images and annotations\" for path , ann in zip ( images , annotations ): instances . append ( DetectionSample ( str ( path ), int ( cls_idx ), str ( ann ))) return instances def _find_classes ( self ): classes = sorted ([ d . name for d in os . scandir ( str ( self . root )) if d . is_dir ()]) class_to_idx = { cls_name : i for i , cls_name in enumerate ( classes , 1 )} # no bg label in NEU return classes , class_to_idx @ staticmethod def _get_bboxes ( ann : str ) -> List [ List [ int ]]: tree = ElementTree () . parse ( ann ) bboxes = [] for bndbox in tree . iterfind ( \"object/bndbox\" ): # should subtract 1 like coco? bbox = [ int ( bndbox . findtext ( t )) - 1 for t in ( \"xmin\" , \"ymin\" , \"xmax\" , \"ymax\" )] # type: ignore assert bbox [ 2 ] > bbox [ 0 ] and bbox [ 3 ] > bbox [ 1 ], f \"box size error, given {bbox}\" bboxes . append ( bbox ) return bboxes def __len__ ( self ): return len ( self . samples ) def __getitem__ ( self , idx : int ): # Note: images are grayscaled BUT resnet needs 3 channels image = cv2 . imread ( self . samples [ idx ] . image_path ) # BGR channel last image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) boxes = self . _get_bboxes ( self . samples [ idx ] . annotations ) num_objs = len ( boxes ) boxes = torch . as_tensor ( boxes , dtype = torch . float32 ) labels = torch . tensor ([ self . samples [ idx ] . class_idx ] * num_objs , dtype = torch . int64 ) image_id = torch . tensor ([ idx ], dtype = torch . int64 ) iscrowd = torch . zeros (( len ( boxes ),), dtype = torch . int64 ) target = {} target [ \"boxes\" ] = boxes target [ \"labels\" ] = labels target [ \"image_id\" ] = image_id target [ \"iscrowd\" ] = iscrowd if self . augmentation is not None : sample = self . augmentation ( ** { \"image\" : image , \"bboxes\" : boxes , \"labels\" : labels }) image = sample [ \"image\" ] target [ \"boxes\" ] = torch . as_tensor ( sample [ \"bboxes\" ], dtype = torch . float32 ) # guards against crops that don't pass the min_visibility augmentation threshold if not target [ \"boxes\" ] . numel (): return None target [ \"labels\" ] = torch . as_tensor ( sample [ \"labels\" ], dtype = torch . int64 ) if self . preprocess is not None : image = self . preprocess ( image = image )[ \"image\" ] boxes = target [ \"boxes\" ] target [ \"area\" ] = ( boxes [:, 3 ] - boxes [:, 1 ]) * ( boxes [:, 2 ] - boxes [:, 0 ]) return image , target , image_id def collate_fn ( self , batch ): batch = filter ( lambda x : x is not None , batch ) return tuple ( zip ( * batch )) Ancestors (in MRO) torch.utils.data.dataset.Dataset Methods collate_fn def collate_fn ( self , batch ) View Source def collate_fn ( self , batch ): batch = filter ( lambda x : x is not None , batch ) return tuple ( zip ( * batch )) RPN class RPN ( out_channels : int = 512 , rpn_pre_nms_top_n_train : int = 1000 , rpn_pre_nms_top_n_test : int = 500 , rpn_post_nms_top_n_train : int = 1000 , rpn_post_nms_top_n_test : int = 500 , rpn_nms_thresh : float = 0.7 , rpn_fg_iou_thresh : float = 0.7 , rpn_bg_iou_thresh : float = 0.3 , rpn_batch_size_per_image : int = 256 , rpn_positive_fraction : float = 0.5 ) RPN Module as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. View Source class RPN ( nn . Module ): \"\"\" RPN Module as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. \"\"\" def __init__ ( self , out_channels: int = 512 , rpn_pre_nms_top_n_train: int = 1000 , # torchvision default 2000, rpn_pre_nms_top_n_test: int = 500 , # torchvision default 1000, rpn_post_nms_top_n_train: int = 1000 , # torchvision default 2000, rpn_post_nms_top_n_test: int = 500 , # torchvision default 1000, rpn_nms_thresh: float = 0.7 , rpn_fg_iou_thresh: float = 0.7 , rpn_bg_iou_thresh: float = 0.3 , rpn_batch_size_per_image: int = 256 , rpn_positive_fraction: float = 0.5 , ) -> None: super (). __init__ () rpn_anchor_generator = AnchorGenerator ( sizes =(( 64 , 128 , 256 , 512 ),), aspect_ratios =(( 0.5 , 1.0 , 2.0 ),)) rpn_head = RPNHead ( out_channels , rpn_anchor_generator . num_anchors_per_location ()[ 0 ]) rpn_pre_nms_top_n = dict ( training = rpn_pre_nms_top_n_train , testing = rpn_pre_nms_top_n_test ) rpn_post_nms_top_n = dict ( training = rpn_post_nms_top_n_train , testing = rpn_post_nms_top_n_test ) self . rpn = RegionProposalNetwork ( rpn_anchor_generator , rpn_head , rpn_fg_iou_thresh , rpn_bg_iou_thresh , rpn_batch_size_per_image , rpn_positive_fraction , rpn_pre_nms_top_n , rpn_post_nms_top_n , rpn_nms_thresh , ) def forward ( self , * args , ** kwargs ): return self . rpn (* args , ** kwargs ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module apply def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf children def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ()) cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device )) double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t ) eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False ) extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t ) forward def forward ( self , * args , ** kwargs ) View Source def forward ( self , * args , ** kwargs ): return self . rpn ( * args , ** kwargs ) half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t ) load_state_dict def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys ) modules def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param register_backward_hook def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_parameter def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self share_memory def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ()) state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert ) train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type )) zero_grad def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ () RoI class RoI ( num_classes : int , box_fg_iou_thresh = 0.5 , box_bg_iou_thresh = 0.5 , box_batch_size_per_image = 512 , box_positive_fraction = 0.25 , bbox_reg_weights = None , box_score_thresh = 0.05 , box_nms_thresh = 0.5 , box_detections_per_img = 100 ) ROI Module as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. View Source class RoI ( nn . Module ) : \"\"\" ROI Module as described in Yu He , Kechen Song , Qinggang Meng , Yunhui Yan , \u201c An End - to - end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features , \u201d IEEE Transactions on Instrumentation and Measuremente , 2020 , 69 ( 4 ), 1493 - 1504. \"\"\" def __init__ ( self , num_classes: int , box_fg_iou_thresh = 0.5 , box_bg_iou_thresh = 0.5 , box_batch_size_per_image = 512 , box_positive_fraction = 0.25 , bbox_reg_weights = None , box_score_thresh = 0.05 , box_nms_thresh = 0.5 , box_detections_per_img = 100 , ) -> None: super (). __init__ () roi_pooler = MultiScaleRoIAlign ( featmap_names = [ \"0\" ], output_size = 7 , sampling_ratio = 2 ) box_head = CustomTwoMLPHead ( 512 * 7 ** 2 , 1024 ) box_predictor = FastRCNNPredictor ( 1024 , num_classes = num_classes ) self . roi_head = RoIHeads ( roi_pooler , box_head , box_predictor , box_fg_iou_thresh , box_bg_iou_thresh , box_batch_size_per_image , box_positive_fraction , bbox_reg_weights , box_score_thresh , box_nms_thresh , box_detections_per_img , ) def forward ( self , * args , ** kwargs ) : return self . roi_head ( * args , ** kwargs ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module apply def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf children def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ()) cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device )) double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t ) eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False ) extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t ) forward def forward ( self , * args , ** kwargs ) View Source def forward ( self , * args , ** kwargs ): return self . roi_head ( * args , ** kwargs ) half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t ) load_state_dict def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys ) modules def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param register_backward_hook def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_parameter def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self share_memory def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ()) state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert ) train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type )) zero_grad def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ ()","title":"Index"},{"location":"reference/sagemaker_defect_detection/#module-sagemaker_defect_detection","text":"View Source try : import pytorch_lightning except ModuleNotFoundError : print ( \"installing the dependencies for sagemaker_defect_detection package ...\" ) import subprocess subprocess . run ( \"python -m pip install -q albumentations==0.4.6 pytorch_lightning==0.8.5 pycocotools==2.0.1\" , shell = True ) from sagemaker_defect_detection.models.ddn import Classification , Detection , RoI , RPN from sagemaker_defect_detection.dataset.neu import NEUCLS , NEUDET from sagemaker_defect_detection.transforms import get_transform , get_augmentation , get_preprocess __all__ = [ \"Classification\" , \"Detection\" , \"RoI\" , \"RPN\" , \"NEUCLS\" , \"NEUDET\" , \"get_transform\" , \"get_augmentation\" , \"get_preprocess\" , ]","title":"Module sagemaker_defect_detection"},{"location":"reference/sagemaker_defect_detection/#sub-modules","text":"sagemaker_defect_detection.classifier sagemaker_defect_detection.dataset sagemaker_defect_detection.detector sagemaker_defect_detection.models sagemaker_defect_detection.transforms sagemaker_defect_detection.utils","title":"Sub-modules"},{"location":"reference/sagemaker_defect_detection/#functions","text":"","title":"Functions"},{"location":"reference/sagemaker_defect_detection/#get_augmentation","text":"def get_augmentation ( split : str ) -> Callable Obtains proper image augmentation in train split for detection task. We have splitted transformations done for detection task into augmentation and preprocessing for clarity","title":"get_augmentation"},{"location":"reference/sagemaker_defect_detection/#parameters","text":"split : str train or else","title":"Parameters"},{"location":"reference/sagemaker_defect_detection/#returns","text":"Callable Image augmentation function View Source def get_augmentation ( split : str ) -> Callable : \"\"\" Obtains proper image augmentation in train split for detection task. We have splitted transformations done for detection task into augmentation and preprocessing for clarity Parameters ---------- split : str train or else Returns ------- Callable Image augmentation function \"\"\" if split == \"train\" : return albu . Compose ( [ albu . Resize ( IMAGE_RESIZE_HEIGHT , IMAGE_RESIZE_WIDTH , always_apply = True ), albu . RandomCrop ( IMAGE_HEIGHT , IMAGE_WIDTH , always_apply = True ), albu . RandomRotate90 ( p = PROBABILITY ), albu . HorizontalFlip ( p = PROBABILITY ), albu . RandomBrightness ( p = PROBABILITY ), ], bbox_params = albu . BboxParams ( format = \"pascal_voc\" , label_fields = [ \"labels\" ], min_visibility = 0 . 2 , ), ) else : return albu . Compose ( [ albu . Resize ( IMAGE_HEIGHT , IMAGE_WIDTH )], bbox_params = albu . BboxParams ( format = \"pascal_voc\" , label_fields = [ \"labels\" ]), )","title":"Returns"},{"location":"reference/sagemaker_defect_detection/#get_preprocess","text":"def get_preprocess ( ) -> Callable Image normalization using albumentation for detection task that aligns well with image augmentation","title":"get_preprocess"},{"location":"reference/sagemaker_defect_detection/#returns_1","text":"Callable Image normalization function View Source def get_preprocess () -> Callable : \"\"\" Image normalization using albumentation for detection task that aligns well with image augmentation Returns ------- Callable Image normalization function \"\"\" return albu . Compose ( [ albu . Normalize ( mean = [ MEAN_RED , MEAN_GREEN , MEAN_BLUE ], std = [ STD_RED , STD_GREEN , STD_BLUE ]), albu_transforms . ToTensorV2 (), ] )","title":"Returns"},{"location":"reference/sagemaker_defect_detection/#get_transform","text":"def get_transform ( split : str ) -> Callable Image data transformations such as normalization for train split for classification task","title":"get_transform"},{"location":"reference/sagemaker_defect_detection/#parameters_1","text":"split : str train or else","title":"Parameters"},{"location":"reference/sagemaker_defect_detection/#returns_2","text":"Callable Image transformation function View Source def get_transform ( split : str ) -> Callable : \"\"\" Image data transformations such as normalization for train split for classification task Parameters ---------- split : str train or else Returns ------- Callable Image transformation function \"\"\" normalize = transforms . Normalize ( mean = [ MEAN_RED , MEAN_GREEN , MEAN_BLUE ], std = [ STD_RED , STD_GREEN , STD_BLUE ]) if split == \"train\" : return transforms . Compose ( [ transforms . RandomResizedCrop ( IMAGE_HEIGHT ), transforms . RandomRotation ( ROTATION_ANGLE ), transforms . RandomHorizontalFlip (), transforms . ToTensor (), normalize , ] ) else : return transforms . Compose ( [ transforms . Resize ( IMAGE_RESIZE_HEIGHT ), transforms . CenterCrop ( IMAGE_HEIGHT ), transforms . ToTensor (), normalize , ] )","title":"Returns"},{"location":"reference/sagemaker_defect_detection/#classes","text":"","title":"Classes"},{"location":"reference/sagemaker_defect_detection/#classification","text":"class Classification ( backbone : str , num_classes : int ) Classification network","title":"Classification"},{"location":"reference/sagemaker_defect_detection/#parameters_2","text":"backbone : str Either resnet34 or resnet50 num_classes : int Number of classes View Source class Classification ( nn . Module ) : \" \"\" Classification network Parameters ---------- backbone : str Either `resnet34` or `resnet50` num_classes : int Number of classes \"\" \" def __init__ ( self , backbone : str , num_classes : int ) -> None : super (). __init__ () self . mfn = MFN ( backbone ) self . flatten = nn . Flatten () self . fc = nn . Linear ( self . mfn . out_channels * 14 ** 2 , num_classes ) def forward ( self , x ) : return self . fc ( self . flatten ( self . mfn ( x )))","title":"Parameters"},{"location":"reference/sagemaker_defect_detection/#ancestors-in-mro","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/sagemaker_defect_detection/#class-variables","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/sagemaker_defect_detection/#methods","text":"","title":"Methods"},{"location":"reference/sagemaker_defect_detection/#add_module","text":"def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module","title":"add_module"},{"location":"reference/sagemaker_defect_detection/#apply","text":"def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self","title":"apply"},{"location":"reference/sagemaker_defect_detection/#bfloat16","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t )","title":"bfloat16"},{"location":"reference/sagemaker_defect_detection/#buffers","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf","title":"buffers"},{"location":"reference/sagemaker_defect_detection/#children","text":"def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module","title":"children"},{"location":"reference/sagemaker_defect_detection/#cpu","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ())","title":"cpu"},{"location":"reference/sagemaker_defect_detection/#cuda","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device ))","title":"cuda"},{"location":"reference/sagemaker_defect_detection/#double","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t )","title":"double"},{"location":"reference/sagemaker_defect_detection/#eval","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False )","title":"eval"},{"location":"reference/sagemaker_defect_detection/#extra_repr","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return ''","title":"extra_repr"},{"location":"reference/sagemaker_defect_detection/#float","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t )","title":"float"},{"location":"reference/sagemaker_defect_detection/#forward","text":"def forward ( self , x ) View Source def forward ( self , x ): return self . fc ( self . flatten ( self . mfn ( x )))","title":"forward"},{"location":"reference/sagemaker_defect_detection/#half","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t )","title":"half"},{"location":"reference/sagemaker_defect_detection/#load_state_dict","text":"def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys )","title":"load_state_dict"},{"location":"reference/sagemaker_defect_detection/#modules","text":"def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module","title":"modules"},{"location":"reference/sagemaker_defect_detection/#named_buffers","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_buffers"},{"location":"reference/sagemaker_defect_detection/#named_children","text":"def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module","title":"named_children"},{"location":"reference/sagemaker_defect_detection/#named_modules","text":"def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m","title":"named_modules"},{"location":"reference/sagemaker_defect_detection/#named_parameters","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_parameters"},{"location":"reference/sagemaker_defect_detection/#parameters_3","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param","title":"parameters"},{"location":"reference/sagemaker_defect_detection/#register_backward_hook","text":"def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle","title":"register_backward_hook"},{"location":"reference/sagemaker_defect_detection/#register_buffer","text":"def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name )","title":"register_buffer"},{"location":"reference/sagemaker_defect_detection/#register_forward_hook","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle","title":"register_forward_hook"},{"location":"reference/sagemaker_defect_detection/#register_forward_pre_hook","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle","title":"register_forward_pre_hook"},{"location":"reference/sagemaker_defect_detection/#register_parameter","text":"def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param","title":"register_parameter"},{"location":"reference/sagemaker_defect_detection/#requires_grad_","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self","title":"requires_grad_"},{"location":"reference/sagemaker_defect_detection/#share_memory","text":"def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ())","title":"share_memory"},{"location":"reference/sagemaker_defect_detection/#state_dict","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination","title":"state_dict"},{"location":"reference/sagemaker_defect_detection/#to","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert )","title":"to"},{"location":"reference/sagemaker_defect_detection/#train","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self","title":"train"},{"location":"reference/sagemaker_defect_detection/#type","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type ))","title":"type"},{"location":"reference/sagemaker_defect_detection/#zero_grad","text":"def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ ()","title":"zero_grad"},{"location":"reference/sagemaker_defect_detection/#detection","text":"class Detection ( mfn , rpn , roi ) Detection network as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. View Source class Detection ( GeneralizedRCNN ): \"\"\" Detection network as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. \"\"\" def __init__ ( self , mfn , rpn , roi ): dummy_transform = GeneralizedRCNNTransform ( 800 , 1333 , [ 00 .0 , 0.0 , 0.0 ], [ 1.0 , 1.0 , 1.0 ]) super (). __init__ ( mfn , rpn , roi , dummy_transform )","title":"Detection"},{"location":"reference/sagemaker_defect_detection/#ancestors-in-mro_1","text":"torchvision.models.detection.generalized_rcnn.GeneralizedRCNN torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/sagemaker_defect_detection/#class-variables_1","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/sagemaker_defect_detection/#methods_1","text":"","title":"Methods"},{"location":"reference/sagemaker_defect_detection/#add_module_1","text":"def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module","title":"add_module"},{"location":"reference/sagemaker_defect_detection/#apply_1","text":"def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self","title":"apply"},{"location":"reference/sagemaker_defect_detection/#bfloat16_1","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t )","title":"bfloat16"},{"location":"reference/sagemaker_defect_detection/#buffers_1","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf","title":"buffers"},{"location":"reference/sagemaker_defect_detection/#children_1","text":"def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module","title":"children"},{"location":"reference/sagemaker_defect_detection/#cpu_1","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ())","title":"cpu"},{"location":"reference/sagemaker_defect_detection/#cuda_1","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device ))","title":"cuda"},{"location":"reference/sagemaker_defect_detection/#double_1","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t )","title":"double"},{"location":"reference/sagemaker_defect_detection/#eager_outputs","text":"def eager_outputs ( self , losses , detections ) View Source @torch . jit . unused def eager_outputs ( self , losses , detections ) : # type : ( Dict [ str, Tensor ] , List [ Dict[str, Tensor ] ] ) -> Tuple [ Dict[str, Tensor ] , List [ Dict[str, Tensor ] ]] if self . training : return losses return detections","title":"eager_outputs"},{"location":"reference/sagemaker_defect_detection/#eval_1","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False )","title":"eval"},{"location":"reference/sagemaker_defect_detection/#extra_repr_1","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return ''","title":"extra_repr"},{"location":"reference/sagemaker_defect_detection/#float_1","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t )","title":"float"},{"location":"reference/sagemaker_defect_detection/#forward_1","text":"def forward ( self , images , targets = None ) Arguments: images (list[Tensor]): images to be processed targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional) Returns: result (list[BoxList] or dict[Tensor]): the output from the model. During training, it returns a dict[Tensor] which contains the losses. During testing, it returns list[BoxList] contains additional fields like scores , labels and mask (for Mask R-CNN models). View Source def forward ( self , images , targets = None ) : # type : ( List [ Tensor ] , Optional [ List[Dict[str, Tensor ] ]] ) -> Tuple [ Dict[str, Tensor ] , List [ Dict[str, Tensor ] ]] \"\"\" Arguments: images (list[Tensor]): images to be processed targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional) Returns: result (list[BoxList] or dict[Tensor]): the output from the model. During training, it returns a dict[Tensor] which contains the losses. During testing, it returns list[BoxList] contains additional fields like `scores`, `labels` and `mask` (for Mask R-CNN models). \"\"\" if self . training and targets is None : raise ValueError ( \"In training mode, targets should be passed\" ) if self . training : assert targets is not None for target in targets : boxes = target [ \"boxes\" ] if isinstance ( boxes , torch . Tensor ) : if len ( boxes . shape ) != 2 or boxes . shape [ -1 ] != 4 : raise ValueError ( \"Expected target boxes to be a tensor\" \"of shape [N, 4], got {:}.\" . format ( boxes . shape )) else : raise ValueError ( \"Expected target boxes to be of type \" \"Tensor, got {:}.\" . format ( type ( boxes ))) original_image_sizes = torch . jit . annotate ( List [ Tuple[int, int ] ] , [] ) for img in images : val = img . shape [ -2: ] assert len ( val ) == 2 original_image_sizes . append (( val [ 0 ] , val [ 1 ] )) images , targets = self . transform ( images , targets ) # Check for degenerate boxes # TODO : Move this to a function if targets is not None : for target_idx , target in enumerate ( targets ) : boxes = target [ \"boxes\" ] degenerate_boxes = boxes [ :, 2: ] <= boxes [ :, :2 ] if degenerate_boxes . any () : # print the first degenrate box bb_idx = degenerate_boxes . any ( dim = 1 ). nonzero (). view ( - 1 ) [ 0 ] degen_bb : List [ float ] = boxes [ bb_idx ] . tolist () raise ValueError ( \"All bounding boxes should have positive height and width.\" \" Found invaid box {} for target at index {}.\" . format ( degen_bb , target_idx )) features = self . backbone ( images . tensors ) if isinstance ( features , torch . Tensor ) : features = OrderedDict ( [ ('0', features) ] ) proposals , proposal_losses = self . rpn ( images , features , targets ) detections , detector_losses = self . roi_heads ( features , proposals , images . image_sizes , targets ) detections = self . transform . postprocess ( detections , images . image_sizes , original_image_sizes ) losses = {} losses . update ( detector_losses ) losses . update ( proposal_losses ) if torch . jit . is_scripting () : if not self . _has_warned : warnings . warn ( \"RCNN always returns a (Losses, Detections) tuple in scripting\" ) self . _has_warned = True return ( losses , detections ) else : return self . eager_outputs ( losses , detections )","title":"forward"},{"location":"reference/sagemaker_defect_detection/#half_1","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t )","title":"half"},{"location":"reference/sagemaker_defect_detection/#load_state_dict_1","text":"def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys )","title":"load_state_dict"},{"location":"reference/sagemaker_defect_detection/#modules_1","text":"def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module","title":"modules"},{"location":"reference/sagemaker_defect_detection/#named_buffers_1","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_buffers"},{"location":"reference/sagemaker_defect_detection/#named_children_1","text":"def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module","title":"named_children"},{"location":"reference/sagemaker_defect_detection/#named_modules_1","text":"def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m","title":"named_modules"},{"location":"reference/sagemaker_defect_detection/#named_parameters_1","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_parameters"},{"location":"reference/sagemaker_defect_detection/#parameters_4","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param","title":"parameters"},{"location":"reference/sagemaker_defect_detection/#register_backward_hook_1","text":"def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle","title":"register_backward_hook"},{"location":"reference/sagemaker_defect_detection/#register_buffer_1","text":"def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name )","title":"register_buffer"},{"location":"reference/sagemaker_defect_detection/#register_forward_hook_1","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle","title":"register_forward_hook"},{"location":"reference/sagemaker_defect_detection/#register_forward_pre_hook_1","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle","title":"register_forward_pre_hook"},{"location":"reference/sagemaker_defect_detection/#register_parameter_1","text":"def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param","title":"register_parameter"},{"location":"reference/sagemaker_defect_detection/#requires_grad__1","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self","title":"requires_grad_"},{"location":"reference/sagemaker_defect_detection/#share_memory_1","text":"def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ())","title":"share_memory"},{"location":"reference/sagemaker_defect_detection/#state_dict_1","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination","title":"state_dict"},{"location":"reference/sagemaker_defect_detection/#to_1","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert )","title":"to"},{"location":"reference/sagemaker_defect_detection/#train_1","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self","title":"train"},{"location":"reference/sagemaker_defect_detection/#type_1","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type ))","title":"type"},{"location":"reference/sagemaker_defect_detection/#zero_grad_1","text":"def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ ()","title":"zero_grad"},{"location":"reference/sagemaker_defect_detection/#neucls","text":"class NEUCLS ( root : str , split : str , augmentation : Union [ Callable , NoneType ] = None , preprocessing : Union [ Callable , NoneType ] = None , seed : int = 123 , ** kwargs ) NEU-CLS dataset processing and loading View Source class NEUCLS ( ImageFolder ) : \"\"\" NEU-CLS dataset processing and loading \"\"\" def __init__ ( self , root : str , split : str , augmentation : Optional [ Callable ] = None , preprocessing : Optional [ Callable ] = None , seed : int = 123 , ** kwargs , ) -> None : \"\"\" NEU-CLS dataset Parameters ---------- root : str Dataset root path split : str Data split from train, val and test augmentation : Optional[Callable], optional Image augmentation function, by default None preprocess : Optional[Callable], optional Image preprocessing function, by default None seed : int, optional Random number generator seed, by default 123 Raises ------ ValueError If unsupported split is used \"\"\" super (). __init__ ( root , ** kwargs ) self . samples : List [ Tuple[str, int ] ] self . split = split self . augmentation = augmentation self . preprocessing = preprocessing n_items = len ( self . samples ) np . random . seed ( seed ) perm = np . random . permutation ( list ( range ( n_items ))) # TODO : add split ratios as parameters train_end = int ( 0.6 * n_items ) val_end = int ( 0.2 * n_items ) + train_end if split == \"train\" : self . samples = [ self.samples[i ] for i in perm [ :train_end ] ] elif split == \"val\" : self . samples = [ self.samples[i ] for i in perm [ train_end:val_end ] ] elif split == \"test\" : self . samples = [ self.samples[i ] for i in perm [ val_end: ] ] else : raise ValueError ( f \"Unknown split mode. Choose from `train`, `val` or `test`. Given {split}\" )","title":"NEUCLS"},{"location":"reference/sagemaker_defect_detection/#ancestors-in-mro_2","text":"torchvision.datasets.folder.ImageFolder torchvision.datasets.folder.DatasetFolder torchvision.datasets.vision.VisionDataset torch.utils.data.dataset.Dataset","title":"Ancestors (in MRO)"},{"location":"reference/sagemaker_defect_detection/#methods_2","text":"","title":"Methods"},{"location":"reference/sagemaker_defect_detection/#extra_repr_2","text":"def extra_repr ( self ) View Source def extra_repr ( self ): return \"\"","title":"extra_repr"},{"location":"reference/sagemaker_defect_detection/#neudet","text":"class NEUDET ( root : str , split : str , augmentation : Union [ Callable , NoneType ] = None , preprocess : Union [ Callable , NoneType ] = None , seed : int = 123 ) NEU-DET dataset processing and loading View Source class NEUDET ( Dataset ): \"\"\" NEU-DET dataset processing and loading \"\"\" def __init__ ( self , root : str , split : str , augmentation : Optional [ Callable ] = None , preprocess : Optional [ Callable ] = None , seed : int = 123 , ): \"\"\" NEU-DET dataset Parameters ---------- root : str Dataset root path split : str Data split from train, val and test augmentation : Optional[Callable], optional Image augmentation function, by default None preprocess : Optional[Callable], optional Image preprocessing function, by default None seed : int, optional Random number generator seed, by default 123 Raises ------ ValueError If unsupported split is used \"\"\" super () . __init__ () self . root = Path ( root ) self . split = split self . classes , self . class_to_idx = self . _find_classes () self . samples : List [ DetectionSample ] = self . _make_dataset () self . augmentation = augmentation self . preprocess = preprocess n_items = len ( self . samples ) np . random . seed ( seed ) perm = np . random . permutation ( list ( range ( n_items ))) train_end = int ( 0.6 * n_items ) val_end = int ( 0.2 * n_items ) + train_end if split == \"train\" : self . samples = [ self . samples [ i ] for i in perm [: train_end ]] elif split == \"val\" : self . samples = [ self . samples [ i ] for i in perm [ train_end : val_end ]] elif split == \"test\" : self . samples = [ self . samples [ i ] for i in perm [ val_end :]] else : raise ValueError ( f \"Unknown split mode. Choose from `train`, `val` or `test`. Given {split}\" ) def _make_dataset ( self ) -> List [ DetectionSample ]: instances = [] base_dir = self . root . expanduser () for target_cls in sorted ( self . class_to_idx . keys ()): cls_idx = self . class_to_idx [ target_cls ] target_dir = base_dir / target_cls if not target_dir . is_dir (): continue images = sorted ( list (( target_dir / \"images\" ) . glob ( \"*.jpg\" ))) annotations = sorted ( list (( target_dir / \"annotations\" ) . glob ( \"*.xml\" ))) assert len ( images ) == len ( annotations ), f \"something is wrong. Mismatched number of images and annotations\" for path , ann in zip ( images , annotations ): instances . append ( DetectionSample ( str ( path ), int ( cls_idx ), str ( ann ))) return instances def _find_classes ( self ): classes = sorted ([ d . name for d in os . scandir ( str ( self . root )) if d . is_dir ()]) class_to_idx = { cls_name : i for i , cls_name in enumerate ( classes , 1 )} # no bg label in NEU return classes , class_to_idx @ staticmethod def _get_bboxes ( ann : str ) -> List [ List [ int ]]: tree = ElementTree () . parse ( ann ) bboxes = [] for bndbox in tree . iterfind ( \"object/bndbox\" ): # should subtract 1 like coco? bbox = [ int ( bndbox . findtext ( t )) - 1 for t in ( \"xmin\" , \"ymin\" , \"xmax\" , \"ymax\" )] # type: ignore assert bbox [ 2 ] > bbox [ 0 ] and bbox [ 3 ] > bbox [ 1 ], f \"box size error, given {bbox}\" bboxes . append ( bbox ) return bboxes def __len__ ( self ): return len ( self . samples ) def __getitem__ ( self , idx : int ): # Note: images are grayscaled BUT resnet needs 3 channels image = cv2 . imread ( self . samples [ idx ] . image_path ) # BGR channel last image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) boxes = self . _get_bboxes ( self . samples [ idx ] . annotations ) num_objs = len ( boxes ) boxes = torch . as_tensor ( boxes , dtype = torch . float32 ) labels = torch . tensor ([ self . samples [ idx ] . class_idx ] * num_objs , dtype = torch . int64 ) image_id = torch . tensor ([ idx ], dtype = torch . int64 ) iscrowd = torch . zeros (( len ( boxes ),), dtype = torch . int64 ) target = {} target [ \"boxes\" ] = boxes target [ \"labels\" ] = labels target [ \"image_id\" ] = image_id target [ \"iscrowd\" ] = iscrowd if self . augmentation is not None : sample = self . augmentation ( ** { \"image\" : image , \"bboxes\" : boxes , \"labels\" : labels }) image = sample [ \"image\" ] target [ \"boxes\" ] = torch . as_tensor ( sample [ \"bboxes\" ], dtype = torch . float32 ) # guards against crops that don't pass the min_visibility augmentation threshold if not target [ \"boxes\" ] . numel (): return None target [ \"labels\" ] = torch . as_tensor ( sample [ \"labels\" ], dtype = torch . int64 ) if self . preprocess is not None : image = self . preprocess ( image = image )[ \"image\" ] boxes = target [ \"boxes\" ] target [ \"area\" ] = ( boxes [:, 3 ] - boxes [:, 1 ]) * ( boxes [:, 2 ] - boxes [:, 0 ]) return image , target , image_id def collate_fn ( self , batch ): batch = filter ( lambda x : x is not None , batch ) return tuple ( zip ( * batch ))","title":"NEUDET"},{"location":"reference/sagemaker_defect_detection/#ancestors-in-mro_3","text":"torch.utils.data.dataset.Dataset","title":"Ancestors (in MRO)"},{"location":"reference/sagemaker_defect_detection/#methods_3","text":"","title":"Methods"},{"location":"reference/sagemaker_defect_detection/#collate_fn","text":"def collate_fn ( self , batch ) View Source def collate_fn ( self , batch ): batch = filter ( lambda x : x is not None , batch ) return tuple ( zip ( * batch ))","title":"collate_fn"},{"location":"reference/sagemaker_defect_detection/#rpn","text":"class RPN ( out_channels : int = 512 , rpn_pre_nms_top_n_train : int = 1000 , rpn_pre_nms_top_n_test : int = 500 , rpn_post_nms_top_n_train : int = 1000 , rpn_post_nms_top_n_test : int = 500 , rpn_nms_thresh : float = 0.7 , rpn_fg_iou_thresh : float = 0.7 , rpn_bg_iou_thresh : float = 0.3 , rpn_batch_size_per_image : int = 256 , rpn_positive_fraction : float = 0.5 ) RPN Module as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. View Source class RPN ( nn . Module ): \"\"\" RPN Module as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. \"\"\" def __init__ ( self , out_channels: int = 512 , rpn_pre_nms_top_n_train: int = 1000 , # torchvision default 2000, rpn_pre_nms_top_n_test: int = 500 , # torchvision default 1000, rpn_post_nms_top_n_train: int = 1000 , # torchvision default 2000, rpn_post_nms_top_n_test: int = 500 , # torchvision default 1000, rpn_nms_thresh: float = 0.7 , rpn_fg_iou_thresh: float = 0.7 , rpn_bg_iou_thresh: float = 0.3 , rpn_batch_size_per_image: int = 256 , rpn_positive_fraction: float = 0.5 , ) -> None: super (). __init__ () rpn_anchor_generator = AnchorGenerator ( sizes =(( 64 , 128 , 256 , 512 ),), aspect_ratios =(( 0.5 , 1.0 , 2.0 ),)) rpn_head = RPNHead ( out_channels , rpn_anchor_generator . num_anchors_per_location ()[ 0 ]) rpn_pre_nms_top_n = dict ( training = rpn_pre_nms_top_n_train , testing = rpn_pre_nms_top_n_test ) rpn_post_nms_top_n = dict ( training = rpn_post_nms_top_n_train , testing = rpn_post_nms_top_n_test ) self . rpn = RegionProposalNetwork ( rpn_anchor_generator , rpn_head , rpn_fg_iou_thresh , rpn_bg_iou_thresh , rpn_batch_size_per_image , rpn_positive_fraction , rpn_pre_nms_top_n , rpn_post_nms_top_n , rpn_nms_thresh , ) def forward ( self , * args , ** kwargs ): return self . rpn (* args , ** kwargs )","title":"RPN"},{"location":"reference/sagemaker_defect_detection/#ancestors-in-mro_4","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/sagemaker_defect_detection/#class-variables_2","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/sagemaker_defect_detection/#methods_4","text":"","title":"Methods"},{"location":"reference/sagemaker_defect_detection/#add_module_2","text":"def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module","title":"add_module"},{"location":"reference/sagemaker_defect_detection/#apply_2","text":"def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self","title":"apply"},{"location":"reference/sagemaker_defect_detection/#bfloat16_2","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t )","title":"bfloat16"},{"location":"reference/sagemaker_defect_detection/#buffers_2","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf","title":"buffers"},{"location":"reference/sagemaker_defect_detection/#children_2","text":"def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module","title":"children"},{"location":"reference/sagemaker_defect_detection/#cpu_2","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ())","title":"cpu"},{"location":"reference/sagemaker_defect_detection/#cuda_2","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device ))","title":"cuda"},{"location":"reference/sagemaker_defect_detection/#double_2","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t )","title":"double"},{"location":"reference/sagemaker_defect_detection/#eval_2","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False )","title":"eval"},{"location":"reference/sagemaker_defect_detection/#extra_repr_3","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return ''","title":"extra_repr"},{"location":"reference/sagemaker_defect_detection/#float_2","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t )","title":"float"},{"location":"reference/sagemaker_defect_detection/#forward_2","text":"def forward ( self , * args , ** kwargs ) View Source def forward ( self , * args , ** kwargs ): return self . rpn ( * args , ** kwargs )","title":"forward"},{"location":"reference/sagemaker_defect_detection/#half_2","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t )","title":"half"},{"location":"reference/sagemaker_defect_detection/#load_state_dict_2","text":"def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys )","title":"load_state_dict"},{"location":"reference/sagemaker_defect_detection/#modules_2","text":"def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module","title":"modules"},{"location":"reference/sagemaker_defect_detection/#named_buffers_2","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_buffers"},{"location":"reference/sagemaker_defect_detection/#named_children_2","text":"def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module","title":"named_children"},{"location":"reference/sagemaker_defect_detection/#named_modules_2","text":"def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m","title":"named_modules"},{"location":"reference/sagemaker_defect_detection/#named_parameters_2","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_parameters"},{"location":"reference/sagemaker_defect_detection/#parameters_5","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param","title":"parameters"},{"location":"reference/sagemaker_defect_detection/#register_backward_hook_2","text":"def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle","title":"register_backward_hook"},{"location":"reference/sagemaker_defect_detection/#register_buffer_2","text":"def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name )","title":"register_buffer"},{"location":"reference/sagemaker_defect_detection/#register_forward_hook_2","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle","title":"register_forward_hook"},{"location":"reference/sagemaker_defect_detection/#register_forward_pre_hook_2","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle","title":"register_forward_pre_hook"},{"location":"reference/sagemaker_defect_detection/#register_parameter_2","text":"def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param","title":"register_parameter"},{"location":"reference/sagemaker_defect_detection/#requires_grad__2","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self","title":"requires_grad_"},{"location":"reference/sagemaker_defect_detection/#share_memory_2","text":"def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ())","title":"share_memory"},{"location":"reference/sagemaker_defect_detection/#state_dict_2","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination","title":"state_dict"},{"location":"reference/sagemaker_defect_detection/#to_2","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert )","title":"to"},{"location":"reference/sagemaker_defect_detection/#train_2","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self","title":"train"},{"location":"reference/sagemaker_defect_detection/#type_2","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type ))","title":"type"},{"location":"reference/sagemaker_defect_detection/#zero_grad_2","text":"def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ ()","title":"zero_grad"},{"location":"reference/sagemaker_defect_detection/#roi","text":"class RoI ( num_classes : int , box_fg_iou_thresh = 0.5 , box_bg_iou_thresh = 0.5 , box_batch_size_per_image = 512 , box_positive_fraction = 0.25 , bbox_reg_weights = None , box_score_thresh = 0.05 , box_nms_thresh = 0.5 , box_detections_per_img = 100 ) ROI Module as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. View Source class RoI ( nn . Module ) : \"\"\" ROI Module as described in Yu He , Kechen Song , Qinggang Meng , Yunhui Yan , \u201c An End - to - end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features , \u201d IEEE Transactions on Instrumentation and Measuremente , 2020 , 69 ( 4 ), 1493 - 1504. \"\"\" def __init__ ( self , num_classes: int , box_fg_iou_thresh = 0.5 , box_bg_iou_thresh = 0.5 , box_batch_size_per_image = 512 , box_positive_fraction = 0.25 , bbox_reg_weights = None , box_score_thresh = 0.05 , box_nms_thresh = 0.5 , box_detections_per_img = 100 , ) -> None: super (). __init__ () roi_pooler = MultiScaleRoIAlign ( featmap_names = [ \"0\" ], output_size = 7 , sampling_ratio = 2 ) box_head = CustomTwoMLPHead ( 512 * 7 ** 2 , 1024 ) box_predictor = FastRCNNPredictor ( 1024 , num_classes = num_classes ) self . roi_head = RoIHeads ( roi_pooler , box_head , box_predictor , box_fg_iou_thresh , box_bg_iou_thresh , box_batch_size_per_image , box_positive_fraction , bbox_reg_weights , box_score_thresh , box_nms_thresh , box_detections_per_img , ) def forward ( self , * args , ** kwargs ) : return self . roi_head ( * args , ** kwargs )","title":"RoI"},{"location":"reference/sagemaker_defect_detection/#ancestors-in-mro_5","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/sagemaker_defect_detection/#class-variables_3","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/sagemaker_defect_detection/#methods_5","text":"","title":"Methods"},{"location":"reference/sagemaker_defect_detection/#add_module_3","text":"def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module","title":"add_module"},{"location":"reference/sagemaker_defect_detection/#apply_3","text":"def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self","title":"apply"},{"location":"reference/sagemaker_defect_detection/#bfloat16_3","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t )","title":"bfloat16"},{"location":"reference/sagemaker_defect_detection/#buffers_3","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf","title":"buffers"},{"location":"reference/sagemaker_defect_detection/#children_3","text":"def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module","title":"children"},{"location":"reference/sagemaker_defect_detection/#cpu_3","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ())","title":"cpu"},{"location":"reference/sagemaker_defect_detection/#cuda_3","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device ))","title":"cuda"},{"location":"reference/sagemaker_defect_detection/#double_3","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t )","title":"double"},{"location":"reference/sagemaker_defect_detection/#eval_3","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False )","title":"eval"},{"location":"reference/sagemaker_defect_detection/#extra_repr_4","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return ''","title":"extra_repr"},{"location":"reference/sagemaker_defect_detection/#float_3","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t )","title":"float"},{"location":"reference/sagemaker_defect_detection/#forward_3","text":"def forward ( self , * args , ** kwargs ) View Source def forward ( self , * args , ** kwargs ): return self . roi_head ( * args , ** kwargs )","title":"forward"},{"location":"reference/sagemaker_defect_detection/#half_3","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t )","title":"half"},{"location":"reference/sagemaker_defect_detection/#load_state_dict_3","text":"def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys )","title":"load_state_dict"},{"location":"reference/sagemaker_defect_detection/#modules_3","text":"def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module","title":"modules"},{"location":"reference/sagemaker_defect_detection/#named_buffers_3","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_buffers"},{"location":"reference/sagemaker_defect_detection/#named_children_3","text":"def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module","title":"named_children"},{"location":"reference/sagemaker_defect_detection/#named_modules_3","text":"def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m","title":"named_modules"},{"location":"reference/sagemaker_defect_detection/#named_parameters_3","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_parameters"},{"location":"reference/sagemaker_defect_detection/#parameters_6","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param","title":"parameters"},{"location":"reference/sagemaker_defect_detection/#register_backward_hook_3","text":"def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle","title":"register_backward_hook"},{"location":"reference/sagemaker_defect_detection/#register_buffer_3","text":"def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name )","title":"register_buffer"},{"location":"reference/sagemaker_defect_detection/#register_forward_hook_3","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle","title":"register_forward_hook"},{"location":"reference/sagemaker_defect_detection/#register_forward_pre_hook_3","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle","title":"register_forward_pre_hook"},{"location":"reference/sagemaker_defect_detection/#register_parameter_3","text":"def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param","title":"register_parameter"},{"location":"reference/sagemaker_defect_detection/#requires_grad__3","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self","title":"requires_grad_"},{"location":"reference/sagemaker_defect_detection/#share_memory_3","text":"def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ())","title":"share_memory"},{"location":"reference/sagemaker_defect_detection/#state_dict_3","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination","title":"state_dict"},{"location":"reference/sagemaker_defect_detection/#to_3","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert )","title":"to"},{"location":"reference/sagemaker_defect_detection/#train_3","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self","title":"train"},{"location":"reference/sagemaker_defect_detection/#type_3","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type ))","title":"type"},{"location":"reference/sagemaker_defect_detection/#zero_grad_3","text":"def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ ()","title":"zero_grad"},{"location":"reference/sagemaker_defect_detection/classifier/","text":"Module sagemaker_defect_detection.classifier View Source # mypy: ignore-errors from typing import Dict import os from collections import OrderedDict from argparse import ArgumentParser , Namespace from multiprocessing import cpu_count import torch import torch.optim as optim import torch.nn.functional as F from torch.utils.data import DataLoader import pytorch_lightning as pl from pytorch_lightning.callbacks import EarlyStopping , ModelCheckpoint import pytorch_lightning.metrics.functional as plm from sagemaker_defect_detection import Classification , NEUCLS , get_transform from sagemaker_defect_detection.utils import load_checkpoint , freeze def metrics ( name : str , out : torch . Tensor , target : torch . Tensor ) -> Dict [ str , torch . Tensor ]: pred = torch . argmax ( out , 1 ) . detach () target = target . detach () metrics = {} metrics [ name + \"_acc\" ] = plm . accuracy ( pred , target ) metrics [ name + \"_prec\" ] = plm . precision ( pred , target ) metrics [ name + \"_recall\" ] = plm . recall ( pred , target ) metrics [ name + \"_f1_score\" ] = plm . recall ( pred , target ) return metrics class DDNClassification ( pl . LightningModule ): def __init__ ( self , data_path : str , backbone : str , freeze_backbone : bool , num_classes : int , learning_rate : float , batch_size : int , momentum : float , weight_decay : float , seed : int , ** kwargs ) -> None : super () . __init__ () self . data_path = data_path self . backbone = backbone self . freeze_backbone = freeze_backbone self . num_classes = num_classes self . learning_rate = learning_rate self . batch_size = batch_size self . momentum = momentum self . weight_decay = weight_decay self . seed = seed self . train_dataset = NEUCLS ( self . data_path , split = \"train\" , transform = get_transform ( \"train\" ), seed = self . seed ) self . val_dataset = NEUCLS ( self . data_path , split = \"val\" , transform = get_transform ( \"val\" ), seed = self . seed ) self . test_dataset = NEUCLS ( self . data_path , split = \"test\" , transform = get_transform ( \"test\" ), seed = self . seed ) self . model = Classification ( self . backbone , self . num_classes ) if self . freeze_backbone : for param in self . model . mfn . backbone . parameters (): param . requires_grad = False def forward ( self , x ): # ignore return self . model ( x ) def training_step ( self , batch , batch_idx ): images , target = batch output = self ( images ) loss_val = F . cross_entropy ( output , target ) metrics_dict = metrics ( \"train\" , output , target ) tqdm_dict = { \"train_loss\" : loss_val , ** metrics_dict } output = OrderedDict ({ \"loss\" : loss_val , \"progress_bar\" : tqdm_dict , \"log\" : tqdm_dict }) return output def validation_step ( self , batch , batch_idx ): images , target = batch output = self ( images ) loss_val = F . cross_entropy ( output , target ) metrics_dict = metrics ( \"val\" , output , target ) output = OrderedDict ({ \"val_loss\" : loss_val , ** metrics_dict }) return output def validation_epoch_end ( self , outputs ): log_dict = {} for metric_name in outputs [ 0 ]: log_dict [ metric_name ] = torch . stack ([ x [ metric_name ] for x in outputs ]) . mean () return { \"log\" : log_dict , \"progress_bar\" : log_dict , ** log_dict } def test_step ( self , batch , batch_idx ): images , target = batch output = self ( images ) loss_val = F . cross_entropy ( output , target ) metrics_dict = metrics ( \"test\" , output , target ) output = OrderedDict ({ \"test_loss\" : loss_val , ** metrics_dict }) return output def test_epoch_end ( self , outputs ): log_dict = {} for metric_name in outputs [ 0 ]: log_dict [ metric_name ] = torch . stack ([ x [ metric_name ] for x in outputs ]) . mean () return { \"log\" : log_dict , \"progress_bar\" : log_dict , ** log_dict } def configure_optimizers ( self ): optimizer = optim . SGD ( self . parameters (), lr = self . learning_rate , momentum = self . momentum , weight_decay = self . weight_decay ) return optimizer def train_dataloader ( self ): train_loader = DataLoader ( dataset = self . train_dataset , batch_size = self . batch_size , shuffle = True , num_workers = cpu_count (), ) return train_loader def val_dataloader ( self ): val_loader = DataLoader ( self . val_dataset , batch_size = self . batch_size , shuffle = False , num_workers = cpu_count () // 2 , ) return val_loader def test_dataloader ( self ): test_loader = DataLoader ( self . test_dataset , batch_size = self . batch_size , shuffle = False , num_workers = cpu_count (), ) return test_loader @staticmethod def add_model_specific_args ( parent_parser ): # pragma: no-cover parser = ArgumentParser ( parents = [ parent_parser ], add_help = False ) aa = parser . add_argument aa ( \"--data-path\" , metavar = \"DIR\" , type = str , default = os . getenv ( \"SM_CHANNEL_TRAINING\" , \"\" ), ) aa ( \"--backbone\" , default = \"resnet34\" , ) aa ( \"--freeze-backbone\" , action = \"store_true\" , ) aa ( \"--num-classes\" , default = 6 , type = int , metavar = \"N\" , ) aa ( \"-b\" , \"--batch-size\" , default = 64 , type = int , metavar = \"N\" , ) aa ( \"--lr\" , \"--learning-rate\" , default = 1e-3 , type = float , metavar = \"LR\" , dest = \"learning_rate\" , ) aa ( \"--momentum\" , default = 0.9 , type = float , metavar = \"M\" , help = \"momentum\" ) aa ( \"--wd\" , \"--weight-decay\" , default = 1e-4 , type = float , metavar = \"W\" , dest = \"weight_decay\" , ) aa ( \"--seed\" , type = int , default = 42 , ) return parser def get_args () -> Namespace : parent_parser = ArgumentParser ( add_help = False ) aa = parent_parser . add_argument aa ( \"--epochs\" , type = int , default = 100 , help = \"number of training epochs\" ) aa ( \"--save-path\" , metavar = \"DIR\" , default = os . getenv ( \"SM_MODEL_DIR\" , \"\" ), type = str , help = \"path to save output\" ) aa ( \"--gpus\" , type = int , default = os . getenv ( \"SM_NUM_GPUS\" , 1 ), help = \"how many gpus\" ) aa ( \"--distributed-backend\" , type = str , default = \"\" , choices = ( \"dp\" , \"ddp\" , \"ddp2\" ), help = \"supports three options dp, ddp, ddp2\" , ) aa ( \"--use-16bit\" , dest = \"use_16bit\" , action = \"store_true\" , help = \"if true uses 16 bit precision\" ) parser = DDNClassification . add_model_specific_args ( parent_parser ) return parser . parse_args () def model_fn ( model_dir ): # TODO: `model_fn` doesn't get more args # see: https://github.com/aws/sagemaker-inference-toolkit/issues/65 backbone = \"resnet34\" num_classes = 6 model = load_checkpoint ( Classification ( backbone , num_classes ), model_dir , prefix = \"model\" ) model = model . eval () freeze ( model ) return model def main ( args : Namespace ) -> None : model = DDNClassification ( ** vars ( args )) if args . seed is not None : pl . seed_everything ( args . seed ) if torch . cuda . device_count () > 1 : torch . cuda . manual_seed_all ( args . seed ) # TODO: add deterministic training # torch.backends.cudnn.deterministic = True checkpoint_callback = ModelCheckpoint ( filepath = os . path . join ( args . save_path , \"{epoch}-{val_loss:.3f}-{val_acc:.3f}\" ), save_top_k = 1 , verbose = True , monitor = \"val_acc\" , mode = \"max\" , ) early_stop_callback = EarlyStopping ( \"val_loss\" , patience = 10 ) trainer = pl . Trainer ( default_root_dir = args . save_path , gpus = args . gpus , max_epochs = args . epochs , early_stop_callback = early_stop_callback , checkpoint_callback = checkpoint_callback , gradient_clip_val = 10 , num_sanity_val_steps = 0 , distributed_backend = args . distributed_backend or None , # precision=16 if args.use_16bit else 32, # TODO: amp apex support ) trainer . fit ( model ) trainer . test () return if __name__ == \"__main__\" : main ( get_args ()) Functions get_args def get_args ( ) -> argparse . Namespace View Source def get_args () -> Namespace : parent_parser = ArgumentParser ( add_help = False ) aa = parent_parser . add_argument aa ( \"--epochs\" , type = int , default = 100 , help = \"number of training epochs\" ) aa ( \"--save-path\" , metavar = \"DIR\" , default = os . getenv ( \"SM_MODEL_DIR\" , \"\" ), type = str , help = \"path to save output\" ) aa ( \"--gpus\" , type = int , default = os . getenv ( \"SM_NUM_GPUS\" , 1 ), help = \"how many gpus\" ) aa ( \"--distributed-backend\" , type = str , default = \"\" , choices = ( \"dp\" , \"ddp\" , \"ddp2\" ), help = \"supports three options dp, ddp, ddp2\" , ) aa ( \"--use-16bit\" , dest = \"use_16bit\" , action = \"store_true\" , help = \"if true uses 16 bit precision\" ) parser = DDNClassification . add_model_specific_args ( parent_parser ) return parser . parse_args () main def main ( args : argparse . Namespace ) -> None View Source def main ( args : Namespace ) -> None : model = DDNClassification ( ** vars ( args )) if args . seed is not None : pl . seed_everything ( args . seed ) if torch . cuda . device_count () > 1 : torch . cuda . manual_seed_all ( args . seed ) # TODO: add deterministic training # torch.backends.cudnn.deterministic = True checkpoint_callback = ModelCheckpoint ( filepath = os . path . join ( args . save_path , \"{epoch}-{val_loss:.3f}-{val_acc:.3f}\" ), save_top_k = 1 , verbose = True , monitor = \"val_acc\" , mode = \"max\" , ) early_stop_callback = EarlyStopping ( \"val_loss\" , patience = 10 ) trainer = pl . Trainer ( default_root_dir = args . save_path , gpus = args . gpus , max_epochs = args . epochs , early_stop_callback = early_stop_callback , checkpoint_callback = checkpoint_callback , gradient_clip_val = 10 , num_sanity_val_steps = 0 , distributed_backend = args . distributed_backend or None , # precision=16 if args.use_16bit else 32, # TODO: amp apex support ) trainer . fit ( model ) trainer . test () return metrics def metrics ( name : str , out : torch . Tensor , target : torch . Tensor ) -> Dict [ str , torch . Tensor ] View Source def metrics ( name : str , out : torch . Tensor , target : torch . Tensor ) -> Dict [ str , torch . Tensor ]: pred = torch . argmax ( out , 1 ). detach () target = target . detach () metrics = {} metrics [ name + \"_acc\" ] = plm . accuracy ( pred , target ) metrics [ name + \"_prec\" ] = plm . precision ( pred , target ) metrics [ name + \"_recall\" ] = plm . recall ( pred , target ) metrics [ name + \"_f1_score\" ] = plm . recall ( pred , target ) return metrics model_fn def model_fn ( model_dir ) View Source def model_fn ( model_dir ): # TODO: `model_fn` doesn't get more args # see: https://github.com/aws/sagemaker-inference-toolkit/issues/65 backbone = \"resnet34\" num_classes = 6 model = load_checkpoint ( Classification ( backbone , num_classes ), model_dir , prefix = \"model\" ) model = model . eval () freeze ( model ) return model Classes DDNClassification class DDNClassification ( data_path : str , backbone : str , freeze_backbone : bool , num_classes : int , learning_rate : float , batch_size : int , momentum : float , weight_decay : float , seed : int , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) pytorch_lightning.core.lightning.LightningModule abc.ABC pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin pytorch_lightning.core.grads.GradInformation pytorch_lightning.core.saving.ModelIO pytorch_lightning.core.hooks.ModelHooks torch.nn.modules.module.Module Class variables CHECKPOINT_HYPER_PARAMS_KEY CHECKPOINT_HYPER_PARAMS_NAME CHECKPOINT_HYPER_PARAMS_TYPE T_destination dump_patches Static methods add_model_specific_args def add_model_specific_args ( parent_parser ) View Source @staticmethod def add_model_specific_args ( parent_parser ) : # pragma : no - cover parser = ArgumentParser ( parents =[ parent_parser ] , add_help = False ) aa = parser . add_argument aa ( \"--data-path\" , metavar = \"DIR\" , type = str , default = os . getenv ( \"SM_CHANNEL_TRAINING\" , \"\" ), ) aa ( \"--backbone\" , default = \"resnet34\" , ) aa ( \"--freeze-backbone\" , action = \"store_true\" , ) aa ( \"--num-classes\" , default = 6 , type = int , metavar = \"N\" , ) aa ( \"-b\" , \"--batch-size\" , default = 64 , type = int , metavar = \"N\" , ) aa ( \"--lr\" , \"--learning-rate\" , default = 1e-3 , type = float , metavar = \"LR\" , dest = \"learning_rate\" , ) aa ( \"--momentum\" , default = 0.9 , type = float , metavar = \"M\" , help = \"momentum\" ) aa ( \"--wd\" , \"--weight-decay\" , default = 1e-4 , type = float , metavar = \"W\" , dest = \"weight_decay\" , ) aa ( \"--seed\" , type = int , default = 42 , ) return parser load_from_checkpoint def load_from_checkpoint ( checkpoint_path : str , * args , map_location : Union [ Dict [ str , str ], str , torch . device , int , Callable , NoneType ] = None , hparams_file : Union [ str , NoneType ] = None , tags_csv : Union [ str , NoneType ] = None , ** kwargs ) Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to __init__ in the checkpoint under module_arguments Any arguments specified through *args and **kwargs will override args stored in hparams . Args: checkpoint_path: Path to checkpoint. This can also be a URL. args: Any positional args needed to init the model. map_location: If your checkpoint saved a GPU model and you now load on CPUs or a different number of GPUs, use this to map to the new setup. The behaviour is the same as in :func: torch.load . hparams_file: Optional path to a .yaml file with hierarchical structure as in this example:: drop_prob : 0.2 dataloader : batch_size : 32 You most likely won 't need this since Lightning will always save the hyperparameters to the checkpoint. However, if your checkpoint weights don' t have the hyperparameters saved , use this method to pass in a . yaml file with the hparams you 'd like to use. These will be converted into a :class:`~dict` and passed into your :class:`LightningModule` for use. If your model' s `hparams` argument is : class : `~argparse.Namespace` and . yaml file has hierarchical structure , you need to refactor your model to treat `hparams` as : class : `~dict` . . csv files are acceptable here till v0 .9.0 , see tags_csv argument for detailed usage . tags_csv : .. warning :: .. deprecated :: 0.7.6 `tags_csv` argument is deprecated in v0 .7.6 . Will be removed v0 .9.0 . Optional path to a . csv file with two columns ( key , value ) as in this example :: key , value drop_prob , 0.2 batch_size , 32 Use this method to pass in a . csv file with the hparams you 'd like to use. hparam_overrides: A dictionary with keys to override in the hparams kwargs: Any keyword args needed to init the model. Return: :class: LightningModule with loaded weights and hyperparameters (if available). Example: .. code-block:: python # load weights without mapping ... MyLightningModule . load_from_checkpoint ( 'path/to/checkpoint.ckpt' ) # or load weights mapping all weights from GPU 1 to GPU 0 ... map_location = { 'cuda:1' : 'cuda:0' } MyLightningModule . load_from_checkpoint ( 'path/to/checkpoint.ckpt' , map_location = map_location ) # or load weights and hyperparameters from separate files. MyLightningModule . load_from_checkpoint ( 'path/to/checkpoint.ckpt' , hparams_file = '/path/to/hparams_file.yaml' ) # override some of the params with new values MyLightningModule . load_from_checkpoint ( PATH , num_layers = 128 , pretrained_ckpt_path : NEW_PATH , ) # predict pretrained_model . eval () pretrained_model . freeze () y_hat = pretrained_model ( x ) View Source @classmethod def load_from_checkpoint ( cls , checkpoint_path : str , * args , map_location : Optional [ Union [ Dict [ str , str ] , str , torch . device , int , Callable ]] = None , hparams_file : Optional [ str ] = None , tags_csv : Optional [ str ] = None , # backward compatible, todo: remove in v0.9.0 ** kwargs ) : r \" \"\" Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to `__init__` in the checkpoint under `module_arguments` Any arguments specified through \\ *args and \\ * \\ *kwargs will override args stored in `hparams`. Args: checkpoint_path: Path to checkpoint. This can also be a URL. args: Any positional args needed to init the model. map_location: If your checkpoint saved a GPU model and you now load on CPUs or a different number of GPUs, use this to map to the new setup. The behaviour is the same as in :func:`torch.load`. hparams_file: Optional path to a .yaml file with hierarchical structure as in this example:: drop_prob: 0.2 dataloader: batch_size: 32 You most likely won't need this since Lightning will always save the hyperparameters to the checkpoint. However, if your checkpoint weights don't have the hyperparameters saved, use this method to pass in a .yaml file with the hparams you'd like to use. These will be converted into a :class:`~dict` and passed into your :class:`LightningModule` for use. If your model's `hparams` argument is :class:`~argparse.Namespace` and .yaml file has hierarchical structure, you need to refactor your model to treat `hparams` as :class:`~dict`. .csv files are acceptable here till v0.9.0, see tags_csv argument for detailed usage. tags_csv: .. warning:: .. deprecated:: 0.7.6 `tags_csv` argument is deprecated in v0.7.6. Will be removed v0.9.0. Optional path to a .csv file with two columns (key, value) as in this example:: key,value drop_prob,0.2 batch_size,32 Use this method to pass in a .csv file with the hparams you'd like to use. hparam_overrides: A dictionary with keys to override in the hparams kwargs: Any keyword args needed to init the model. Return: :class:`LightningModule` with loaded weights and hyperparameters (if available). Example: .. code-block:: python # load weights without mapping ... MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt') # or load weights mapping all weights from GPU 1 to GPU 0 ... map_location = {'cuda:1':'cuda:0'} MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', map_location=map_location ) # or load weights and hyperparameters from separate files. MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', hparams_file='/path/to/hparams_file.yaml' ) # override some of the params with new values MyLightningModule.load_from_checkpoint( PATH, num_layers=128, pretrained_ckpt_path: NEW_PATH, ) # predict pretrained_model.eval() pretrained_model.freeze() y_hat = pretrained_model(x) \"\" \" if map_location is not None : checkpoint = pl_load ( checkpoint_path , map_location = map_location ) else : checkpoint = pl_load ( checkpoint_path , map_location = lambda storage , loc : storage ) # add the hparams from csv file to checkpoint if tags_csv is not None : hparams_file = tags_csv rank_zero_warn ( '`tags_csv` argument is deprecated in v0.7.6. Will be removed v0.9.0' , DeprecationWarning ) if hparams_file is not None : extension = hparams_file . split ( '.' ) [ - 1 ] if extension . lower () in ( 'csv' ) : hparams = load_hparams_from_tags_csv ( hparams_file ) elif extension . lower () in ( 'yml' , 'yaml' ) : hparams = load_hparams_from_yaml ( hparams_file ) else : raise ValueError ( '.csv, .yml or .yaml is required for `hparams_file`' ) hparams [ 'on_gpu' ] = False # overwrite hparams by the given file checkpoint [ cls . CHECKPOINT_HYPER_PARAMS_KEY ] = hparams # for past checkpoint need to add the new key if cls . CHECKPOINT_HYPER_PARAMS_KEY not in checkpoint : checkpoint [ cls . CHECKPOINT_HYPER_PARAMS_KEY ] = {} # override the hparams with values that were passed in checkpoint [ cls . CHECKPOINT_HYPER_PARAMS_KEY ] . update ( kwargs ) model = cls . _load_model_state ( checkpoint , * args , ** kwargs ) return model load_from_metrics def load_from_metrics ( weights_path , tags_csv , map_location = None ) Warning: Deprecated in version 0.7.0. You should use :meth: load_from_checkpoint instead. Will be removed in v0.9.0. View Source @classmethod def load_from_metrics ( cls , weights_path , tags_csv , map_location = None ) : r \" \"\" Warning: Deprecated in version 0.7.0. You should use :meth:`load_from_checkpoint` instead. Will be removed in v0.9.0. \"\" \" rank_zero_warn ( \"`load_from_metrics` method has been unified with `load_from_checkpoint` in v0.7.0.\" \" The deprecated method will be removed in v0.9.0.\" , DeprecationWarning ) return cls . load_from_checkpoint ( weights_path , tags_csv = tags_csv , map_location = map_location ) Instance variables device dtype example_input_array hparams on_gpu True if your model is currently running on GPUs. Useful to set flags around the LightningModule for different CPU vs GPU behavior. Methods add_module def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module amp_scale_loss def amp_scale_loss ( self , unscaled_loss , optimizer , optimizer_idx ) View Source def amp_scale_loss ( self , unscaled_loss , optimizer , optimizer_idx ): if NATIVE_AMP_AVALAIBLE : scaled_loss = self . trainer . scaler . scale ( unscaled_loss ) else : scaled_loss = amp . scale_loss ( unscaled_loss , optimizer ) return scaled_loss apply def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self backward def backward ( self , trainer , loss : torch . Tensor , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) -> None Override backward with your own implementation if you need to. Args: trainer: Pointer to the trainer loss: Loss is already scaled by accumulated grads optimizer: Current optimizer being used optimizer_idx: Index of the current optimizer being used Called to perform backward step. Feel free to override as needed. The loss passed in has already been scaled for accumulated gradients if requested. Example:: def backward(self, trainer, loss, optimizer, optimizer_idx): loss.backward() View Source def backward ( self , trainer , loss : Tensor , optimizer : Optimizer , optimizer_idx : int ) -> None : \"\"\" Override backward with your own implementation if you need to. Args: trainer: Pointer to the trainer loss: Loss is already scaled by accumulated grads optimizer: Current optimizer being used optimizer_idx: Index of the current optimizer being used Called to perform backward step. Feel free to override as needed. The loss passed in has already been scaled for accumulated gradients if requested. Example:: def backward(self, trainer, loss, optimizer, optimizer_idx): loss.backward() \"\"\" loss . backward () bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf children def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module configure_apex def configure_apex ( self , amp : object , model : 'LightningModule' , optimizers : List [ torch . optim . optimizer . Optimizer ], amp_level : str ) -> Tuple [ _ForwardRef ( 'LightningModule' ), List [ torch . optim . optimizer . Optimizer ]] Override to init AMP your own way. Must return a model and list of optimizers. Args: amp: pointer to amp library object. model: pointer to current :class: LightningModule . optimizers: list of optimizers passed in :meth: configure_optimizers . amp_level: AMP mode chosen ('O1', 'O2', etc...) Return: Apex wrapped model and optimizers Examples: .. code-block:: python # Default implementation used by Trainer . def configure_apex ( self , amp , model , optimizers , amp_level ): model , optimizers = amp . initialize ( model , optimizers , opt_level = amp_level , ) return model , optimizers View Source def configure_apex ( self , amp : object , model : 'LightningModule' , optimizers : List [ Optimizer ] , amp_level : str ) -> Tuple [ 'LightningModule' , List [ Optimizer ]] : r \" \"\" Override to init AMP your own way. Must return a model and list of optimizers. Args: amp: pointer to amp library object. model: pointer to current :class:`LightningModule`. optimizers: list of optimizers passed in :meth:`configure_optimizers`. amp_level: AMP mode chosen ('O1', 'O2', etc...) Return: Apex wrapped model and optimizers Examples: .. code-block:: python # Default implementation used by Trainer. def configure_apex(self, amp, model, optimizers, amp_level): model, optimizers = amp.initialize( model, optimizers, opt_level=amp_level, ) return model, optimizers \"\" \" model , optimizers = amp . initialize ( model , optimizers , opt_level = amp_level ) return model , optimizers configure_ddp def configure_ddp ( self , model : 'LightningModule' , device_ids : List [ int ] ) -> torch . nn . parallel . distributed . DistributedDataParallel Override to init DDP in your own way or with your own wrapper. The only requirements are that: On a validation batch the call goes to model.validation_step . On a training batch the call goes to model.training_step . On a testing batch, the call goes to model.test_step .+ Args: model: the :class: LightningModule currently being optimized. device_ids: the list of GPU ids. Return: DDP wrapped model Examples: .. code-block:: python # default implementation used in Trainer def configure_ddp(self, model, device_ids): # Lightning DDP simply routes to test_step, val_step, etc... model = LightningDistributedDataParallel( model, device_ids=device_ids, find_unused_parameters=True ) return model View Source def configure_ddp ( self , model : 'LightningModule' , device_ids : List [ int ] ) -> DistributedDataParallel : r \" \"\" Override to init DDP in your own way or with your own wrapper. The only requirements are that: 1. On a validation batch the call goes to ``model.validation_step``. 2. On a training batch the call goes to ``model.training_step``. 3. On a testing batch, the call goes to ``model.test_step``.+ Args: model: the :class:`LightningModule` currently being optimized. device_ids: the list of GPU ids. Return: DDP wrapped model Examples: .. code-block:: python # default implementation used in Trainer def configure_ddp(self, model, device_ids): # Lightning DDP simply routes to test_step, val_step, etc... model = LightningDistributedDataParallel( model, device_ids=device_ids, find_unused_parameters=True ) return model \"\" \" model = LightningDistributedDataParallel ( model , device_ids = device_ids , find_unused_parameters = True ) return model configure_optimizers def configure_optimizers ( self ) View Source def configure_optimizers ( self ): optimizer = optim . SGD ( self . parameters (), lr = self . learning_rate , momentum = self . momentum , weight_decay = self . weight_decay ) return optimizer cpu def cpu ( self ) -> torch . nn . modules . module . Module Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self ) -> Module : \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" self . _device = torch . device ( 'cpu') return super (). cpu () cuda def cuda ( self , device : Union [ int , NoneType ] = None ) -> torch . nn . modules . module . Module Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device: if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self , device : Optional [ int ] = None ) -> Module : \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device: if specified, all parameters will be copied to that device Returns: Module: self \"\"\" self . _device = torch . device ( 'cuda' , index = device ) return super (). cuda ( device = device ) double def double ( self ) -> torch . nn . modules . module . Module Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self ) -> Module : \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" self . _dtype = torch . double return super (). double () eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False ) extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float def float ( self ) -> torch . nn . modules . module . Module Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self ) -> Module : \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" self . _dtype = torch . float return super (). float () forward def forward ( self , x ) View Source def forward ( self , x ): # ignore return self . model ( x ) freeze def freeze ( self ) -> None Freeze all params for inference. Example: .. code-block:: python model = MyLightningModule(...) model.freeze() View Source def freeze ( self ) -> None : r \"\"\" Freeze all params for inference. Example: .. code-block:: python model = MyLightningModule(...) model.freeze() \"\"\" for param in self . parameters (): param . requires_grad = False self . eval () get_progress_bar_dict def get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]] Additional items to be displayed in the progress bar. Return: Dictionary with the items to be displayed in the progress bar. View Source def get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]]: r \"\"\" Additional items to be displayed in the progress bar. Return: Dictionary with the items to be displayed in the progress bar. \"\"\" # call . item () only once but store elements without graphs running_train_loss = self . trainer . running_loss . mean () avg_training_loss = running_train_loss . cpu (). item () if running_train_loss is not None else float ( 'NaN' ) tqdm_dict = { 'loss' : '{:.3f}' . format ( avg_training_loss ) } if self . trainer . truncated_bptt_steps is not None : tqdm_dict [ 'split_idx' ] = self . trainer . split_idx if self . trainer . logger is not None and self . trainer . logger . version is not None : tqdm_dict [ 'v_num' ] = self . trainer . logger . version return tqdm_dict get_tqdm_dict def get_tqdm_dict ( self ) -> Dict [ str , Union [ int , str ]] Additional items to be displayed in the progress bar. Return: Dictionary with the items to be displayed in the progress bar. Warning: Deprecated since v0.7.3. Use :meth: get_progress_bar_dict instead. View Source def get_tqdm_dict ( self ) -> Dict [ str , Union [ int , str ]] : \" \"\" Additional items to be displayed in the progress bar. Return: Dictionary with the items to be displayed in the progress bar. Warning: Deprecated since v0.7.3. Use :meth:`get_progress_bar_dict` instead. \"\" \" rank_zero_warn ( \"`get_tqdm_dict` was renamed to `get_progress_bar_dict` in v0.7.3\" \" and this method will be removed in v1.0.0\" , DeprecationWarning ) return self . get_progress_bar_dict () grad_norm def grad_norm ( self , norm_type : Union [ float , int , str ] ) -> Dict [ str , float ] Compute each parameter's gradient's norm and their overall norm. The overall norm is computed over all gradients together, as if they were concatenated into a single vector. Args: norm_type: The type of the used p-norm, cast to float if necessary. Can be 'inf' for infinity norm. Return: norms: The dictionary of p-norms of each parameter's gradient and a special entry for the total p-norm of the gradients viewed as a single vector. View Source def grad_norm ( self , norm_type : Union [ float , int , str ]) -> Dict [ str , float ]: \"\"\"Compute each parameter's gradient's norm and their overall norm. The overall norm is computed over all gradients together, as if they were concatenated into a single vector. Args: norm_type: The type of the used p-norm, cast to float if necessary. Can be ``'inf'`` for infinity norm. Return: norms: The dictionary of p-norms of each parameter's gradient and a special entry for the total p-norm of the gradients viewed as a single vector. \"\"\" norm_type = float ( norm_type ) norms , all_norms = {} , [] for name , p in self . named_parameters (): if p . grad is None : continue param_norm = float ( p . grad . data . norm ( norm_type )) norms [ f 'grad_{norm_type}_norm_{name}' ] = round ( param_norm , 3 ) all_norms . append ( param_norm ) total_norm = float ( torch . tensor ( all_norms ). norm ( norm_type )) norms [ f 'grad_{norm_type}_norm_total' ] = round ( total_norm , 3 ) return norms half def half ( self ) -> torch . nn . modules . module . Module Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self ) -> Module : \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" self . _dtype = torch . half return super (). half () init_ddp_connection def init_ddp_connection ( self , global_rank : int , world_size : int , is_slurm_managing_tasks : bool = True ) -> None Override to define your custom way of setting up a distributed environment. Lightning's implementation uses env:// init by default and sets the first node as root for SLURM managed cluster. Args: global_rank: The global process idx. world_size: Number of GPUs being use across all nodes. (num_nodes * num_gpus). is_slurm_managing_tasks: is cluster managed by SLURM. View Source def init_ddp_connection ( self , global_rank : int , world_size : int , is_slurm_managing_tasks : bool = True ) -> None : \"\"\" Override to define your custom way of setting up a distributed environment. Lightning's implementation uses env:// init by default and sets the first node as root for SLURM managed cluster. Args: global_rank: The global process idx. world_size: Number of GPUs being use across all nodes. (num_nodes * num_gpus). is_slurm_managing_tasks: is cluster managed by SLURM. \"\"\" if is_slurm_managing_tasks : self . _init_slurm_connection () if 'MASTER_ADDR' not in os . environ : rank_zero_warn ( \"MASTER_ADDR environment variable is not defined. Set as localhost\" ) os . environ [ 'MASTER_ADDR' ] = '127.0.0.1' log . debug ( f \"MASTER_ADDR: {os.environ['MASTER_ADDR']}\" ) if 'MASTER_PORT' not in os . environ : rank_zero_warn ( \"MASTER_PORT environment variable is not defined. Set as 12910\" ) os . environ [ 'MASTER_PORT' ] = '12910' log . debug ( f \"MASTER_PORT: {os.environ['MASTER_PORT']}\" ) if 'WORLD_SIZE' in os . environ and int ( os . environ [ 'WORLD_SIZE' ]) != world_size : rank_zero_warn ( f \"WORLD_SIZE environment variable ({os.environ['WORLD_SIZE']}) \" f \"is not equal to the computed world size ({world_size}). Ignored.\" ) torch_backend = \"nccl\" if self . trainer . on_gpu else \"gloo\" log . info ( f \"initializing ddp: GLOBAL_RANK: {global_rank}, MEMBER: {global_rank+1}/{world_size}\" ) torch_distrib . init_process_group ( torch_backend , rank = global_rank , world_size = world_size ) load_state_dict def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys ) modules def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem on_after_backward def on_after_backward ( self ) -> None Called in the training loop after loss.backward() and before optimizers do anything. This is the ideal place to inspect or log gradient information. Example:: def on_after_backward(self): # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge params = self.state_dict() for k, v in params.items(): grads = v name = k self.logger.experiment.add_histogram(tag=name, values=grads, global_step=self.trainer.global_step) View Source def on_after_backward ( self ) -> None : \"\"\" Called in the training loop after loss.backward() and before optimizers do anything. This is the ideal place to inspect or log gradient information. Example:: def on_after_backward(self): # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge params = self.state_dict() for k, v in params.items(): grads = v name = k self.logger.experiment.add_histogram(tag=name, values=grads, global_step=self.trainer.global_step) \"\"\" on_batch_end def on_batch_end ( self ) -> None Called in the training loop after the batch. View Source def on_batch_end ( self ) -> None : \"\"\" Called in the training loop after the batch. \"\"\" on_batch_start def on_batch_start ( self , batch : Any ) -> None Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Args: batch: The batched data as it is returned by the training DataLoader. View Source def on_batch_start ( self , batch : Any ) -> None : \"\"\" Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Args: batch: The batched data as it is returned by the training DataLoader. \"\"\" on_before_zero_grad def on_before_zero_grad ( self , optimizer : torch . optim . optimizer . Optimizer ) -> None Called after optimizer.step() and before optimizer.zero_grad(). Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: optimizer.step() model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad Args: optimizer: The optimizer for which grads should be zeroed. View Source def on_before_zero_grad ( self , optimizer : Optimizer ) -> None : \"\"\" Called after optimizer.step() and before optimizer.zero_grad(). Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: optimizer.step() model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad Args: optimizer: The optimizer for which grads should be zeroed. \"\"\" on_epoch_end def on_epoch_end ( self ) -> None Called in the training loop at the very end of the epoch. View Source def on_epoch_end ( self ) -> None : \"\"\" Called in the training loop at the very end of the epoch. \"\"\" on_epoch_start def on_epoch_start ( self ) -> None Called in the training loop at the very beginning of the epoch. View Source def on_epoch_start ( self ) -> None : \"\"\" Called in the training loop at the very beginning of the epoch. \"\"\" on_fit_end def on_fit_end ( self ) Called at the very end of fit. If on DDP it is called on every process View Source def on_fit_end ( self ): \"\"\" Called at the very end of fit. If on DDP it is called on every process \"\"\" on_fit_start def on_fit_start ( self ) Called at the very beginning of fit. If on DDP it is called on every process View Source def on_fit_start ( self ): \"\"\" Called at the very beginning of fit. If on DDP it is called on every process \"\"\" on_hpc_load def on_hpc_load ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager loads the model. Args: checkpoint: A dictionary with variables from the checkpoint. View Source def on_hpc_load ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Hook to do whatever you need right before Slurm manager loads the model. Args: checkpoint: A dictionary with variables from the checkpoint. \"\"\" on_hpc_save def on_hpc_save ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager saves the model. Args: checkpoint: A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. View Source def on_hpc_save ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Hook to do whatever you need right before Slurm manager saves the model. Args: checkpoint: A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. \"\"\" on_load_checkpoint def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example: .. code-block:: python def on_load_checkpoint ( self , checkpoint ): # 99% of the time you don't need to implement this method self . something_cool_i_want_to_save = checkpoint [ 'something_cool_i_want_to_save' ] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. View Source def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None : r \" \"\" Called by Lightning to restore your model. If you saved something with :meth:`on_save_checkpoint` this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example: .. code-block:: python def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. \"\" \" on_post_performance_check def on_post_performance_check ( self ) -> None Called at the very end of the validation loop. View Source def on_post_performance_check ( self ) -> None : \"\"\" Called at the very end of the validation loop. \"\"\" on_pre_performance_check def on_pre_performance_check ( self ) -> None Called at the very beginning of the validation loop. View Source def on_pre_performance_check ( self ) -> None : \"\"\" Called at the very beginning of the validation loop. \"\"\" on_sanity_check_start def on_sanity_check_start ( self ) Called before starting evaluation. Warning: Deprecated. Will be removed in v0.9.0. View Source def on_sanity_check_start ( self ): \"\"\" Called before starting evaluation. Warning: Deprecated. Will be removed in v0.9.0. \"\"\" on_save_checkpoint def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: Checkpoint to be saved Example: .. code-block:: python def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. View Source def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None : r \"\"\" Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: Checkpoint to be saved Example: .. code-block:: python def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. \"\"\" on_train_end def on_train_end ( self ) -> None Called at the end of training before logger experiment is closed. View Source def on_train_end ( self ) -> None : \"\"\" Called at the end of training before logger experiment is closed. \"\"\" on_train_start def on_train_start ( self ) -> None Called at the beginning of training before sanity check. View Source def on_train_start ( self ) -> None : \"\"\" Called at the beginning of training before sanity check. \"\"\" optimizer_step def optimizer_step ( self , epoch : int , batch_idx : int , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int , second_order_closure : Union [ Callable , NoneType ] = None , on_tpu : bool = False , using_native_amp : bool = False , using_lbfgs : bool = False ) -> None Override this method to adjust the default way the :class: ~pytorch_lightning.trainer.trainer.Trainer calls each optimizer. By default, Lightning calls step() and zero_grad() as shown in the example once per optimizer. Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers this indexes into that list. second_order_closure: closure for second order methods on_tpu: true if TPU backward is required using_native_amp: True if using native amp using_lbfgs: True if the matching optimizer is lbfgs Examples: .. code-block:: python # DEFAULT def optimizer_step ( self , current_epoch , batch_idx , optimizer , optimizer_idx , second_order_closure , on_tpu , using_native_amp , using_lbfgs ): optimizer . step () # Alternating schedule for optimizer steps ( i . e .: GANs ) def optimizer_step ( self , current_epoch , batch_idx , optimizer , optimizer_idx , second_order_closure , on_tpu , using_native_amp , using_lbfgs ): # update generator opt every 2 steps if optimizer_idx == 0 : if batch_idx % 2 == 0 : optimizer . step () optimizer . zero_grad () # update discriminator opt every 4 steps if optimizer_idx == 1 : if batch_idx % 4 == 0 : optimizer . step () optimizer . zero_grad () # ... # add as many optimizers as you want Here 's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1., float(self.trainer.global_step + 1) / 500.) for pg in optimizer.param_groups: pg[' lr ' ] = lr_scale * self . learning_rate # update params optimizer . step () optimizer . zero_grad () Note: If you also override the :meth: ~pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad model hook don't forget to add the call to it before optimizer.zero_grad() yourself. View Source def optimizer_step ( self , epoch : int , batch_idx : int , optimizer : Optimizer , optimizer_idx : int , second_order_closure : Optional [ Callable ] = None , on_tpu : bool = False , using_native_amp : bool = False , using_lbfgs : bool = False , ) -> None : r \"\"\" Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls each optimizer. By default, Lightning calls ``step()`` and ``zero_grad()`` as shown in the example once per optimizer. Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers this indexes into that list. second_order_closure: closure for second order methods on_tpu: true if TPU backward is required using_native_amp: True if using native amp using_lbfgs: True if the matching optimizer is lbfgs Examples: .. code-block:: python # DEFAULT def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step() # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every 2 steps if optimizer_idx == 0: if batch_idx % 2 == 0 : optimizer.step() optimizer.zero_grad() # update discriminator opt every 4 steps if optimizer_idx == 1: if batch_idx % 4 == 0 : optimizer.step() optimizer.zero_grad() # ... # add as many optimizers as you want Here's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1., float(self.trainer.global_step + 1) / 500.) for pg in optimizer.param_groups: pg['lr'] = lr_scale * self.learning_rate # update params optimizer.step() optimizer.zero_grad() Note: If you also override the :meth:`~pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad` model hook don't forget to add the call to it before ``optimizer.zero_grad()`` yourself. \"\"\" if on_tpu : xm . optimizer_step ( optimizer ) elif using_native_amp : self . trainer . scaler . step ( optimizer ) elif using_lbfgs : optimizer . step ( second_order_closure ) else : optimizer . step () optimizer_zero_grad def optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) View Source def optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : Optimizer , optimizer_idx : int ): optimizer . zero_grad () parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param prepare_data def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data ( self ): # good download_data () tokenize () etc () # bad self . split = data_split self . some_state = some_other_state () In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK = 0 of that node Trainer ( prepare_data_per_node = True ) # call on GLOBAL_RANK = 0 ( great for shared file systems ) Trainer ( prepare_data_per_node = False ) This is called before requesting the dataloaders: .. code-block:: python model . prepare_data () if ddp / tpu : init () model . setup ( stage ) model . train_dataloader () model . val_dataloader () model . test_dataloader () View Source def prepare_data ( self ) -> None : \" \"\" Use this to download and prepare data. .. warning:: DO NOT set state to the model (use `setup` instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): 1. Once per node. This is the default and is only called on LOCAL_RANK=0. 2. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() if ddp/tpu: init() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() \"\" \" print def print ( self , * args , ** kwargs ) -> None Prints only from process 0. Use this in any distributed mode to log only once. Args: args: The thing to print. Will be passed to Python's built-in print function. *kwargs: Will be passed to Python's built-in print function. Example: .. code-block :: python def forward ( self , x ): self . print ( x , 'in forward' ) View Source def print ( self , * args , ** kwargs ) -> None : r \"\"\" Prints only from process 0. Use this in any distributed mode to log only once. Args: *args: The thing to print. Will be passed to Python's built-in print function. **kwargs: Will be passed to Python's built-in print function. Example: .. code-block:: python def forward(self, x): self.print(x, 'in forward') \"\"\" if self . trainer . is_global_zero : print ( * args , ** kwargs ) register_backward_hook def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_parameter def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self save_hyperparameters def save_hyperparameters ( self , * args , frame = None ) -> None Save all model arguments. Args: args: single object of dict , NameSpace or OmegaConf or string names or argumenst from class __init__ from collections import OrderedDict class ManuallyArgsModel(LightningModule): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assine arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... model = ManuallyArgsModel(1, 'abc', 3.14) model.hparams \"arg1\": 1 \"arg3\": 3.14 class AutomaticArgsModel(LightningModule): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, args, *kwargs): ... ... model = AutomaticArgsModel(1, 'abc', 3.14) model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 class SingleArgModel(LightningModule): ... def init (self, params): ... super(). init () ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, args, *kwargs): ... ... model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 View Source def save_hyperparameters ( self , * args , frame = None ) -> None : \"\"\"Save all model arguments. Args: args: single object of `dict`, `NameSpace` or `OmegaConf` or string names or argumenst from class `__init__` >>> from collections import OrderedDict >>> class ManuallyArgsModel(LightningModule): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # manually assine arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(LightningModule): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(LightningModule): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 \"\"\" if not frame : frame = inspect . currentframe () . f_back init_args = get_init_args ( frame ) assert init_args , 'failed to inspect the self init' if not args : hp = init_args self . _hparams_name = 'kwargs' if hp else None else : isx_non_str = [ i for i , arg in enumerate ( args ) if not isinstance ( arg , str )] if len ( isx_non_str ) == 1 : hp = args [ isx_non_str [ 0 ]] cand_names = [ k for k , v in init_args . items () if v == hp ] self . _hparams_name = cand_names [ 0 ] if cand_names else None else : hp = { arg : init_args [ arg ] for arg in args if isinstance ( arg , str )} self . _hparams_name = 'kwargs' # `hparams` are expected here if hp : self . _set_hparams ( hp ) setup def setup ( self , stage : str ) Called at the beginning of fit and test. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Args: stage: either 'fit' or 'test' Example:: class LitModel ( ... ): def __init__ ( self ): self . l1 = None def prepare_data ( self ): download_data () tokenize () # don't do this self . something = else def setup ( stage ): data = Load_data ( ... ) self . l1 = nn . Linear ( 28 , data . num_classes ) View Source def setup ( self , stage : str ): \"\"\" Called at the beginning of fit and test. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Args: stage: either 'fit' or 'test' Example:: class LitModel(...): def __init__(self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) \"\"\" share_memory def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ()) state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination summarize def summarize ( self , mode : str = 'top' ) -> pytorch_lightning . core . memory . ModelSummary View Source def summarize ( self , mode : str = ModelSummary . MODE_DEFAULT ) -> ModelSummary : model_summary = ModelSummary ( self , mode = mode ) log . info ( '\\n' + str ( model_summary )) return model_summary tbptt_split_batch def tbptt_split_batch ( self , batch : torch . Tensor , split_size : int ) -> list When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Args: batch: Current batch split_size: The size of the split Return: List of batch splits. Each split will be passed to :meth: training_step to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples: .. code-block:: python def tbptt_split_batch ( self , batch , split_size ) : splits = [] for t in range ( 0 , time_dims [ 0 ] , split_size ) : batch_split = [] for i , x in enumerate ( batch ) : if isinstance ( x , torch . Tensor ) : split_x = x [ :, t:t + split_size ] elif isinstance ( x , collections . Sequence ) : split_x = [ None ] * len ( x ) for batch_idx in range ( len ( x )) : split_x [ batch_idx ] = x [ batch_idx ][ t:t + split_size ] batch_split . append ( split_x ) splits . append ( batch_split ) return splits Note: Called in the training loop after :meth: ~pytorch_lightning.callbacks.base.Callback.on_batch_start if :paramref: ~pytorch_lightning.trainer.Trainer.truncated_bptt_steps > 0. Each returned batch split is passed separately to :meth: training_step . View Source def tbptt_split_batch ( self , batch : Tensor , split_size : int ) -> list : r \"\"\" When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Args: batch: Current batch split_size: The size of the split Return: List of batch splits. Each split will be passed to :meth:`training_step` to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples: .. code-block:: python def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits Note: Called in the training loop after :meth:`~pytorch_lightning.callbacks.base.Callback.on_batch_start` if :paramref:`~pytorch_lightning.trainer.Trainer.truncated_bptt_steps` > 0. Each returned batch split is passed separately to :meth:`training_step`. \"\"\" time_dims = [ len(x[0 ] ) for x in batch if isinstance ( x , ( torch . Tensor , collections . Sequence )) ] assert len ( time_dims ) >= 1 , \"Unable to determine batch time dimension\" assert all ( x == time_dims [ 0 ] for x in time_dims ), \"Batch time dimension length is ambiguous\" splits = [] for t in range ( 0 , time_dims [ 0 ] , split_size ) : batch_split = [] for i , x in enumerate ( batch ) : if isinstance ( x , torch . Tensor ) : split_x = x [ :, t:t + split_size ] elif isinstance ( x , collections . Sequence ) : split_x = [ None ] * len ( x ) for batch_idx in range ( len ( x )) : split_x [ batch_idx ] = x [ batch_idx ][ t:t + split_size ] batch_split . append ( split_x ) splits . append ( batch_split ) return splits teardown def teardown ( self , stage : str ) Called at the end of fit and test. Args: stage: either 'fit' or 'test' View Source def teardown ( self , stage : str ): \"\"\" Called at the end of fit and test. Args: stage: either 'fit' or 'test' \"\"\" test_dataloader def test_dataloader ( self ) View Source def test_dataloader ( self ): test_loader = DataLoader ( self . test_dataset , batch_size = self . batch_size , shuffle = False , num_workers = cpu_count (), ) return test_loader test_end def test_end ( self , outputs ) Warnings: Deprecated in v0.7.0. Use :meth: test_epoch_end instead. Will be removed in 1.0.0. View Source def test_end ( self , outputs ) : \" \"\" Warnings: Deprecated in v0.7.0. Use :meth:`test_epoch_end` instead. Will be removed in 1.0.0. \"\" \" test_epoch_end def test_epoch_end ( self , outputs ) View Source def test_epoch_end ( self , outputs ) : log_dict = {} for metric_name in outputs [ 0 ] : log_dict [ metric_name ] = torch . stack ( [ x[metric_name ] for x in outputs ] ). mean () return { \"log\" : log_dict , \"progress_bar\" : log_dict , ** log_dict } test_step def test_step ( self , batch , batch_idx ) View Source def test_step ( self , batch , batch_idx ): images , target = batch output = self ( images ) loss_val = F . cross_entropy ( output , target ) metrics_dict = metrics ( \"test\" , output , target ) output = OrderedDict ( { \"test_loss\" : loss_val , ** metrics_dict } ) return output test_step_end def test_step_end ( self , * args , ** kwargs ) -> Dict [ str , torch . Tensor ] Use this when testing with dp or ddp2 because :meth: test_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: test_step for each batch part. Return: Dict or OrderedDict - passed to the :meth: test_epoch_end . Examples: .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2 , this batch is 1 / num_gpus large def test_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) loss = self . softmax ( out ) loss = nce_loss ( loss ) return { 'loss' : loss } # -------------- # with test_step_end to do softmax over the full batch def test_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) return { 'out' : out } def test_step_end ( self , outputs ): # this out is now the full size of the batch out = outputs [ 'out' ] # this softmax now uses the full batch size loss = nce_loss ( loss ) return { 'loss' : loss } See Also: See the :ref: multi-gpu-training guide for more details. View Source def test_step_end ( self , * args , ** kwargs ) -> Dict [ str , Tensor ] : \" \"\" Use this when testing with dp or ddp2 because :meth:`test_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth:`test_step` for each batch part. Return: Dict or OrderedDict - passed to the :meth:`test_epoch_end`. Examples: .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) loss = nce_loss(loss) return {'loss': loss} # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return {'out': out} def test_step_end(self, outputs): # this out is now the full size of the batch out = outputs['out'] # this softmax now uses the full batch size loss = nce_loss(loss) return {'loss': loss} See Also: See the :ref:`multi-gpu-training` guide for more details. \"\" \" tng_dataloader def tng_dataloader ( self ) Warnings: Deprecated in v0.5.0. Use :meth: train_dataloader instead. Will be removed in 1.0.0. View Source def tng_dataloader ( self ) : # todo: remove in v1.0.0 \" \"\" Warnings: Deprecated in v0.5.0. Use :meth:`train_dataloader` instead. Will be removed in 1.0.0. \"\" \" output = self . train_dataloader () rank_zero_warn ( \"`tng_dataloader` has been renamed to `train_dataloader` since v0.5.0.\" \" and this method will be removed in v1.0.0\" , DeprecationWarning ) return output to def to ( self , * args , ** kwargs ) -> torch . nn . modules . module . Module Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note: This method modifies the module in-place. Args: device: the desired device of the parameters and buffers in this module dtype: the desired floating point type of the floating point parameters and buffers in this module tensor: Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module Returns: Module: self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def init (self, weight: torch.Tensor): ... super(). init () ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) -> Module : \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note: This method modifies the module in-place. Args: device: the desired device of the parameters and buffers in this module dtype: the desired floating point type of the floating point parameters and buffers in this module tensor: Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module Returns: Module: self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def __init__(self, weight: torch.Tensor): ... super().__init__() ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) \"\" \" # there is diff nb vars in PT 1.5 out = torch . _C . _nn . _parse_to ( * args , ** kwargs ) device = out [ 0 ] dtype = out [ 1 ] if device is not None : self . _device = device if dtype is not None : self . _dtype = dtype return super (). to ( * args , ** kwargs ) train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self train_dataloader def train_dataloader ( self ) View Source def train_dataloader ( self ): train_loader = DataLoader ( dataset = self . train_dataset , batch_size = self . batch_size , shuffle = True , num_workers = cpu_count (), ) return train_loader training_end def training_end ( self , * args , ** kwargs ) Warnings: Deprecated in v0.7.0. Use :meth: training_step_end instead. View Source def training_end ( self , * args , ** kwargs ) : \" \"\" Warnings: Deprecated in v0.7.0. Use :meth:`training_step_end` instead. \"\" \" training_epoch_end def training_epoch_end ( self , outputs : Union [ List [ Dict [ str , torch . Tensor ]], List [ List [ Dict [ str , torch . Tensor ]]]] ) -> Dict [ str , Dict [ str , torch . Tensor ]] Called at the end of the training epoch with the outputs of all training steps. .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Args: outputs: List of outputs you defined in :meth: training_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: Dict or OrderedDict. May contain the following optional keys: - log (metrics to be added to the logger; only tensors) - progress_bar (dict for progress bar display) - any metric used in a callback (e.g. early stopping). Note: If this method is not overridden, this won't be called. The outputs here are strictly for logging or progress bar. If you don't need to display anything, don't return anything. If you want to manually set current step, you can specify the 'step' key in the 'log' dict. Examples: With a single dataloader: .. code - block :: python def training_epoch_end ( self , outputs ) : train_acc_mean = 0 for output in outputs : train_acc_mean += output [ 'train_acc' ] train_acc_mean /= len ( outputs ) # log training accuracy at the end of an epoch results = { 'log' : { 'train_acc' : train_acc_mean . item () } , 'progress_bar' : { 'train_acc' : train_acc_mean } , } return results With multiple dataloaders , ``outputs`` will be a list of lists . The outer list contains one entry per dataloader , while the inner list contains the individual outputs of each training step for that dataloader . .. code - block :: python def training_epoch_end ( self , outputs ) : train_acc_mean = 0 i = 0 for dataloader_outputs in outputs : for output in dataloader_outputs : train_acc_mean += output [ 'train_acc' ] i += 1 train_acc_mean /= i # log training accuracy at the end of an epoch results = { 'log' : { 'train_acc' : train_acc_mean . item (), 'step' : self . current_epoch } 'progress_bar' : { 'train_acc' : train_acc_mean } , } return results View Source def training_epoch_end ( self , outputs : Union [ List [ Dict [ str , Tensor ]] , List [ List [ Dict [ str , Tensor ]]]] ) -> Dict [ str , Dict [ str , Tensor ]] : \" \"\" Called at the end of the training epoch with the outputs of all training steps. .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Args: outputs: List of outputs you defined in :meth:`training_step`, or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: Dict or OrderedDict. May contain the following optional keys: - log (metrics to be added to the logger; only tensors) - progress_bar (dict for progress bar display) - any metric used in a callback (e.g. early stopping). Note: If this method is not overridden, this won't be called. - The outputs here are strictly for logging or progress bar. - If you don't need to display anything, don't return anything. - If you want to manually set current step, you can specify the 'step' key in the 'log' dict. Examples: With a single dataloader: .. code-block:: python def training_epoch_end(self, outputs): train_acc_mean = 0 for output in outputs: train_acc_mean += output['train_acc'] train_acc_mean /= len(outputs) # log training accuracy at the end of an epoch results = { 'log': {'train_acc': train_acc_mean.item()}, 'progress_bar': {'train_acc': train_acc_mean}, } return results With multiple dataloaders, ``outputs`` will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each training step for that dataloader. .. code-block:: python def training_epoch_end(self, outputs): train_acc_mean = 0 i = 0 for dataloader_outputs in outputs: for output in dataloader_outputs: train_acc_mean += output['train_acc'] i += 1 train_acc_mean /= i # log training accuracy at the end of an epoch results = { 'log': {'train_acc': train_acc_mean.item(), 'step': self.current_epoch} 'progress_bar': {'train_acc': train_acc_mean}, } return results \"\" \" training_step def training_step ( self , batch , batch_idx ) View Source def training_step ( self , batch , batch_idx ): images , target = batch output = self ( images ) loss_val = F . cross_entropy ( output , target ) metrics_dict = metrics ( \"train\" , output , target ) tqdm_dict = { \"train_loss\" : loss_val , ** metrics_dict } output = OrderedDict ( { \"loss\" : loss_val , \"progress_bar\" : tqdm_dict , \"log\" : tqdm_dict } ) return output training_step_end def training_step_end ( self , * args , ** kwargs ) -> Dict [ str , Union [ torch . Tensor , Dict [ str , torch . Tensor ]]] Use this when training with dp or ddp2 because :meth: training_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in training_step for each batch part. Return: Dict with loss key and optional log or progress bar keys. - loss -> tensor scalar **REQUIRED** - progress_bar -> Dict for progress bar display. Must have only tensors - log -> Dict of metrics to add to logger. Must have only tensors (no images, etc) Examples: .. code-block:: python # WITHOUT training_step_end # if used in DP or DDP2 , this batch is 1 / num_gpus large def training_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) loss = self . softmax ( out ) loss = nce_loss ( loss ) return { 'loss' : loss } # -------------- # with training_step_end to do softmax over the full batch def training_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) return { 'out' : out } def training_step_end ( self , outputs ): # this out is now the full size of the batch out = outputs [ 'out' ] # this softmax now uses the full batch size loss = nce_loss ( loss ) return { 'loss' : loss } See Also: See the :ref: multi-gpu-training guide for more details. View Source def training_step_end ( self , * args , ** kwargs ) -> Dict [ str , Union [ Tensor , Dict [ str , Tensor ]] ] : \" \"\" Use this when training with dp or ddp2 because :meth:`training_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in `training_step` for each batch part. Return: Dict with loss key and optional log or progress bar keys. - loss -> tensor scalar **REQUIRED** - progress_bar -> Dict for progress bar display. Must have only tensors - log -> Dict of metrics to add to logger. Must have only tensors (no images, etc) Examples: .. code-block:: python # WITHOUT training_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) loss = nce_loss(loss) return {'loss': loss} # -------------- # with training_step_end to do softmax over the full batch def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return {'out': out} def training_step_end(self, outputs): # this out is now the full size of the batch out = outputs['out'] # this softmax now uses the full batch size loss = nce_loss(loss) return {'loss': loss} See Also: See the :ref:`multi-gpu-training` guide for more details. \"\" \" transfer_batch_to_device def transfer_batch_to_device ( self , batch : Any , device : torch . device ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Example:: def transfer_batch_to_device(self, batch, device) if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) else: batch = super().transfer_batch_to_device(data, device) return batch Args: batch: A batch of data that needs to be transferred to a new device. device: The target device as defined in PyTorch. Returns: A reference to the data on the new device. Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). The :class: ~pytorch_lightning.trainer.trainer.Trainer already takes care of splitting the batch and determines the target devices. See Also: - :func: ~pytorch_lightning.utilities.apply_func.move_data_to_device - :func: ~pytorch_lightning.utilities.apply_func.apply_to_collection View Source def transfer_batch_to_device ( self , batch : Any , device : torch . device ) -> Any : \" \"\" Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: - :class:`torch.Tensor` or anything that implements `.to(...)` - :class:`list` - :class:`dict` - :class:`tuple` - :class:`torchtext.data.batch.Batch` For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Example:: def transfer_batch_to_device(self, batch, device) if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) else: batch = super().transfer_batch_to_device(data, device) return batch Args: batch: A batch of data that needs to be transferred to a new device. device: The target device as defined in PyTorch. Returns: A reference to the data on the new device. Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). The :class:`~pytorch_lightning.trainer.trainer.Trainer` already takes care of splitting the batch and determines the target devices. See Also: - :func:`~pytorch_lightning.utilities.apply_func.move_data_to_device` - :func:`~pytorch_lightning.utilities.apply_func.apply_to_collection` \"\" \" return move_data_to_device ( batch , device ) type def type ( self , dst_type : Union [ str , torch . dtype ] ) -> torch . nn . modules . module . Module Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self , dst_type : Union [ str , torch . dtype ] ) -> Module : \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" self . _dtype = dst_type return super (). type ( dst_type = dst_type ) unfreeze def unfreeze ( self ) -> None Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() View Source def unfreeze ( self ) -> None : \"\"\" Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() \"\"\" for param in self . parameters (): param . requires_grad = True self . train () val_dataloader def val_dataloader ( self ) View Source def val_dataloader ( self ): val_loader = DataLoader ( self . val_dataset , batch_size = self . batch_size , shuffle = False , num_workers = cpu_count () // 2 , ) return val_loader validation_end def validation_end ( self , outputs ) Warnings: Deprecated in v0.7.0. Use :meth: validation_epoch_end instead. Will be removed in 1.0.0. View Source def validation_end ( self , outputs ) : \" \"\" Warnings: Deprecated in v0.7.0. Use :meth:`validation_epoch_end` instead. Will be removed in 1.0.0. \"\" \" validation_epoch_end def validation_epoch_end ( self , outputs ) View Source def validation_epoch_end ( self , outputs ) : log_dict = {} for metric_name in outputs [ 0 ] : log_dict [ metric_name ] = torch . stack ( [ x[metric_name ] for x in outputs ] ). mean () return { \"log\" : log_dict , \"progress_bar\" : log_dict , ** log_dict } validation_step def validation_step ( self , batch , batch_idx ) View Source def validation_step ( self , batch , batch_idx ): images , target = batch output = self ( images ) loss_val = F . cross_entropy ( output , target ) metrics_dict = metrics ( \"val\" , output , target ) output = OrderedDict ( { \"val_loss\" : loss_val , ** metrics_dict } ) return output validation_step_end def validation_step_end ( self , * args , ** kwargs ) -> Dict [ str , torch . Tensor ] Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: validation_step for each batch part. Return: Dict or OrderedDict - passed to the :meth: validation_epoch_end method. Examples: .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2 , this batch is 1 / num_gpus large def validation_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) loss = self . softmax ( out ) loss = nce_loss ( loss ) return { 'loss' : loss } # -------------- # with validation_step_end to do softmax over the full batch def validation_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) return { 'out' : out } def validation_epoch_end ( self , outputs ): # this out is now the full size of the batch out = outputs [ 'out' ] # this softmax now uses the full batch size loss = nce_loss ( loss ) return { 'loss' : loss } See Also: See the :ref: multi-gpu-training guide for more details. View Source def validation_step_end ( self , * args , ** kwargs ) -> Dict [ str , Tensor ] : \" \"\" Use this when validating with dp or ddp2 because :meth:`validation_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth:`validation_step` for each batch part. Return: Dict or OrderedDict - passed to the :meth:`validation_epoch_end` method. Examples: .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) loss = nce_loss(loss) return {'loss': loss} # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return {'out': out} def validation_epoch_end(self, outputs): # this out is now the full size of the batch out = outputs['out'] # this softmax now uses the full batch size loss = nce_loss(loss) return {'loss': loss} See Also: See the :ref:`multi-gpu-training` guide for more details. \"\" \" zero_grad def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ ()","title":"Classifier"},{"location":"reference/sagemaker_defect_detection/classifier/#module-sagemaker_defect_detectionclassifier","text":"View Source # mypy: ignore-errors from typing import Dict import os from collections import OrderedDict from argparse import ArgumentParser , Namespace from multiprocessing import cpu_count import torch import torch.optim as optim import torch.nn.functional as F from torch.utils.data import DataLoader import pytorch_lightning as pl from pytorch_lightning.callbacks import EarlyStopping , ModelCheckpoint import pytorch_lightning.metrics.functional as plm from sagemaker_defect_detection import Classification , NEUCLS , get_transform from sagemaker_defect_detection.utils import load_checkpoint , freeze def metrics ( name : str , out : torch . Tensor , target : torch . Tensor ) -> Dict [ str , torch . Tensor ]: pred = torch . argmax ( out , 1 ) . detach () target = target . detach () metrics = {} metrics [ name + \"_acc\" ] = plm . accuracy ( pred , target ) metrics [ name + \"_prec\" ] = plm . precision ( pred , target ) metrics [ name + \"_recall\" ] = plm . recall ( pred , target ) metrics [ name + \"_f1_score\" ] = plm . recall ( pred , target ) return metrics class DDNClassification ( pl . LightningModule ): def __init__ ( self , data_path : str , backbone : str , freeze_backbone : bool , num_classes : int , learning_rate : float , batch_size : int , momentum : float , weight_decay : float , seed : int , ** kwargs ) -> None : super () . __init__ () self . data_path = data_path self . backbone = backbone self . freeze_backbone = freeze_backbone self . num_classes = num_classes self . learning_rate = learning_rate self . batch_size = batch_size self . momentum = momentum self . weight_decay = weight_decay self . seed = seed self . train_dataset = NEUCLS ( self . data_path , split = \"train\" , transform = get_transform ( \"train\" ), seed = self . seed ) self . val_dataset = NEUCLS ( self . data_path , split = \"val\" , transform = get_transform ( \"val\" ), seed = self . seed ) self . test_dataset = NEUCLS ( self . data_path , split = \"test\" , transform = get_transform ( \"test\" ), seed = self . seed ) self . model = Classification ( self . backbone , self . num_classes ) if self . freeze_backbone : for param in self . model . mfn . backbone . parameters (): param . requires_grad = False def forward ( self , x ): # ignore return self . model ( x ) def training_step ( self , batch , batch_idx ): images , target = batch output = self ( images ) loss_val = F . cross_entropy ( output , target ) metrics_dict = metrics ( \"train\" , output , target ) tqdm_dict = { \"train_loss\" : loss_val , ** metrics_dict } output = OrderedDict ({ \"loss\" : loss_val , \"progress_bar\" : tqdm_dict , \"log\" : tqdm_dict }) return output def validation_step ( self , batch , batch_idx ): images , target = batch output = self ( images ) loss_val = F . cross_entropy ( output , target ) metrics_dict = metrics ( \"val\" , output , target ) output = OrderedDict ({ \"val_loss\" : loss_val , ** metrics_dict }) return output def validation_epoch_end ( self , outputs ): log_dict = {} for metric_name in outputs [ 0 ]: log_dict [ metric_name ] = torch . stack ([ x [ metric_name ] for x in outputs ]) . mean () return { \"log\" : log_dict , \"progress_bar\" : log_dict , ** log_dict } def test_step ( self , batch , batch_idx ): images , target = batch output = self ( images ) loss_val = F . cross_entropy ( output , target ) metrics_dict = metrics ( \"test\" , output , target ) output = OrderedDict ({ \"test_loss\" : loss_val , ** metrics_dict }) return output def test_epoch_end ( self , outputs ): log_dict = {} for metric_name in outputs [ 0 ]: log_dict [ metric_name ] = torch . stack ([ x [ metric_name ] for x in outputs ]) . mean () return { \"log\" : log_dict , \"progress_bar\" : log_dict , ** log_dict } def configure_optimizers ( self ): optimizer = optim . SGD ( self . parameters (), lr = self . learning_rate , momentum = self . momentum , weight_decay = self . weight_decay ) return optimizer def train_dataloader ( self ): train_loader = DataLoader ( dataset = self . train_dataset , batch_size = self . batch_size , shuffle = True , num_workers = cpu_count (), ) return train_loader def val_dataloader ( self ): val_loader = DataLoader ( self . val_dataset , batch_size = self . batch_size , shuffle = False , num_workers = cpu_count () // 2 , ) return val_loader def test_dataloader ( self ): test_loader = DataLoader ( self . test_dataset , batch_size = self . batch_size , shuffle = False , num_workers = cpu_count (), ) return test_loader @staticmethod def add_model_specific_args ( parent_parser ): # pragma: no-cover parser = ArgumentParser ( parents = [ parent_parser ], add_help = False ) aa = parser . add_argument aa ( \"--data-path\" , metavar = \"DIR\" , type = str , default = os . getenv ( \"SM_CHANNEL_TRAINING\" , \"\" ), ) aa ( \"--backbone\" , default = \"resnet34\" , ) aa ( \"--freeze-backbone\" , action = \"store_true\" , ) aa ( \"--num-classes\" , default = 6 , type = int , metavar = \"N\" , ) aa ( \"-b\" , \"--batch-size\" , default = 64 , type = int , metavar = \"N\" , ) aa ( \"--lr\" , \"--learning-rate\" , default = 1e-3 , type = float , metavar = \"LR\" , dest = \"learning_rate\" , ) aa ( \"--momentum\" , default = 0.9 , type = float , metavar = \"M\" , help = \"momentum\" ) aa ( \"--wd\" , \"--weight-decay\" , default = 1e-4 , type = float , metavar = \"W\" , dest = \"weight_decay\" , ) aa ( \"--seed\" , type = int , default = 42 , ) return parser def get_args () -> Namespace : parent_parser = ArgumentParser ( add_help = False ) aa = parent_parser . add_argument aa ( \"--epochs\" , type = int , default = 100 , help = \"number of training epochs\" ) aa ( \"--save-path\" , metavar = \"DIR\" , default = os . getenv ( \"SM_MODEL_DIR\" , \"\" ), type = str , help = \"path to save output\" ) aa ( \"--gpus\" , type = int , default = os . getenv ( \"SM_NUM_GPUS\" , 1 ), help = \"how many gpus\" ) aa ( \"--distributed-backend\" , type = str , default = \"\" , choices = ( \"dp\" , \"ddp\" , \"ddp2\" ), help = \"supports three options dp, ddp, ddp2\" , ) aa ( \"--use-16bit\" , dest = \"use_16bit\" , action = \"store_true\" , help = \"if true uses 16 bit precision\" ) parser = DDNClassification . add_model_specific_args ( parent_parser ) return parser . parse_args () def model_fn ( model_dir ): # TODO: `model_fn` doesn't get more args # see: https://github.com/aws/sagemaker-inference-toolkit/issues/65 backbone = \"resnet34\" num_classes = 6 model = load_checkpoint ( Classification ( backbone , num_classes ), model_dir , prefix = \"model\" ) model = model . eval () freeze ( model ) return model def main ( args : Namespace ) -> None : model = DDNClassification ( ** vars ( args )) if args . seed is not None : pl . seed_everything ( args . seed ) if torch . cuda . device_count () > 1 : torch . cuda . manual_seed_all ( args . seed ) # TODO: add deterministic training # torch.backends.cudnn.deterministic = True checkpoint_callback = ModelCheckpoint ( filepath = os . path . join ( args . save_path , \"{epoch}-{val_loss:.3f}-{val_acc:.3f}\" ), save_top_k = 1 , verbose = True , monitor = \"val_acc\" , mode = \"max\" , ) early_stop_callback = EarlyStopping ( \"val_loss\" , patience = 10 ) trainer = pl . Trainer ( default_root_dir = args . save_path , gpus = args . gpus , max_epochs = args . epochs , early_stop_callback = early_stop_callback , checkpoint_callback = checkpoint_callback , gradient_clip_val = 10 , num_sanity_val_steps = 0 , distributed_backend = args . distributed_backend or None , # precision=16 if args.use_16bit else 32, # TODO: amp apex support ) trainer . fit ( model ) trainer . test () return if __name__ == \"__main__\" : main ( get_args ())","title":"Module sagemaker_defect_detection.classifier"},{"location":"reference/sagemaker_defect_detection/classifier/#functions","text":"","title":"Functions"},{"location":"reference/sagemaker_defect_detection/classifier/#get_args","text":"def get_args ( ) -> argparse . Namespace View Source def get_args () -> Namespace : parent_parser = ArgumentParser ( add_help = False ) aa = parent_parser . add_argument aa ( \"--epochs\" , type = int , default = 100 , help = \"number of training epochs\" ) aa ( \"--save-path\" , metavar = \"DIR\" , default = os . getenv ( \"SM_MODEL_DIR\" , \"\" ), type = str , help = \"path to save output\" ) aa ( \"--gpus\" , type = int , default = os . getenv ( \"SM_NUM_GPUS\" , 1 ), help = \"how many gpus\" ) aa ( \"--distributed-backend\" , type = str , default = \"\" , choices = ( \"dp\" , \"ddp\" , \"ddp2\" ), help = \"supports three options dp, ddp, ddp2\" , ) aa ( \"--use-16bit\" , dest = \"use_16bit\" , action = \"store_true\" , help = \"if true uses 16 bit precision\" ) parser = DDNClassification . add_model_specific_args ( parent_parser ) return parser . parse_args ()","title":"get_args"},{"location":"reference/sagemaker_defect_detection/classifier/#main","text":"def main ( args : argparse . Namespace ) -> None View Source def main ( args : Namespace ) -> None : model = DDNClassification ( ** vars ( args )) if args . seed is not None : pl . seed_everything ( args . seed ) if torch . cuda . device_count () > 1 : torch . cuda . manual_seed_all ( args . seed ) # TODO: add deterministic training # torch.backends.cudnn.deterministic = True checkpoint_callback = ModelCheckpoint ( filepath = os . path . join ( args . save_path , \"{epoch}-{val_loss:.3f}-{val_acc:.3f}\" ), save_top_k = 1 , verbose = True , monitor = \"val_acc\" , mode = \"max\" , ) early_stop_callback = EarlyStopping ( \"val_loss\" , patience = 10 ) trainer = pl . Trainer ( default_root_dir = args . save_path , gpus = args . gpus , max_epochs = args . epochs , early_stop_callback = early_stop_callback , checkpoint_callback = checkpoint_callback , gradient_clip_val = 10 , num_sanity_val_steps = 0 , distributed_backend = args . distributed_backend or None , # precision=16 if args.use_16bit else 32, # TODO: amp apex support ) trainer . fit ( model ) trainer . test () return","title":"main"},{"location":"reference/sagemaker_defect_detection/classifier/#metrics","text":"def metrics ( name : str , out : torch . Tensor , target : torch . Tensor ) -> Dict [ str , torch . Tensor ] View Source def metrics ( name : str , out : torch . Tensor , target : torch . Tensor ) -> Dict [ str , torch . Tensor ]: pred = torch . argmax ( out , 1 ). detach () target = target . detach () metrics = {} metrics [ name + \"_acc\" ] = plm . accuracy ( pred , target ) metrics [ name + \"_prec\" ] = plm . precision ( pred , target ) metrics [ name + \"_recall\" ] = plm . recall ( pred , target ) metrics [ name + \"_f1_score\" ] = plm . recall ( pred , target ) return metrics","title":"metrics"},{"location":"reference/sagemaker_defect_detection/classifier/#model_fn","text":"def model_fn ( model_dir ) View Source def model_fn ( model_dir ): # TODO: `model_fn` doesn't get more args # see: https://github.com/aws/sagemaker-inference-toolkit/issues/65 backbone = \"resnet34\" num_classes = 6 model = load_checkpoint ( Classification ( backbone , num_classes ), model_dir , prefix = \"model\" ) model = model . eval () freeze ( model ) return model","title":"model_fn"},{"location":"reference/sagemaker_defect_detection/classifier/#classes","text":"","title":"Classes"},{"location":"reference/sagemaker_defect_detection/classifier/#ddnclassification","text":"class DDNClassification ( data_path : str , backbone : str , freeze_backbone : bool , num_classes : int , learning_rate : float , batch_size : int , momentum : float , weight_decay : float , seed : int , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance.","title":"DDNClassification"},{"location":"reference/sagemaker_defect_detection/classifier/#ancestors-in-mro","text":"pytorch_lightning.core.lightning.LightningModule abc.ABC pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin pytorch_lightning.core.grads.GradInformation pytorch_lightning.core.saving.ModelIO pytorch_lightning.core.hooks.ModelHooks torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/sagemaker_defect_detection/classifier/#class-variables","text":"CHECKPOINT_HYPER_PARAMS_KEY CHECKPOINT_HYPER_PARAMS_NAME CHECKPOINT_HYPER_PARAMS_TYPE T_destination dump_patches","title":"Class variables"},{"location":"reference/sagemaker_defect_detection/classifier/#static-methods","text":"","title":"Static methods"},{"location":"reference/sagemaker_defect_detection/classifier/#add_model_specific_args","text":"def add_model_specific_args ( parent_parser ) View Source @staticmethod def add_model_specific_args ( parent_parser ) : # pragma : no - cover parser = ArgumentParser ( parents =[ parent_parser ] , add_help = False ) aa = parser . add_argument aa ( \"--data-path\" , metavar = \"DIR\" , type = str , default = os . getenv ( \"SM_CHANNEL_TRAINING\" , \"\" ), ) aa ( \"--backbone\" , default = \"resnet34\" , ) aa ( \"--freeze-backbone\" , action = \"store_true\" , ) aa ( \"--num-classes\" , default = 6 , type = int , metavar = \"N\" , ) aa ( \"-b\" , \"--batch-size\" , default = 64 , type = int , metavar = \"N\" , ) aa ( \"--lr\" , \"--learning-rate\" , default = 1e-3 , type = float , metavar = \"LR\" , dest = \"learning_rate\" , ) aa ( \"--momentum\" , default = 0.9 , type = float , metavar = \"M\" , help = \"momentum\" ) aa ( \"--wd\" , \"--weight-decay\" , default = 1e-4 , type = float , metavar = \"W\" , dest = \"weight_decay\" , ) aa ( \"--seed\" , type = int , default = 42 , ) return parser","title":"add_model_specific_args"},{"location":"reference/sagemaker_defect_detection/classifier/#load_from_checkpoint","text":"def load_from_checkpoint ( checkpoint_path : str , * args , map_location : Union [ Dict [ str , str ], str , torch . device , int , Callable , NoneType ] = None , hparams_file : Union [ str , NoneType ] = None , tags_csv : Union [ str , NoneType ] = None , ** kwargs ) Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to __init__ in the checkpoint under module_arguments Any arguments specified through *args and **kwargs will override args stored in hparams . Args: checkpoint_path: Path to checkpoint. This can also be a URL. args: Any positional args needed to init the model. map_location: If your checkpoint saved a GPU model and you now load on CPUs or a different number of GPUs, use this to map to the new setup. The behaviour is the same as in :func: torch.load . hparams_file: Optional path to a .yaml file with hierarchical structure as in this example:: drop_prob : 0.2 dataloader : batch_size : 32 You most likely won 't need this since Lightning will always save the hyperparameters to the checkpoint. However, if your checkpoint weights don' t have the hyperparameters saved , use this method to pass in a . yaml file with the hparams you 'd like to use. These will be converted into a :class:`~dict` and passed into your :class:`LightningModule` for use. If your model' s `hparams` argument is : class : `~argparse.Namespace` and . yaml file has hierarchical structure , you need to refactor your model to treat `hparams` as : class : `~dict` . . csv files are acceptable here till v0 .9.0 , see tags_csv argument for detailed usage . tags_csv : .. warning :: .. deprecated :: 0.7.6 `tags_csv` argument is deprecated in v0 .7.6 . Will be removed v0 .9.0 . Optional path to a . csv file with two columns ( key , value ) as in this example :: key , value drop_prob , 0.2 batch_size , 32 Use this method to pass in a . csv file with the hparams you 'd like to use. hparam_overrides: A dictionary with keys to override in the hparams kwargs: Any keyword args needed to init the model. Return: :class: LightningModule with loaded weights and hyperparameters (if available). Example: .. code-block:: python # load weights without mapping ... MyLightningModule . load_from_checkpoint ( 'path/to/checkpoint.ckpt' ) # or load weights mapping all weights from GPU 1 to GPU 0 ... map_location = { 'cuda:1' : 'cuda:0' } MyLightningModule . load_from_checkpoint ( 'path/to/checkpoint.ckpt' , map_location = map_location ) # or load weights and hyperparameters from separate files. MyLightningModule . load_from_checkpoint ( 'path/to/checkpoint.ckpt' , hparams_file = '/path/to/hparams_file.yaml' ) # override some of the params with new values MyLightningModule . load_from_checkpoint ( PATH , num_layers = 128 , pretrained_ckpt_path : NEW_PATH , ) # predict pretrained_model . eval () pretrained_model . freeze () y_hat = pretrained_model ( x ) View Source @classmethod def load_from_checkpoint ( cls , checkpoint_path : str , * args , map_location : Optional [ Union [ Dict [ str , str ] , str , torch . device , int , Callable ]] = None , hparams_file : Optional [ str ] = None , tags_csv : Optional [ str ] = None , # backward compatible, todo: remove in v0.9.0 ** kwargs ) : r \" \"\" Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to `__init__` in the checkpoint under `module_arguments` Any arguments specified through \\ *args and \\ * \\ *kwargs will override args stored in `hparams`. Args: checkpoint_path: Path to checkpoint. This can also be a URL. args: Any positional args needed to init the model. map_location: If your checkpoint saved a GPU model and you now load on CPUs or a different number of GPUs, use this to map to the new setup. The behaviour is the same as in :func:`torch.load`. hparams_file: Optional path to a .yaml file with hierarchical structure as in this example:: drop_prob: 0.2 dataloader: batch_size: 32 You most likely won't need this since Lightning will always save the hyperparameters to the checkpoint. However, if your checkpoint weights don't have the hyperparameters saved, use this method to pass in a .yaml file with the hparams you'd like to use. These will be converted into a :class:`~dict` and passed into your :class:`LightningModule` for use. If your model's `hparams` argument is :class:`~argparse.Namespace` and .yaml file has hierarchical structure, you need to refactor your model to treat `hparams` as :class:`~dict`. .csv files are acceptable here till v0.9.0, see tags_csv argument for detailed usage. tags_csv: .. warning:: .. deprecated:: 0.7.6 `tags_csv` argument is deprecated in v0.7.6. Will be removed v0.9.0. Optional path to a .csv file with two columns (key, value) as in this example:: key,value drop_prob,0.2 batch_size,32 Use this method to pass in a .csv file with the hparams you'd like to use. hparam_overrides: A dictionary with keys to override in the hparams kwargs: Any keyword args needed to init the model. Return: :class:`LightningModule` with loaded weights and hyperparameters (if available). Example: .. code-block:: python # load weights without mapping ... MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt') # or load weights mapping all weights from GPU 1 to GPU 0 ... map_location = {'cuda:1':'cuda:0'} MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', map_location=map_location ) # or load weights and hyperparameters from separate files. MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', hparams_file='/path/to/hparams_file.yaml' ) # override some of the params with new values MyLightningModule.load_from_checkpoint( PATH, num_layers=128, pretrained_ckpt_path: NEW_PATH, ) # predict pretrained_model.eval() pretrained_model.freeze() y_hat = pretrained_model(x) \"\" \" if map_location is not None : checkpoint = pl_load ( checkpoint_path , map_location = map_location ) else : checkpoint = pl_load ( checkpoint_path , map_location = lambda storage , loc : storage ) # add the hparams from csv file to checkpoint if tags_csv is not None : hparams_file = tags_csv rank_zero_warn ( '`tags_csv` argument is deprecated in v0.7.6. Will be removed v0.9.0' , DeprecationWarning ) if hparams_file is not None : extension = hparams_file . split ( '.' ) [ - 1 ] if extension . lower () in ( 'csv' ) : hparams = load_hparams_from_tags_csv ( hparams_file ) elif extension . lower () in ( 'yml' , 'yaml' ) : hparams = load_hparams_from_yaml ( hparams_file ) else : raise ValueError ( '.csv, .yml or .yaml is required for `hparams_file`' ) hparams [ 'on_gpu' ] = False # overwrite hparams by the given file checkpoint [ cls . CHECKPOINT_HYPER_PARAMS_KEY ] = hparams # for past checkpoint need to add the new key if cls . CHECKPOINT_HYPER_PARAMS_KEY not in checkpoint : checkpoint [ cls . CHECKPOINT_HYPER_PARAMS_KEY ] = {} # override the hparams with values that were passed in checkpoint [ cls . CHECKPOINT_HYPER_PARAMS_KEY ] . update ( kwargs ) model = cls . _load_model_state ( checkpoint , * args , ** kwargs ) return model","title":"load_from_checkpoint"},{"location":"reference/sagemaker_defect_detection/classifier/#load_from_metrics","text":"def load_from_metrics ( weights_path , tags_csv , map_location = None ) Warning: Deprecated in version 0.7.0. You should use :meth: load_from_checkpoint instead. Will be removed in v0.9.0. View Source @classmethod def load_from_metrics ( cls , weights_path , tags_csv , map_location = None ) : r \" \"\" Warning: Deprecated in version 0.7.0. You should use :meth:`load_from_checkpoint` instead. Will be removed in v0.9.0. \"\" \" rank_zero_warn ( \"`load_from_metrics` method has been unified with `load_from_checkpoint` in v0.7.0.\" \" The deprecated method will be removed in v0.9.0.\" , DeprecationWarning ) return cls . load_from_checkpoint ( weights_path , tags_csv = tags_csv , map_location = map_location )","title":"load_from_metrics"},{"location":"reference/sagemaker_defect_detection/classifier/#instance-variables","text":"device dtype example_input_array hparams on_gpu True if your model is currently running on GPUs. Useful to set flags around the LightningModule for different CPU vs GPU behavior.","title":"Instance variables"},{"location":"reference/sagemaker_defect_detection/classifier/#methods","text":"","title":"Methods"},{"location":"reference/sagemaker_defect_detection/classifier/#add_module","text":"def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module","title":"add_module"},{"location":"reference/sagemaker_defect_detection/classifier/#amp_scale_loss","text":"def amp_scale_loss ( self , unscaled_loss , optimizer , optimizer_idx ) View Source def amp_scale_loss ( self , unscaled_loss , optimizer , optimizer_idx ): if NATIVE_AMP_AVALAIBLE : scaled_loss = self . trainer . scaler . scale ( unscaled_loss ) else : scaled_loss = amp . scale_loss ( unscaled_loss , optimizer ) return scaled_loss","title":"amp_scale_loss"},{"location":"reference/sagemaker_defect_detection/classifier/#apply","text":"def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self","title":"apply"},{"location":"reference/sagemaker_defect_detection/classifier/#backward","text":"def backward ( self , trainer , loss : torch . Tensor , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) -> None Override backward with your own implementation if you need to. Args: trainer: Pointer to the trainer loss: Loss is already scaled by accumulated grads optimizer: Current optimizer being used optimizer_idx: Index of the current optimizer being used Called to perform backward step. Feel free to override as needed. The loss passed in has already been scaled for accumulated gradients if requested. Example:: def backward(self, trainer, loss, optimizer, optimizer_idx): loss.backward() View Source def backward ( self , trainer , loss : Tensor , optimizer : Optimizer , optimizer_idx : int ) -> None : \"\"\" Override backward with your own implementation if you need to. Args: trainer: Pointer to the trainer loss: Loss is already scaled by accumulated grads optimizer: Current optimizer being used optimizer_idx: Index of the current optimizer being used Called to perform backward step. Feel free to override as needed. The loss passed in has already been scaled for accumulated gradients if requested. Example:: def backward(self, trainer, loss, optimizer, optimizer_idx): loss.backward() \"\"\" loss . backward ()","title":"backward"},{"location":"reference/sagemaker_defect_detection/classifier/#bfloat16","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t )","title":"bfloat16"},{"location":"reference/sagemaker_defect_detection/classifier/#buffers","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf","title":"buffers"},{"location":"reference/sagemaker_defect_detection/classifier/#children","text":"def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module","title":"children"},{"location":"reference/sagemaker_defect_detection/classifier/#configure_apex","text":"def configure_apex ( self , amp : object , model : 'LightningModule' , optimizers : List [ torch . optim . optimizer . Optimizer ], amp_level : str ) -> Tuple [ _ForwardRef ( 'LightningModule' ), List [ torch . optim . optimizer . Optimizer ]] Override to init AMP your own way. Must return a model and list of optimizers. Args: amp: pointer to amp library object. model: pointer to current :class: LightningModule . optimizers: list of optimizers passed in :meth: configure_optimizers . amp_level: AMP mode chosen ('O1', 'O2', etc...) Return: Apex wrapped model and optimizers Examples: .. code-block:: python # Default implementation used by Trainer . def configure_apex ( self , amp , model , optimizers , amp_level ): model , optimizers = amp . initialize ( model , optimizers , opt_level = amp_level , ) return model , optimizers View Source def configure_apex ( self , amp : object , model : 'LightningModule' , optimizers : List [ Optimizer ] , amp_level : str ) -> Tuple [ 'LightningModule' , List [ Optimizer ]] : r \" \"\" Override to init AMP your own way. Must return a model and list of optimizers. Args: amp: pointer to amp library object. model: pointer to current :class:`LightningModule`. optimizers: list of optimizers passed in :meth:`configure_optimizers`. amp_level: AMP mode chosen ('O1', 'O2', etc...) Return: Apex wrapped model and optimizers Examples: .. code-block:: python # Default implementation used by Trainer. def configure_apex(self, amp, model, optimizers, amp_level): model, optimizers = amp.initialize( model, optimizers, opt_level=amp_level, ) return model, optimizers \"\" \" model , optimizers = amp . initialize ( model , optimizers , opt_level = amp_level ) return model , optimizers","title":"configure_apex"},{"location":"reference/sagemaker_defect_detection/classifier/#configure_ddp","text":"def configure_ddp ( self , model : 'LightningModule' , device_ids : List [ int ] ) -> torch . nn . parallel . distributed . DistributedDataParallel Override to init DDP in your own way or with your own wrapper. The only requirements are that: On a validation batch the call goes to model.validation_step . On a training batch the call goes to model.training_step . On a testing batch, the call goes to model.test_step .+ Args: model: the :class: LightningModule currently being optimized. device_ids: the list of GPU ids. Return: DDP wrapped model Examples: .. code-block:: python # default implementation used in Trainer def configure_ddp(self, model, device_ids): # Lightning DDP simply routes to test_step, val_step, etc... model = LightningDistributedDataParallel( model, device_ids=device_ids, find_unused_parameters=True ) return model View Source def configure_ddp ( self , model : 'LightningModule' , device_ids : List [ int ] ) -> DistributedDataParallel : r \" \"\" Override to init DDP in your own way or with your own wrapper. The only requirements are that: 1. On a validation batch the call goes to ``model.validation_step``. 2. On a training batch the call goes to ``model.training_step``. 3. On a testing batch, the call goes to ``model.test_step``.+ Args: model: the :class:`LightningModule` currently being optimized. device_ids: the list of GPU ids. Return: DDP wrapped model Examples: .. code-block:: python # default implementation used in Trainer def configure_ddp(self, model, device_ids): # Lightning DDP simply routes to test_step, val_step, etc... model = LightningDistributedDataParallel( model, device_ids=device_ids, find_unused_parameters=True ) return model \"\" \" model = LightningDistributedDataParallel ( model , device_ids = device_ids , find_unused_parameters = True ) return model","title":"configure_ddp"},{"location":"reference/sagemaker_defect_detection/classifier/#configure_optimizers","text":"def configure_optimizers ( self ) View Source def configure_optimizers ( self ): optimizer = optim . SGD ( self . parameters (), lr = self . learning_rate , momentum = self . momentum , weight_decay = self . weight_decay ) return optimizer","title":"configure_optimizers"},{"location":"reference/sagemaker_defect_detection/classifier/#cpu","text":"def cpu ( self ) -> torch . nn . modules . module . Module Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self ) -> Module : \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" self . _device = torch . device ( 'cpu') return super (). cpu ()","title":"cpu"},{"location":"reference/sagemaker_defect_detection/classifier/#cuda","text":"def cuda ( self , device : Union [ int , NoneType ] = None ) -> torch . nn . modules . module . Module Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device: if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self , device : Optional [ int ] = None ) -> Module : \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device: if specified, all parameters will be copied to that device Returns: Module: self \"\"\" self . _device = torch . device ( 'cuda' , index = device ) return super (). cuda ( device = device )","title":"cuda"},{"location":"reference/sagemaker_defect_detection/classifier/#double","text":"def double ( self ) -> torch . nn . modules . module . Module Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self ) -> Module : \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" self . _dtype = torch . double return super (). double ()","title":"double"},{"location":"reference/sagemaker_defect_detection/classifier/#eval","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False )","title":"eval"},{"location":"reference/sagemaker_defect_detection/classifier/#extra_repr","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return ''","title":"extra_repr"},{"location":"reference/sagemaker_defect_detection/classifier/#float","text":"def float ( self ) -> torch . nn . modules . module . Module Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self ) -> Module : \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" self . _dtype = torch . float return super (). float ()","title":"float"},{"location":"reference/sagemaker_defect_detection/classifier/#forward","text":"def forward ( self , x ) View Source def forward ( self , x ): # ignore return self . model ( x )","title":"forward"},{"location":"reference/sagemaker_defect_detection/classifier/#freeze","text":"def freeze ( self ) -> None Freeze all params for inference. Example: .. code-block:: python model = MyLightningModule(...) model.freeze() View Source def freeze ( self ) -> None : r \"\"\" Freeze all params for inference. Example: .. code-block:: python model = MyLightningModule(...) model.freeze() \"\"\" for param in self . parameters (): param . requires_grad = False self . eval ()","title":"freeze"},{"location":"reference/sagemaker_defect_detection/classifier/#get_progress_bar_dict","text":"def get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]] Additional items to be displayed in the progress bar. Return: Dictionary with the items to be displayed in the progress bar. View Source def get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]]: r \"\"\" Additional items to be displayed in the progress bar. Return: Dictionary with the items to be displayed in the progress bar. \"\"\" # call . item () only once but store elements without graphs running_train_loss = self . trainer . running_loss . mean () avg_training_loss = running_train_loss . cpu (). item () if running_train_loss is not None else float ( 'NaN' ) tqdm_dict = { 'loss' : '{:.3f}' . format ( avg_training_loss ) } if self . trainer . truncated_bptt_steps is not None : tqdm_dict [ 'split_idx' ] = self . trainer . split_idx if self . trainer . logger is not None and self . trainer . logger . version is not None : tqdm_dict [ 'v_num' ] = self . trainer . logger . version return tqdm_dict","title":"get_progress_bar_dict"},{"location":"reference/sagemaker_defect_detection/classifier/#get_tqdm_dict","text":"def get_tqdm_dict ( self ) -> Dict [ str , Union [ int , str ]] Additional items to be displayed in the progress bar. Return: Dictionary with the items to be displayed in the progress bar. Warning: Deprecated since v0.7.3. Use :meth: get_progress_bar_dict instead. View Source def get_tqdm_dict ( self ) -> Dict [ str , Union [ int , str ]] : \" \"\" Additional items to be displayed in the progress bar. Return: Dictionary with the items to be displayed in the progress bar. Warning: Deprecated since v0.7.3. Use :meth:`get_progress_bar_dict` instead. \"\" \" rank_zero_warn ( \"`get_tqdm_dict` was renamed to `get_progress_bar_dict` in v0.7.3\" \" and this method will be removed in v1.0.0\" , DeprecationWarning ) return self . get_progress_bar_dict ()","title":"get_tqdm_dict"},{"location":"reference/sagemaker_defect_detection/classifier/#grad_norm","text":"def grad_norm ( self , norm_type : Union [ float , int , str ] ) -> Dict [ str , float ] Compute each parameter's gradient's norm and their overall norm. The overall norm is computed over all gradients together, as if they were concatenated into a single vector. Args: norm_type: The type of the used p-norm, cast to float if necessary. Can be 'inf' for infinity norm. Return: norms: The dictionary of p-norms of each parameter's gradient and a special entry for the total p-norm of the gradients viewed as a single vector. View Source def grad_norm ( self , norm_type : Union [ float , int , str ]) -> Dict [ str , float ]: \"\"\"Compute each parameter's gradient's norm and their overall norm. The overall norm is computed over all gradients together, as if they were concatenated into a single vector. Args: norm_type: The type of the used p-norm, cast to float if necessary. Can be ``'inf'`` for infinity norm. Return: norms: The dictionary of p-norms of each parameter's gradient and a special entry for the total p-norm of the gradients viewed as a single vector. \"\"\" norm_type = float ( norm_type ) norms , all_norms = {} , [] for name , p in self . named_parameters (): if p . grad is None : continue param_norm = float ( p . grad . data . norm ( norm_type )) norms [ f 'grad_{norm_type}_norm_{name}' ] = round ( param_norm , 3 ) all_norms . append ( param_norm ) total_norm = float ( torch . tensor ( all_norms ). norm ( norm_type )) norms [ f 'grad_{norm_type}_norm_total' ] = round ( total_norm , 3 ) return norms","title":"grad_norm"},{"location":"reference/sagemaker_defect_detection/classifier/#half","text":"def half ( self ) -> torch . nn . modules . module . Module Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self ) -> Module : \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" self . _dtype = torch . half return super (). half ()","title":"half"},{"location":"reference/sagemaker_defect_detection/classifier/#init_ddp_connection","text":"def init_ddp_connection ( self , global_rank : int , world_size : int , is_slurm_managing_tasks : bool = True ) -> None Override to define your custom way of setting up a distributed environment. Lightning's implementation uses env:// init by default and sets the first node as root for SLURM managed cluster. Args: global_rank: The global process idx. world_size: Number of GPUs being use across all nodes. (num_nodes * num_gpus). is_slurm_managing_tasks: is cluster managed by SLURM. View Source def init_ddp_connection ( self , global_rank : int , world_size : int , is_slurm_managing_tasks : bool = True ) -> None : \"\"\" Override to define your custom way of setting up a distributed environment. Lightning's implementation uses env:// init by default and sets the first node as root for SLURM managed cluster. Args: global_rank: The global process idx. world_size: Number of GPUs being use across all nodes. (num_nodes * num_gpus). is_slurm_managing_tasks: is cluster managed by SLURM. \"\"\" if is_slurm_managing_tasks : self . _init_slurm_connection () if 'MASTER_ADDR' not in os . environ : rank_zero_warn ( \"MASTER_ADDR environment variable is not defined. Set as localhost\" ) os . environ [ 'MASTER_ADDR' ] = '127.0.0.1' log . debug ( f \"MASTER_ADDR: {os.environ['MASTER_ADDR']}\" ) if 'MASTER_PORT' not in os . environ : rank_zero_warn ( \"MASTER_PORT environment variable is not defined. Set as 12910\" ) os . environ [ 'MASTER_PORT' ] = '12910' log . debug ( f \"MASTER_PORT: {os.environ['MASTER_PORT']}\" ) if 'WORLD_SIZE' in os . environ and int ( os . environ [ 'WORLD_SIZE' ]) != world_size : rank_zero_warn ( f \"WORLD_SIZE environment variable ({os.environ['WORLD_SIZE']}) \" f \"is not equal to the computed world size ({world_size}). Ignored.\" ) torch_backend = \"nccl\" if self . trainer . on_gpu else \"gloo\" log . info ( f \"initializing ddp: GLOBAL_RANK: {global_rank}, MEMBER: {global_rank+1}/{world_size}\" ) torch_distrib . init_process_group ( torch_backend , rank = global_rank , world_size = world_size )","title":"init_ddp_connection"},{"location":"reference/sagemaker_defect_detection/classifier/#load_state_dict","text":"def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys )","title":"load_state_dict"},{"location":"reference/sagemaker_defect_detection/classifier/#modules","text":"def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module","title":"modules"},{"location":"reference/sagemaker_defect_detection/classifier/#named_buffers","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_buffers"},{"location":"reference/sagemaker_defect_detection/classifier/#named_children","text":"def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module","title":"named_children"},{"location":"reference/sagemaker_defect_detection/classifier/#named_modules","text":"def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m","title":"named_modules"},{"location":"reference/sagemaker_defect_detection/classifier/#named_parameters","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_parameters"},{"location":"reference/sagemaker_defect_detection/classifier/#on_after_backward","text":"def on_after_backward ( self ) -> None Called in the training loop after loss.backward() and before optimizers do anything. This is the ideal place to inspect or log gradient information. Example:: def on_after_backward(self): # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge params = self.state_dict() for k, v in params.items(): grads = v name = k self.logger.experiment.add_histogram(tag=name, values=grads, global_step=self.trainer.global_step) View Source def on_after_backward ( self ) -> None : \"\"\" Called in the training loop after loss.backward() and before optimizers do anything. This is the ideal place to inspect or log gradient information. Example:: def on_after_backward(self): # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge params = self.state_dict() for k, v in params.items(): grads = v name = k self.logger.experiment.add_histogram(tag=name, values=grads, global_step=self.trainer.global_step) \"\"\"","title":"on_after_backward"},{"location":"reference/sagemaker_defect_detection/classifier/#on_batch_end","text":"def on_batch_end ( self ) -> None Called in the training loop after the batch. View Source def on_batch_end ( self ) -> None : \"\"\" Called in the training loop after the batch. \"\"\"","title":"on_batch_end"},{"location":"reference/sagemaker_defect_detection/classifier/#on_batch_start","text":"def on_batch_start ( self , batch : Any ) -> None Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Args: batch: The batched data as it is returned by the training DataLoader. View Source def on_batch_start ( self , batch : Any ) -> None : \"\"\" Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Args: batch: The batched data as it is returned by the training DataLoader. \"\"\"","title":"on_batch_start"},{"location":"reference/sagemaker_defect_detection/classifier/#on_before_zero_grad","text":"def on_before_zero_grad ( self , optimizer : torch . optim . optimizer . Optimizer ) -> None Called after optimizer.step() and before optimizer.zero_grad(). Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: optimizer.step() model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad Args: optimizer: The optimizer for which grads should be zeroed. View Source def on_before_zero_grad ( self , optimizer : Optimizer ) -> None : \"\"\" Called after optimizer.step() and before optimizer.zero_grad(). Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: optimizer.step() model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad Args: optimizer: The optimizer for which grads should be zeroed. \"\"\"","title":"on_before_zero_grad"},{"location":"reference/sagemaker_defect_detection/classifier/#on_epoch_end","text":"def on_epoch_end ( self ) -> None Called in the training loop at the very end of the epoch. View Source def on_epoch_end ( self ) -> None : \"\"\" Called in the training loop at the very end of the epoch. \"\"\"","title":"on_epoch_end"},{"location":"reference/sagemaker_defect_detection/classifier/#on_epoch_start","text":"def on_epoch_start ( self ) -> None Called in the training loop at the very beginning of the epoch. View Source def on_epoch_start ( self ) -> None : \"\"\" Called in the training loop at the very beginning of the epoch. \"\"\"","title":"on_epoch_start"},{"location":"reference/sagemaker_defect_detection/classifier/#on_fit_end","text":"def on_fit_end ( self ) Called at the very end of fit. If on DDP it is called on every process View Source def on_fit_end ( self ): \"\"\" Called at the very end of fit. If on DDP it is called on every process \"\"\"","title":"on_fit_end"},{"location":"reference/sagemaker_defect_detection/classifier/#on_fit_start","text":"def on_fit_start ( self ) Called at the very beginning of fit. If on DDP it is called on every process View Source def on_fit_start ( self ): \"\"\" Called at the very beginning of fit. If on DDP it is called on every process \"\"\"","title":"on_fit_start"},{"location":"reference/sagemaker_defect_detection/classifier/#on_hpc_load","text":"def on_hpc_load ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager loads the model. Args: checkpoint: A dictionary with variables from the checkpoint. View Source def on_hpc_load ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Hook to do whatever you need right before Slurm manager loads the model. Args: checkpoint: A dictionary with variables from the checkpoint. \"\"\"","title":"on_hpc_load"},{"location":"reference/sagemaker_defect_detection/classifier/#on_hpc_save","text":"def on_hpc_save ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager saves the model. Args: checkpoint: A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. View Source def on_hpc_save ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Hook to do whatever you need right before Slurm manager saves the model. Args: checkpoint: A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. \"\"\"","title":"on_hpc_save"},{"location":"reference/sagemaker_defect_detection/classifier/#on_load_checkpoint","text":"def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example: .. code-block:: python def on_load_checkpoint ( self , checkpoint ): # 99% of the time you don't need to implement this method self . something_cool_i_want_to_save = checkpoint [ 'something_cool_i_want_to_save' ] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. View Source def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None : r \" \"\" Called by Lightning to restore your model. If you saved something with :meth:`on_save_checkpoint` this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example: .. code-block:: python def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. \"\" \"","title":"on_load_checkpoint"},{"location":"reference/sagemaker_defect_detection/classifier/#on_post_performance_check","text":"def on_post_performance_check ( self ) -> None Called at the very end of the validation loop. View Source def on_post_performance_check ( self ) -> None : \"\"\" Called at the very end of the validation loop. \"\"\"","title":"on_post_performance_check"},{"location":"reference/sagemaker_defect_detection/classifier/#on_pre_performance_check","text":"def on_pre_performance_check ( self ) -> None Called at the very beginning of the validation loop. View Source def on_pre_performance_check ( self ) -> None : \"\"\" Called at the very beginning of the validation loop. \"\"\"","title":"on_pre_performance_check"},{"location":"reference/sagemaker_defect_detection/classifier/#on_sanity_check_start","text":"def on_sanity_check_start ( self ) Called before starting evaluation. Warning: Deprecated. Will be removed in v0.9.0. View Source def on_sanity_check_start ( self ): \"\"\" Called before starting evaluation. Warning: Deprecated. Will be removed in v0.9.0. \"\"\"","title":"on_sanity_check_start"},{"location":"reference/sagemaker_defect_detection/classifier/#on_save_checkpoint","text":"def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: Checkpoint to be saved Example: .. code-block:: python def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. View Source def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None : r \"\"\" Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: Checkpoint to be saved Example: .. code-block:: python def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. \"\"\"","title":"on_save_checkpoint"},{"location":"reference/sagemaker_defect_detection/classifier/#on_train_end","text":"def on_train_end ( self ) -> None Called at the end of training before logger experiment is closed. View Source def on_train_end ( self ) -> None : \"\"\" Called at the end of training before logger experiment is closed. \"\"\"","title":"on_train_end"},{"location":"reference/sagemaker_defect_detection/classifier/#on_train_start","text":"def on_train_start ( self ) -> None Called at the beginning of training before sanity check. View Source def on_train_start ( self ) -> None : \"\"\" Called at the beginning of training before sanity check. \"\"\"","title":"on_train_start"},{"location":"reference/sagemaker_defect_detection/classifier/#optimizer_step","text":"def optimizer_step ( self , epoch : int , batch_idx : int , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int , second_order_closure : Union [ Callable , NoneType ] = None , on_tpu : bool = False , using_native_amp : bool = False , using_lbfgs : bool = False ) -> None Override this method to adjust the default way the :class: ~pytorch_lightning.trainer.trainer.Trainer calls each optimizer. By default, Lightning calls step() and zero_grad() as shown in the example once per optimizer. Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers this indexes into that list. second_order_closure: closure for second order methods on_tpu: true if TPU backward is required using_native_amp: True if using native amp using_lbfgs: True if the matching optimizer is lbfgs Examples: .. code-block:: python # DEFAULT def optimizer_step ( self , current_epoch , batch_idx , optimizer , optimizer_idx , second_order_closure , on_tpu , using_native_amp , using_lbfgs ): optimizer . step () # Alternating schedule for optimizer steps ( i . e .: GANs ) def optimizer_step ( self , current_epoch , batch_idx , optimizer , optimizer_idx , second_order_closure , on_tpu , using_native_amp , using_lbfgs ): # update generator opt every 2 steps if optimizer_idx == 0 : if batch_idx % 2 == 0 : optimizer . step () optimizer . zero_grad () # update discriminator opt every 4 steps if optimizer_idx == 1 : if batch_idx % 4 == 0 : optimizer . step () optimizer . zero_grad () # ... # add as many optimizers as you want Here 's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1., float(self.trainer.global_step + 1) / 500.) for pg in optimizer.param_groups: pg[' lr ' ] = lr_scale * self . learning_rate # update params optimizer . step () optimizer . zero_grad () Note: If you also override the :meth: ~pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad model hook don't forget to add the call to it before optimizer.zero_grad() yourself. View Source def optimizer_step ( self , epoch : int , batch_idx : int , optimizer : Optimizer , optimizer_idx : int , second_order_closure : Optional [ Callable ] = None , on_tpu : bool = False , using_native_amp : bool = False , using_lbfgs : bool = False , ) -> None : r \"\"\" Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls each optimizer. By default, Lightning calls ``step()`` and ``zero_grad()`` as shown in the example once per optimizer. Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers this indexes into that list. second_order_closure: closure for second order methods on_tpu: true if TPU backward is required using_native_amp: True if using native amp using_lbfgs: True if the matching optimizer is lbfgs Examples: .. code-block:: python # DEFAULT def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step() # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every 2 steps if optimizer_idx == 0: if batch_idx % 2 == 0 : optimizer.step() optimizer.zero_grad() # update discriminator opt every 4 steps if optimizer_idx == 1: if batch_idx % 4 == 0 : optimizer.step() optimizer.zero_grad() # ... # add as many optimizers as you want Here's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1., float(self.trainer.global_step + 1) / 500.) for pg in optimizer.param_groups: pg['lr'] = lr_scale * self.learning_rate # update params optimizer.step() optimizer.zero_grad() Note: If you also override the :meth:`~pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad` model hook don't forget to add the call to it before ``optimizer.zero_grad()`` yourself. \"\"\" if on_tpu : xm . optimizer_step ( optimizer ) elif using_native_amp : self . trainer . scaler . step ( optimizer ) elif using_lbfgs : optimizer . step ( second_order_closure ) else : optimizer . step ()","title":"optimizer_step"},{"location":"reference/sagemaker_defect_detection/classifier/#optimizer_zero_grad","text":"def optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) View Source def optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : Optimizer , optimizer_idx : int ): optimizer . zero_grad ()","title":"optimizer_zero_grad"},{"location":"reference/sagemaker_defect_detection/classifier/#parameters","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param","title":"parameters"},{"location":"reference/sagemaker_defect_detection/classifier/#prepare_data","text":"def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data ( self ): # good download_data () tokenize () etc () # bad self . split = data_split self . some_state = some_other_state () In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK = 0 of that node Trainer ( prepare_data_per_node = True ) # call on GLOBAL_RANK = 0 ( great for shared file systems ) Trainer ( prepare_data_per_node = False ) This is called before requesting the dataloaders: .. code-block:: python model . prepare_data () if ddp / tpu : init () model . setup ( stage ) model . train_dataloader () model . val_dataloader () model . test_dataloader () View Source def prepare_data ( self ) -> None : \" \"\" Use this to download and prepare data. .. warning:: DO NOT set state to the model (use `setup` instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): 1. Once per node. This is the default and is only called on LOCAL_RANK=0. 2. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() if ddp/tpu: init() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() \"\" \"","title":"prepare_data"},{"location":"reference/sagemaker_defect_detection/classifier/#print","text":"def print ( self , * args , ** kwargs ) -> None Prints only from process 0. Use this in any distributed mode to log only once. Args: args: The thing to print. Will be passed to Python's built-in print function. *kwargs: Will be passed to Python's built-in print function. Example: .. code-block :: python def forward ( self , x ): self . print ( x , 'in forward' ) View Source def print ( self , * args , ** kwargs ) -> None : r \"\"\" Prints only from process 0. Use this in any distributed mode to log only once. Args: *args: The thing to print. Will be passed to Python's built-in print function. **kwargs: Will be passed to Python's built-in print function. Example: .. code-block:: python def forward(self, x): self.print(x, 'in forward') \"\"\" if self . trainer . is_global_zero : print ( * args , ** kwargs )","title":"print"},{"location":"reference/sagemaker_defect_detection/classifier/#register_backward_hook","text":"def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle","title":"register_backward_hook"},{"location":"reference/sagemaker_defect_detection/classifier/#register_buffer","text":"def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name )","title":"register_buffer"},{"location":"reference/sagemaker_defect_detection/classifier/#register_forward_hook","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle","title":"register_forward_hook"},{"location":"reference/sagemaker_defect_detection/classifier/#register_forward_pre_hook","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle","title":"register_forward_pre_hook"},{"location":"reference/sagemaker_defect_detection/classifier/#register_parameter","text":"def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param","title":"register_parameter"},{"location":"reference/sagemaker_defect_detection/classifier/#requires_grad_","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self","title":"requires_grad_"},{"location":"reference/sagemaker_defect_detection/classifier/#save_hyperparameters","text":"def save_hyperparameters ( self , * args , frame = None ) -> None Save all model arguments. Args: args: single object of dict , NameSpace or OmegaConf or string names or argumenst from class __init__ from collections import OrderedDict class ManuallyArgsModel(LightningModule): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assine arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... model = ManuallyArgsModel(1, 'abc', 3.14) model.hparams \"arg1\": 1 \"arg3\": 3.14 class AutomaticArgsModel(LightningModule): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, args, *kwargs): ... ... model = AutomaticArgsModel(1, 'abc', 3.14) model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 class SingleArgModel(LightningModule): ... def init (self, params): ... super(). init () ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, args, *kwargs): ... ... model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 View Source def save_hyperparameters ( self , * args , frame = None ) -> None : \"\"\"Save all model arguments. Args: args: single object of `dict`, `NameSpace` or `OmegaConf` or string names or argumenst from class `__init__` >>> from collections import OrderedDict >>> class ManuallyArgsModel(LightningModule): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # manually assine arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(LightningModule): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(LightningModule): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 \"\"\" if not frame : frame = inspect . currentframe () . f_back init_args = get_init_args ( frame ) assert init_args , 'failed to inspect the self init' if not args : hp = init_args self . _hparams_name = 'kwargs' if hp else None else : isx_non_str = [ i for i , arg in enumerate ( args ) if not isinstance ( arg , str )] if len ( isx_non_str ) == 1 : hp = args [ isx_non_str [ 0 ]] cand_names = [ k for k , v in init_args . items () if v == hp ] self . _hparams_name = cand_names [ 0 ] if cand_names else None else : hp = { arg : init_args [ arg ] for arg in args if isinstance ( arg , str )} self . _hparams_name = 'kwargs' # `hparams` are expected here if hp : self . _set_hparams ( hp )","title":"save_hyperparameters"},{"location":"reference/sagemaker_defect_detection/classifier/#setup","text":"def setup ( self , stage : str ) Called at the beginning of fit and test. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Args: stage: either 'fit' or 'test' Example:: class LitModel ( ... ): def __init__ ( self ): self . l1 = None def prepare_data ( self ): download_data () tokenize () # don't do this self . something = else def setup ( stage ): data = Load_data ( ... ) self . l1 = nn . Linear ( 28 , data . num_classes ) View Source def setup ( self , stage : str ): \"\"\" Called at the beginning of fit and test. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Args: stage: either 'fit' or 'test' Example:: class LitModel(...): def __init__(self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) \"\"\"","title":"setup"},{"location":"reference/sagemaker_defect_detection/classifier/#share_memory","text":"def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ())","title":"share_memory"},{"location":"reference/sagemaker_defect_detection/classifier/#state_dict","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination","title":"state_dict"},{"location":"reference/sagemaker_defect_detection/classifier/#summarize","text":"def summarize ( self , mode : str = 'top' ) -> pytorch_lightning . core . memory . ModelSummary View Source def summarize ( self , mode : str = ModelSummary . MODE_DEFAULT ) -> ModelSummary : model_summary = ModelSummary ( self , mode = mode ) log . info ( '\\n' + str ( model_summary )) return model_summary","title":"summarize"},{"location":"reference/sagemaker_defect_detection/classifier/#tbptt_split_batch","text":"def tbptt_split_batch ( self , batch : torch . Tensor , split_size : int ) -> list When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Args: batch: Current batch split_size: The size of the split Return: List of batch splits. Each split will be passed to :meth: training_step to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples: .. code-block:: python def tbptt_split_batch ( self , batch , split_size ) : splits = [] for t in range ( 0 , time_dims [ 0 ] , split_size ) : batch_split = [] for i , x in enumerate ( batch ) : if isinstance ( x , torch . Tensor ) : split_x = x [ :, t:t + split_size ] elif isinstance ( x , collections . Sequence ) : split_x = [ None ] * len ( x ) for batch_idx in range ( len ( x )) : split_x [ batch_idx ] = x [ batch_idx ][ t:t + split_size ] batch_split . append ( split_x ) splits . append ( batch_split ) return splits Note: Called in the training loop after :meth: ~pytorch_lightning.callbacks.base.Callback.on_batch_start if :paramref: ~pytorch_lightning.trainer.Trainer.truncated_bptt_steps > 0. Each returned batch split is passed separately to :meth: training_step . View Source def tbptt_split_batch ( self , batch : Tensor , split_size : int ) -> list : r \"\"\" When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Args: batch: Current batch split_size: The size of the split Return: List of batch splits. Each split will be passed to :meth:`training_step` to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples: .. code-block:: python def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits Note: Called in the training loop after :meth:`~pytorch_lightning.callbacks.base.Callback.on_batch_start` if :paramref:`~pytorch_lightning.trainer.Trainer.truncated_bptt_steps` > 0. Each returned batch split is passed separately to :meth:`training_step`. \"\"\" time_dims = [ len(x[0 ] ) for x in batch if isinstance ( x , ( torch . Tensor , collections . Sequence )) ] assert len ( time_dims ) >= 1 , \"Unable to determine batch time dimension\" assert all ( x == time_dims [ 0 ] for x in time_dims ), \"Batch time dimension length is ambiguous\" splits = [] for t in range ( 0 , time_dims [ 0 ] , split_size ) : batch_split = [] for i , x in enumerate ( batch ) : if isinstance ( x , torch . Tensor ) : split_x = x [ :, t:t + split_size ] elif isinstance ( x , collections . Sequence ) : split_x = [ None ] * len ( x ) for batch_idx in range ( len ( x )) : split_x [ batch_idx ] = x [ batch_idx ][ t:t + split_size ] batch_split . append ( split_x ) splits . append ( batch_split ) return splits","title":"tbptt_split_batch"},{"location":"reference/sagemaker_defect_detection/classifier/#teardown","text":"def teardown ( self , stage : str ) Called at the end of fit and test. Args: stage: either 'fit' or 'test' View Source def teardown ( self , stage : str ): \"\"\" Called at the end of fit and test. Args: stage: either 'fit' or 'test' \"\"\"","title":"teardown"},{"location":"reference/sagemaker_defect_detection/classifier/#test_dataloader","text":"def test_dataloader ( self ) View Source def test_dataloader ( self ): test_loader = DataLoader ( self . test_dataset , batch_size = self . batch_size , shuffle = False , num_workers = cpu_count (), ) return test_loader","title":"test_dataloader"},{"location":"reference/sagemaker_defect_detection/classifier/#test_end","text":"def test_end ( self , outputs ) Warnings: Deprecated in v0.7.0. Use :meth: test_epoch_end instead. Will be removed in 1.0.0. View Source def test_end ( self , outputs ) : \" \"\" Warnings: Deprecated in v0.7.0. Use :meth:`test_epoch_end` instead. Will be removed in 1.0.0. \"\" \"","title":"test_end"},{"location":"reference/sagemaker_defect_detection/classifier/#test_epoch_end","text":"def test_epoch_end ( self , outputs ) View Source def test_epoch_end ( self , outputs ) : log_dict = {} for metric_name in outputs [ 0 ] : log_dict [ metric_name ] = torch . stack ( [ x[metric_name ] for x in outputs ] ). mean () return { \"log\" : log_dict , \"progress_bar\" : log_dict , ** log_dict }","title":"test_epoch_end"},{"location":"reference/sagemaker_defect_detection/classifier/#test_step","text":"def test_step ( self , batch , batch_idx ) View Source def test_step ( self , batch , batch_idx ): images , target = batch output = self ( images ) loss_val = F . cross_entropy ( output , target ) metrics_dict = metrics ( \"test\" , output , target ) output = OrderedDict ( { \"test_loss\" : loss_val , ** metrics_dict } ) return output","title":"test_step"},{"location":"reference/sagemaker_defect_detection/classifier/#test_step_end","text":"def test_step_end ( self , * args , ** kwargs ) -> Dict [ str , torch . Tensor ] Use this when testing with dp or ddp2 because :meth: test_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: test_step for each batch part. Return: Dict or OrderedDict - passed to the :meth: test_epoch_end . Examples: .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2 , this batch is 1 / num_gpus large def test_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) loss = self . softmax ( out ) loss = nce_loss ( loss ) return { 'loss' : loss } # -------------- # with test_step_end to do softmax over the full batch def test_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) return { 'out' : out } def test_step_end ( self , outputs ): # this out is now the full size of the batch out = outputs [ 'out' ] # this softmax now uses the full batch size loss = nce_loss ( loss ) return { 'loss' : loss } See Also: See the :ref: multi-gpu-training guide for more details. View Source def test_step_end ( self , * args , ** kwargs ) -> Dict [ str , Tensor ] : \" \"\" Use this when testing with dp or ddp2 because :meth:`test_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth:`test_step` for each batch part. Return: Dict or OrderedDict - passed to the :meth:`test_epoch_end`. Examples: .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) loss = nce_loss(loss) return {'loss': loss} # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return {'out': out} def test_step_end(self, outputs): # this out is now the full size of the batch out = outputs['out'] # this softmax now uses the full batch size loss = nce_loss(loss) return {'loss': loss} See Also: See the :ref:`multi-gpu-training` guide for more details. \"\" \"","title":"test_step_end"},{"location":"reference/sagemaker_defect_detection/classifier/#tng_dataloader","text":"def tng_dataloader ( self ) Warnings: Deprecated in v0.5.0. Use :meth: train_dataloader instead. Will be removed in 1.0.0. View Source def tng_dataloader ( self ) : # todo: remove in v1.0.0 \" \"\" Warnings: Deprecated in v0.5.0. Use :meth:`train_dataloader` instead. Will be removed in 1.0.0. \"\" \" output = self . train_dataloader () rank_zero_warn ( \"`tng_dataloader` has been renamed to `train_dataloader` since v0.5.0.\" \" and this method will be removed in v1.0.0\" , DeprecationWarning ) return output","title":"tng_dataloader"},{"location":"reference/sagemaker_defect_detection/classifier/#to","text":"def to ( self , * args , ** kwargs ) -> torch . nn . modules . module . Module Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note: This method modifies the module in-place. Args: device: the desired device of the parameters and buffers in this module dtype: the desired floating point type of the floating point parameters and buffers in this module tensor: Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module Returns: Module: self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def init (self, weight: torch.Tensor): ... super(). init () ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) -> Module : \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note: This method modifies the module in-place. Args: device: the desired device of the parameters and buffers in this module dtype: the desired floating point type of the floating point parameters and buffers in this module tensor: Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module Returns: Module: self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def __init__(self, weight: torch.Tensor): ... super().__init__() ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) \"\" \" # there is diff nb vars in PT 1.5 out = torch . _C . _nn . _parse_to ( * args , ** kwargs ) device = out [ 0 ] dtype = out [ 1 ] if device is not None : self . _device = device if dtype is not None : self . _dtype = dtype return super (). to ( * args , ** kwargs )","title":"to"},{"location":"reference/sagemaker_defect_detection/classifier/#train","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self","title":"train"},{"location":"reference/sagemaker_defect_detection/classifier/#train_dataloader","text":"def train_dataloader ( self ) View Source def train_dataloader ( self ): train_loader = DataLoader ( dataset = self . train_dataset , batch_size = self . batch_size , shuffle = True , num_workers = cpu_count (), ) return train_loader","title":"train_dataloader"},{"location":"reference/sagemaker_defect_detection/classifier/#training_end","text":"def training_end ( self , * args , ** kwargs ) Warnings: Deprecated in v0.7.0. Use :meth: training_step_end instead. View Source def training_end ( self , * args , ** kwargs ) : \" \"\" Warnings: Deprecated in v0.7.0. Use :meth:`training_step_end` instead. \"\" \"","title":"training_end"},{"location":"reference/sagemaker_defect_detection/classifier/#training_epoch_end","text":"def training_epoch_end ( self , outputs : Union [ List [ Dict [ str , torch . Tensor ]], List [ List [ Dict [ str , torch . Tensor ]]]] ) -> Dict [ str , Dict [ str , torch . Tensor ]] Called at the end of the training epoch with the outputs of all training steps. .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Args: outputs: List of outputs you defined in :meth: training_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: Dict or OrderedDict. May contain the following optional keys: - log (metrics to be added to the logger; only tensors) - progress_bar (dict for progress bar display) - any metric used in a callback (e.g. early stopping). Note: If this method is not overridden, this won't be called. The outputs here are strictly for logging or progress bar. If you don't need to display anything, don't return anything. If you want to manually set current step, you can specify the 'step' key in the 'log' dict. Examples: With a single dataloader: .. code - block :: python def training_epoch_end ( self , outputs ) : train_acc_mean = 0 for output in outputs : train_acc_mean += output [ 'train_acc' ] train_acc_mean /= len ( outputs ) # log training accuracy at the end of an epoch results = { 'log' : { 'train_acc' : train_acc_mean . item () } , 'progress_bar' : { 'train_acc' : train_acc_mean } , } return results With multiple dataloaders , ``outputs`` will be a list of lists . The outer list contains one entry per dataloader , while the inner list contains the individual outputs of each training step for that dataloader . .. code - block :: python def training_epoch_end ( self , outputs ) : train_acc_mean = 0 i = 0 for dataloader_outputs in outputs : for output in dataloader_outputs : train_acc_mean += output [ 'train_acc' ] i += 1 train_acc_mean /= i # log training accuracy at the end of an epoch results = { 'log' : { 'train_acc' : train_acc_mean . item (), 'step' : self . current_epoch } 'progress_bar' : { 'train_acc' : train_acc_mean } , } return results View Source def training_epoch_end ( self , outputs : Union [ List [ Dict [ str , Tensor ]] , List [ List [ Dict [ str , Tensor ]]]] ) -> Dict [ str , Dict [ str , Tensor ]] : \" \"\" Called at the end of the training epoch with the outputs of all training steps. .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Args: outputs: List of outputs you defined in :meth:`training_step`, or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: Dict or OrderedDict. May contain the following optional keys: - log (metrics to be added to the logger; only tensors) - progress_bar (dict for progress bar display) - any metric used in a callback (e.g. early stopping). Note: If this method is not overridden, this won't be called. - The outputs here are strictly for logging or progress bar. - If you don't need to display anything, don't return anything. - If you want to manually set current step, you can specify the 'step' key in the 'log' dict. Examples: With a single dataloader: .. code-block:: python def training_epoch_end(self, outputs): train_acc_mean = 0 for output in outputs: train_acc_mean += output['train_acc'] train_acc_mean /= len(outputs) # log training accuracy at the end of an epoch results = { 'log': {'train_acc': train_acc_mean.item()}, 'progress_bar': {'train_acc': train_acc_mean}, } return results With multiple dataloaders, ``outputs`` will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each training step for that dataloader. .. code-block:: python def training_epoch_end(self, outputs): train_acc_mean = 0 i = 0 for dataloader_outputs in outputs: for output in dataloader_outputs: train_acc_mean += output['train_acc'] i += 1 train_acc_mean /= i # log training accuracy at the end of an epoch results = { 'log': {'train_acc': train_acc_mean.item(), 'step': self.current_epoch} 'progress_bar': {'train_acc': train_acc_mean}, } return results \"\" \"","title":"training_epoch_end"},{"location":"reference/sagemaker_defect_detection/classifier/#training_step","text":"def training_step ( self , batch , batch_idx ) View Source def training_step ( self , batch , batch_idx ): images , target = batch output = self ( images ) loss_val = F . cross_entropy ( output , target ) metrics_dict = metrics ( \"train\" , output , target ) tqdm_dict = { \"train_loss\" : loss_val , ** metrics_dict } output = OrderedDict ( { \"loss\" : loss_val , \"progress_bar\" : tqdm_dict , \"log\" : tqdm_dict } ) return output","title":"training_step"},{"location":"reference/sagemaker_defect_detection/classifier/#training_step_end","text":"def training_step_end ( self , * args , ** kwargs ) -> Dict [ str , Union [ torch . Tensor , Dict [ str , torch . Tensor ]]] Use this when training with dp or ddp2 because :meth: training_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in training_step for each batch part. Return: Dict with loss key and optional log or progress bar keys. - loss -> tensor scalar **REQUIRED** - progress_bar -> Dict for progress bar display. Must have only tensors - log -> Dict of metrics to add to logger. Must have only tensors (no images, etc) Examples: .. code-block:: python # WITHOUT training_step_end # if used in DP or DDP2 , this batch is 1 / num_gpus large def training_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) loss = self . softmax ( out ) loss = nce_loss ( loss ) return { 'loss' : loss } # -------------- # with training_step_end to do softmax over the full batch def training_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) return { 'out' : out } def training_step_end ( self , outputs ): # this out is now the full size of the batch out = outputs [ 'out' ] # this softmax now uses the full batch size loss = nce_loss ( loss ) return { 'loss' : loss } See Also: See the :ref: multi-gpu-training guide for more details. View Source def training_step_end ( self , * args , ** kwargs ) -> Dict [ str , Union [ Tensor , Dict [ str , Tensor ]] ] : \" \"\" Use this when training with dp or ddp2 because :meth:`training_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in `training_step` for each batch part. Return: Dict with loss key and optional log or progress bar keys. - loss -> tensor scalar **REQUIRED** - progress_bar -> Dict for progress bar display. Must have only tensors - log -> Dict of metrics to add to logger. Must have only tensors (no images, etc) Examples: .. code-block:: python # WITHOUT training_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) loss = nce_loss(loss) return {'loss': loss} # -------------- # with training_step_end to do softmax over the full batch def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return {'out': out} def training_step_end(self, outputs): # this out is now the full size of the batch out = outputs['out'] # this softmax now uses the full batch size loss = nce_loss(loss) return {'loss': loss} See Also: See the :ref:`multi-gpu-training` guide for more details. \"\" \"","title":"training_step_end"},{"location":"reference/sagemaker_defect_detection/classifier/#transfer_batch_to_device","text":"def transfer_batch_to_device ( self , batch : Any , device : torch . device ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Example:: def transfer_batch_to_device(self, batch, device) if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) else: batch = super().transfer_batch_to_device(data, device) return batch Args: batch: A batch of data that needs to be transferred to a new device. device: The target device as defined in PyTorch. Returns: A reference to the data on the new device. Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). The :class: ~pytorch_lightning.trainer.trainer.Trainer already takes care of splitting the batch and determines the target devices. See Also: - :func: ~pytorch_lightning.utilities.apply_func.move_data_to_device - :func: ~pytorch_lightning.utilities.apply_func.apply_to_collection View Source def transfer_batch_to_device ( self , batch : Any , device : torch . device ) -> Any : \" \"\" Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: - :class:`torch.Tensor` or anything that implements `.to(...)` - :class:`list` - :class:`dict` - :class:`tuple` - :class:`torchtext.data.batch.Batch` For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Example:: def transfer_batch_to_device(self, batch, device) if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) else: batch = super().transfer_batch_to_device(data, device) return batch Args: batch: A batch of data that needs to be transferred to a new device. device: The target device as defined in PyTorch. Returns: A reference to the data on the new device. Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). The :class:`~pytorch_lightning.trainer.trainer.Trainer` already takes care of splitting the batch and determines the target devices. See Also: - :func:`~pytorch_lightning.utilities.apply_func.move_data_to_device` - :func:`~pytorch_lightning.utilities.apply_func.apply_to_collection` \"\" \" return move_data_to_device ( batch , device )","title":"transfer_batch_to_device"},{"location":"reference/sagemaker_defect_detection/classifier/#type","text":"def type ( self , dst_type : Union [ str , torch . dtype ] ) -> torch . nn . modules . module . Module Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self , dst_type : Union [ str , torch . dtype ] ) -> Module : \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" self . _dtype = dst_type return super (). type ( dst_type = dst_type )","title":"type"},{"location":"reference/sagemaker_defect_detection/classifier/#unfreeze","text":"def unfreeze ( self ) -> None Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() View Source def unfreeze ( self ) -> None : \"\"\" Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() \"\"\" for param in self . parameters (): param . requires_grad = True self . train ()","title":"unfreeze"},{"location":"reference/sagemaker_defect_detection/classifier/#val_dataloader","text":"def val_dataloader ( self ) View Source def val_dataloader ( self ): val_loader = DataLoader ( self . val_dataset , batch_size = self . batch_size , shuffle = False , num_workers = cpu_count () // 2 , ) return val_loader","title":"val_dataloader"},{"location":"reference/sagemaker_defect_detection/classifier/#validation_end","text":"def validation_end ( self , outputs ) Warnings: Deprecated in v0.7.0. Use :meth: validation_epoch_end instead. Will be removed in 1.0.0. View Source def validation_end ( self , outputs ) : \" \"\" Warnings: Deprecated in v0.7.0. Use :meth:`validation_epoch_end` instead. Will be removed in 1.0.0. \"\" \"","title":"validation_end"},{"location":"reference/sagemaker_defect_detection/classifier/#validation_epoch_end","text":"def validation_epoch_end ( self , outputs ) View Source def validation_epoch_end ( self , outputs ) : log_dict = {} for metric_name in outputs [ 0 ] : log_dict [ metric_name ] = torch . stack ( [ x[metric_name ] for x in outputs ] ). mean () return { \"log\" : log_dict , \"progress_bar\" : log_dict , ** log_dict }","title":"validation_epoch_end"},{"location":"reference/sagemaker_defect_detection/classifier/#validation_step","text":"def validation_step ( self , batch , batch_idx ) View Source def validation_step ( self , batch , batch_idx ): images , target = batch output = self ( images ) loss_val = F . cross_entropy ( output , target ) metrics_dict = metrics ( \"val\" , output , target ) output = OrderedDict ( { \"val_loss\" : loss_val , ** metrics_dict } ) return output","title":"validation_step"},{"location":"reference/sagemaker_defect_detection/classifier/#validation_step_end","text":"def validation_step_end ( self , * args , ** kwargs ) -> Dict [ str , torch . Tensor ] Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: validation_step for each batch part. Return: Dict or OrderedDict - passed to the :meth: validation_epoch_end method. Examples: .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2 , this batch is 1 / num_gpus large def validation_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) loss = self . softmax ( out ) loss = nce_loss ( loss ) return { 'loss' : loss } # -------------- # with validation_step_end to do softmax over the full batch def validation_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) return { 'out' : out } def validation_epoch_end ( self , outputs ): # this out is now the full size of the batch out = outputs [ 'out' ] # this softmax now uses the full batch size loss = nce_loss ( loss ) return { 'loss' : loss } See Also: See the :ref: multi-gpu-training guide for more details. View Source def validation_step_end ( self , * args , ** kwargs ) -> Dict [ str , Tensor ] : \" \"\" Use this when validating with dp or ddp2 because :meth:`validation_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth:`validation_step` for each batch part. Return: Dict or OrderedDict - passed to the :meth:`validation_epoch_end` method. Examples: .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) loss = nce_loss(loss) return {'loss': loss} # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return {'out': out} def validation_epoch_end(self, outputs): # this out is now the full size of the batch out = outputs['out'] # this softmax now uses the full batch size loss = nce_loss(loss) return {'loss': loss} See Also: See the :ref:`multi-gpu-training` guide for more details. \"\" \"","title":"validation_step_end"},{"location":"reference/sagemaker_defect_detection/classifier/#zero_grad","text":"def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ ()","title":"zero_grad"},{"location":"reference/sagemaker_defect_detection/detector/","text":"Module sagemaker_defect_detection.detector View Source # mypy: ignore-errors from typing import Optional from pathlib import Path from os import path as osp from collections import OrderedDict from argparse import ArgumentParser , Namespace from multiprocessing import cpu_count import os import math import sys import numpy as np import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler from torch.utils.data import DataLoader from torchvision.models.detection.image_list import ImageList import pytorch_lightning as pl from pytorch_lightning.core.decorators import auto_move_data from pytorch_lightning.callbacks import EarlyStopping , ModelCheckpoint from sagemaker_defect_detection import Detection , NEUDET , Classification , RPN , RoI , get_augmentation , get_preprocess from sagemaker_defect_detection.utils.coco_eval import CocoEvaluator from sagemaker_defect_detection.utils.coco_utils import convert_to_coco_api from sagemaker_defect_detection.utils import freeze , load_checkpoint class DDNDetection ( pl . LightningModule ): def __init__ ( self , train_rpn : bool , train_roi : bool , finetune_rpn : bool , finetune_roi : bool , data_path : str , backbone : str , num_classes : int , learning_rate : float , batch_size : int , momentum : float , weight_decay : float , seed : int , pretrained_mfn_ckpt : Optional [ str ] = None , pretrained_rpn_ckpt : Optional [ str ] = None , pretrained_roi_ckpt : Optional [ str ] = None , finetuned_rpn_ckpt : Optional [ str ] = None , finetuned_roi_ckpt : Optional [ str ] = None , resume_sagemaker_from_checkpoint : Optional [ str ] = None , ** kwargs , ) -> None : super () . __init__ () self . train_rpn = train_rpn self . train_roi = train_roi self . finetune_rpn = finetune_rpn self . finetune_roi = finetune_roi self . data_path = data_path self . backbone = backbone self . num_classes = num_classes self . learning_rate = learning_rate self . batch_size = batch_size self . momentum = momentum self . weight_decay = weight_decay self . seed = seed self . train_dataset = NEUDET ( self . data_path , split = \"train\" , augmentation = get_augmentation ( \"train\" ), preprocess = get_preprocess (), seed = self . seed , ) self . val_dataset = NEUDET ( self . data_path , split = \"val\" , augmentation = get_augmentation ( \"val\" ), preprocess = get_preprocess (), seed = self . seed , ) self . pretrained_mfn_ckpt = pretrained_mfn_ckpt self . pretrained_rpn_ckpt = pretrained_rpn_ckpt self . pretrained_roi_ckpt = pretrained_roi_ckpt self . finetuned_rpn_ckpt = finetuned_rpn_ckpt self . finetuned_roi_ckpt = finetuned_roi_ckpt self . resume_sagemaker_from_checkpoint = resume_sagemaker_from_checkpoint self . coco_evaluator = self . _get_evaluator ( self . val_dataset ) def setup ( self , stage ) -> None : if self . train_rpn : # step 2 self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . pretrained_mfn_ckpt , \"model.mfn\" ) self . rpn = RPN () elif self . train_roi : # step 3 self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . pretrained_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . pretrained_rpn_ckpt , prefix = \"rpn\" ) freeze ( self . rpn ) self . roi = RoI ( self . num_classes ) elif self . finetune_rpn : # step 4 or extra finetune rpn if self . finetuned_rpn_ckpt and self . finetuned_roi_ckpt : # extra finetune rpn self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . finetuned_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . finetuned_rpn_ckpt , prefix = \"rpn\" ) self . roi = load_checkpoint ( RoI ( self . num_classes ), self . finetuned_roi_ckpt , prefix = \"roi\" ) freeze ( self . roi ) self . model = Detection ( self . mfn , self . rpn , self . roi ) else : self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . pretrained_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . pretrained_rpn_ckpt , prefix = \"rpn\" ) self . roi = load_checkpoint ( RoI ( self . num_classes ), self . pretrained_roi_ckpt , prefix = \"roi\" ) freeze ( self . roi ) self . model = Detection ( self . mfn , self . rpn , self . roi ) elif self . finetune_roi : # step 5 or extra finetune roi if self . finetuned_rpn_ckpt and self . finetuned_roi_ckpt : # extra finetune roi self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . finetuned_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . finetuned_rpn_ckpt , prefix = \"rpn\" ) freeze ( self . rpn ) self . roi = load_checkpoint ( RoI ( self . num_classes ), self . finetuned_roi_ckpt , prefix = \"roi\" ) self . model = Detection ( self . mfn , self . rpn , self . roi ) else : self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . finetuned_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . finetuned_rpn_ckpt , prefix = \"rpn\" ) freeze ( self . rpn ) self . roi = load_checkpoint ( RoI ( self . num_classes ), self . pretrained_roi_ckpt , prefix = \"roi\" ) self . model = Detection ( self . mfn , self . rpn , self . roi ) else : # step 6: final/joint model load_checkpoint_fn = load_checkpoint if self . finetuned_roi_ckpt is not None : ckpt_path = self . finetuned_rpn_ckpt elif self . resume_sagemaker_from_checkpoint is not None : ckpt_path = self . resume_sagemaker_from_checkpoint else : ckpt_path = None # ignore load_checkpoint load_checkpoint_fn = lambda * args : args [ 0 ] self . mfn = load_checkpoint_fn ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , ckpt_path , \"mfn\" ) self . rpn = load_checkpoint_fn ( RPN (), ckpt_path , \"rpn\" ) self . roi = load_checkpoint_fn ( RoI ( self . num_classes ), ckpt_path , \"roi\" ) self . model = Detection ( self . mfn , self . rpn , self . roi ) return @auto_move_data def forward ( self , images , * args , ** kwargs ): if self . train_rpn : # step 2 images = torch . stack ( images ) features = self . mfn ( images ) features = OrderedDict ({ str ( i ): t . unsqueeze ( 0 ) for i , t in enumerate ( features )}) images = ImageList ( images , [( 224 , 224 )]) return self . rpn ( images , features , targets = kwargs . get ( \"targets\" )) elif self . train_roi : # step 3 self . mfn . eval () self . rpn . eval () images = torch . stack ( images ) features = self . mfn ( images ) features = OrderedDict ({ str ( i ): t . unsqueeze ( 0 ) for i , t in enumerate ( features )}) images = ImageList ( images , [( 224 , 224 )]) proposals , _ = self . rpn ( images , features , targets = None ) return self . roi ( features , proposals , [( 224 , 224 )], targets = kwargs . get ( \"targets\" )) elif self . finetune_rpn : self . model . backbone . eval () self . model . roi_heads . eval () return self . model ( images , targets = kwargs . get ( \"targets\" )) elif self . finetune_roi : self . model . backbone . eval () self . model . rpn . eval () return self . model ( images , targets = kwargs . get ( \"targets\" )) else : return self . model ( images , targets = kwargs . get ( \"targets\" )) def _get_evaluator ( self , dataset ): coco = convert_to_coco_api ( dataset ) return CocoEvaluator ( coco , [ \"bbox\" ]) def train_dataloader ( self ): train_loader = DataLoader ( dataset = self . train_dataset , batch_size = self . batch_size , collate_fn = self . train_dataset . collate_fn , shuffle = True , num_workers = cpu_count (), ) return train_loader def val_dataloader ( self ): val_loader = DataLoader ( self . val_dataset , batch_size = self . batch_size , collate_fn = self . val_dataset . collate_fn , shuffle = False , num_workers = cpu_count () // 2 , ) self . coco_evaluator = self . _get_evaluator ( val_loader . dataset ) return val_loader def configure_optimizers ( self ): params = [ p for p in self . parameters () if p . requires_grad ] optimizer = optim . SGD ( params , lr = self . learning_rate , momentum = self . momentum , weight_decay = self . weight_decay ) return optimizer def training_step ( self , batch , batch_idx ): images , targets , _ = batch if self . train_rpn : targets = [{ \"boxes\" : t [ \"boxes\" ]} for t in targets ] _ , loss_dict = self ( images , targets = targets ) loss = sum ( loss for loss in loss_dict . values ()) return OrderedDict ({ \"loss\" : loss , \"progress_bar\" : loss_dict , \"log\" : loss_dict }) elif self . train_roi : _ , loss_dict = self ( images , targets = targets ) loss = sum ( loss for loss in loss_dict . values ()) return OrderedDict ({ \"loss\" : loss , \"progress_bar\" : loss_dict , \"log\" : loss_dict }) else : images = list ( image for image in images ) targets = [{ k : v for k , v in t . items ()} for t in targets ] loss_dict = self ( images , targets = targets ) # loss keys: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg'] loss = sum ( loss for loss in loss_dict . values ()) if not math . isfinite ( loss . item ()): sys . exit ( 1 ) return OrderedDict ({ \"loss\" : loss , \"progress_bar\" : loss_dict , \"log\" : loss_dict }) @auto_move_data def validation_step ( self , batch , batch_idx ): images , targets , _ = batch if self . train_rpn : # rpn doesn't compute loss for val return {} elif self . train_roi : # TODO: scores are predictions scores, not a metric! iou? + acc? return {} else : images = list ( image for image in images ) targets = [{ k : v for k , v in t . items ()} for t in targets ] outputs = self ( images , targets = targets ) ret = { target [ \"image_id\" ] . item (): output for target , output in zip ( targets , outputs )} self . coco_evaluator . update ( ret ) return {} @auto_move_data def validation_epoch_end ( self , outputs ): if self . train_rpn : return {} elif self . train_roi : # TODO: above return {} else : self . coco_evaluator . synchronize_between_processes () self . coco_evaluator . accumulate () self . coco_evaluator . summarize () metric = self . coco_evaluator . coco_eval [ \"bbox\" ] . stats [ 0 ] metric = torch . as_tensor ( metric ) tensorboard_logs = { \"main_score\" : metric } self . coco_evaluator = self . _get_evaluator ( self . val_dataset ) # need to update for the new evaluation return { \"val_loss\" : metric , \"log\" : tensorboard_logs , \"progress_bar\" : tensorboard_logs } @staticmethod def add_model_specific_args ( parent_parser ): # pragma: no-cover parser = ArgumentParser ( parents = [ parent_parser ], add_help = False ) aa = parser . add_argument aa ( \"--train-rpn\" , action = \"store_true\" ) aa ( \"--train-roi\" , action = \"store_true\" ) aa ( \"--finetune-rpn\" , action = \"store_true\" ) aa ( \"--finetune-roi\" , action = \"store_true\" ) aa ( \"--data-path\" , metavar = \"DIR\" , type = str , default = os . environ [ \"SM_CHANNEL_TRAINING\" ]) aa ( \"--backbone\" , default = \"resnet34\" , help = \"backbone model either resnet34 (default) or resnet50\" ) aa ( \"--num-classes\" , default = 7 , type = int , metavar = \"N\" , help = \"number of classes including the background\" ) aa ( \"-b\" , \"--batch-size\" , default = 16 , type = int , metavar = \"N\" , help = \"mini-batch size (default: 16), this is the total \" \"batch size of all GPUs on the current node when \" \"using Data Parallel or Distributed Data Parallel\" , ) aa ( \"--lr\" , \"--learning-rate\" , default = 1e-3 , type = float , metavar = \"LR\" , help = \"initial learning rate\" , dest = \"learning_rate\" , ) aa ( \"--momentum\" , default = 0.9 , type = float , metavar = \"M\" , help = \"momentum\" ) aa ( \"--wd\" , \"--weight-decay\" , default = 1e-4 , type = float , metavar = \"W\" , help = \"weight decay (default: 1e-4)\" , dest = \"weight_decay\" , ) aa ( \"--seed\" , type = int , default = 123 , help = \"seed for initializing training\" ) aa ( \"--pretrained-mfn-ckpt\" , type = str ) aa ( \"--pretrained-rpn-ckpt\" , type = str ) aa ( \"--pretrained-roi-ckpt\" , type = str ) aa ( \"--finetuned-rpn-ckpt\" , type = str ) aa ( \"--finetuned-roi-ckpt\" , type = str ) aa ( \"--resume-from-checkpoint\" , type = str ) aa ( \"--resume-sagemaker-from-checkpoint\" , type = str , default = os . getenv ( \"SM_CHANNEL_PRETRAINED_CHECKPOINT\" , None )) return parser def get_args (): parent_parser = ArgumentParser ( add_help = False ) aa = parent_parser . add_argument aa ( \"--epochs\" , type = int , default = 1 , help = \"number of training epochs\" ) aa ( \"--save-path\" , metavar = \"DIR\" , default = os . environ [ \"SM_MODEL_DIR\" ], type = str , help = \"path to save output\" ) aa ( \"--gpus\" , type = int , default = os . getenv ( \"SM_NUM_GPUS\" , 1 ), help = \"how many gpus\" ) aa ( \"--distributed-backend\" , type = str , default = \"\" , choices = ( \"dp\" , \"ddp\" , \"ddp2\" ), help = \"supports three options dp, ddp, ddp2\" , ) # aa(\"--use-16bit\", dest=\"use_16bit\", action=\"store_true\", help=\"if true uses 16 bit precision\") parser = DDNDetection . add_model_specific_args ( parent_parser ) return parser . parse_args () def model_fn ( model_dir ): # TODO: `model_fn` doesn't get more args # see: https://github.com/aws/sagemaker-inference-toolkit/issues/65 backbone = \"resnet34\" num_classes = 7 # including the background mfn = load_checkpoint ( Classification ( backbone , num_classes - 1 ) . mfn , model_dir , \"mfn\" ) rpn = load_checkpoint ( RPN (), model_dir , \"rpn\" ) roi = load_checkpoint ( RoI ( num_classes ), model_dir , \"roi\" ) model = Detection ( mfn , rpn , roi ) model = model . eval () freeze ( model ) return model def main ( args : Namespace ) -> None : ddn = DDNDetection ( ** vars ( args )) if args . seed is not None : pl . seed_everything ( args . seed ) # doesn't do multi-gpu if torch . cuda . device_count () > 1 : torch . cuda . manual_seed_all ( args . seed ) # TODO: add deterministic training # torch.backends.cudnn.deterministic = True if ddn . train_rpn : checkpoint_callback = ModelCheckpoint ( filepath = os . path . join ( args . save_path , \" {epoch} - {loss:.3f} \" ), save_top_k = 1 , verbose = True , monitor = \"loss\" , mode = \"min\" , ) early_stop_callback = None elif ddn . train_roi : checkpoint_callback = ModelCheckpoint ( filepath = os . path . join ( args . save_path , \" {epoch} - {loss:.3f} \" ), save_top_k = 1 , verbose = True , monitor = \"loss\" , mode = \"min\" , ) early_stop_callback = None else : checkpoint_callback = ModelCheckpoint ( filepath = os . path . join ( args . save_path , \" {epoch} - {loss:.3f} - {main_score:.3f} \" ), save_top_k = 1 , verbose = True , monitor = \"main_score\" , mode = \"max\" , ) early_stop_callback = EarlyStopping ( \"main_score\" , patience = 50 , mode = \"max\" ) trainer = pl . Trainer ( default_root_dir = args . save_path , num_sanity_val_steps = 1 , limit_val_batches = 1.0 , gpus = args . gpus , max_epochs = args . epochs , early_stop_callback = early_stop_callback , checkpoint_callback = checkpoint_callback , distributed_backend = args . distributed_backend or None , # precision=16 if args.use_16bit else 32, # TODO: apex weights_summary = \"top\" , resume_from_checkpoint = None if args . resume_from_checkpoint == \"\" else args . resume_from_checkpoint , ) trainer . fit ( ddn ) return if __name__ == \"__main__\" : main ( get_args ()) Functions get_args def get_args ( ) View Source def get_args (): parent_parser = ArgumentParser ( add_help = False ) aa = parent_parser . add_argument aa ( \"--epochs\" , type = int , default = 1 , help = \"number of training epochs\" ) aa ( \"--save-path\" , metavar = \"DIR\" , default = os . environ [ \"SM_MODEL_DIR\" ], type = str , help = \"path to save output\" ) aa ( \"--gpus\" , type = int , default = os . getenv ( \"SM_NUM_GPUS\" , 1 ), help = \"how many gpus\" ) aa ( \"--distributed-backend\" , type = str , default = \"\" , choices = ( \"dp\" , \"ddp\" , \"ddp2\" ), help = \"supports three options dp, ddp, ddp2\" , ) # aa(\"--use-16bit\", dest=\"use_16bit\", action=\"store_true\", help=\"if true uses 16 bit precision\") parser = DDNDetection . add_model_specific_args ( parent_parser ) return parser . parse_args () main def main ( args : argparse . Namespace ) -> None View Source def main ( args : Namespace ) -> None : ddn = DDNDetection ( ** vars ( args )) if args . seed is not None : pl . seed_everything ( args . seed ) # doesn't do multi-gpu if torch . cuda . device_count () > 1 : torch . cuda . manual_seed_all ( args . seed ) # TODO: add deterministic training # torch.backends.cudnn.deterministic = True if ddn . train_rpn : checkpoint_callback = ModelCheckpoint ( filepath = os . path . join ( args . save_path , \"{epoch}-{loss:.3f}\" ), save_top_k = 1 , verbose = True , monitor = \"loss\" , mode = \"min\" , ) early_stop_callback = None elif ddn . train_roi : checkpoint_callback = ModelCheckpoint ( filepath = os . path . join ( args . save_path , \"{epoch}-{loss:.3f}\" ), save_top_k = 1 , verbose = True , monitor = \"loss\" , mode = \"min\" , ) early_stop_callback = None else : checkpoint_callback = ModelCheckpoint ( filepath = os . path . join ( args . save_path , \"{epoch}-{loss:.3f}-{main_score:.3f}\" ), save_top_k = 1 , verbose = True , monitor = \"main_score\" , mode = \"max\" , ) early_stop_callback = EarlyStopping ( \"main_score\" , patience = 50 , mode = \"max\" ) trainer = pl . Trainer ( default_root_dir = args . save_path , num_sanity_val_steps = 1 , limit_val_batches = 1.0 , gpus = args . gpus , max_epochs = args . epochs , early_stop_callback = early_stop_callback , checkpoint_callback = checkpoint_callback , distributed_backend = args . distributed_backend or None , # precision=16 if args.use_16bit else 32, # TODO: apex weights_summary = \"top\" , resume_from_checkpoint = None if args . resume_from_checkpoint == \"\" else args . resume_from_checkpoint , ) trainer . fit ( ddn ) return model_fn def model_fn ( model_dir ) View Source def model_fn ( model_dir ): # TODO: `model_fn` doesn't get more args # see: https://github.com/aws/sagemaker-inference-toolkit/issues/65 backbone = \"resnet34\" num_classes = 7 # including the background mfn = load_checkpoint ( Classification ( backbone , num_classes - 1 ) . mfn , model_dir , \"mfn\" ) rpn = load_checkpoint ( RPN (), model_dir , \"rpn\" ) roi = load_checkpoint ( RoI ( num_classes ), model_dir , \"roi\" ) model = Detection ( mfn , rpn , roi ) model = model . eval () freeze ( model ) return model Classes DDNDetection class DDNDetection ( train_rpn : bool , train_roi : bool , finetune_rpn : bool , finetune_roi : bool , data_path : str , backbone : str , num_classes : int , learning_rate : float , batch_size : int , momentum : float , weight_decay : float , seed : int , pretrained_mfn_ckpt : Union [ str , NoneType ] = None , pretrained_rpn_ckpt : Union [ str , NoneType ] = None , pretrained_roi_ckpt : Union [ str , NoneType ] = None , finetuned_rpn_ckpt : Union [ str , NoneType ] = None , finetuned_roi_ckpt : Union [ str , NoneType ] = None , resume_sagemaker_from_checkpoint : Union [ str , NoneType ] = None , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance. Ancestors (in MRO) pytorch_lightning.core.lightning.LightningModule abc.ABC pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin pytorch_lightning.core.grads.GradInformation pytorch_lightning.core.saving.ModelIO pytorch_lightning.core.hooks.ModelHooks torch.nn.modules.module.Module Class variables CHECKPOINT_HYPER_PARAMS_KEY CHECKPOINT_HYPER_PARAMS_NAME CHECKPOINT_HYPER_PARAMS_TYPE T_destination dump_patches Static methods add_model_specific_args def add_model_specific_args ( parent_parser ) View Source @staticmethod def add_model_specific_args ( parent_parser ) : # pragma : no - cover parser = ArgumentParser ( parents =[ parent_parser ] , add_help = False ) aa = parser . add_argument aa ( \"--train-rpn\" , action = \"store_true\" ) aa ( \"--train-roi\" , action = \"store_true\" ) aa ( \"--finetune-rpn\" , action = \"store_true\" ) aa ( \"--finetune-roi\" , action = \"store_true\" ) aa ( \"--data-path\" , metavar = \"DIR\" , type = str , default = os . environ [ \"SM_CHANNEL_TRAINING\" ] ) aa ( \"--backbone\" , default = \"resnet34\" , help = \"backbone model either resnet34 (default) or resnet50\" ) aa ( \"--num-classes\" , default = 7 , type = int , metavar = \"N\" , help = \"number of classes including the background\" ) aa ( \"-b\" , \"--batch-size\" , default = 16 , type = int , metavar = \"N\" , help = \"mini-batch size (default: 16), this is the total \" \"batch size of all GPUs on the current node when \" \"using Data Parallel or Distributed Data Parallel\" , ) aa ( \"--lr\" , \"--learning-rate\" , default = 1e-3 , type = float , metavar = \"LR\" , help = \"initial learning rate\" , dest = \"learning_rate\" , ) aa ( \"--momentum\" , default = 0.9 , type = float , metavar = \"M\" , help = \"momentum\" ) aa ( \"--wd\" , \"--weight-decay\" , default = 1e-4 , type = float , metavar = \"W\" , help = \"weight decay (default: 1e-4)\" , dest = \"weight_decay\" , ) aa ( \"--seed\" , type = int , default = 123 , help = \"seed for initializing training\" ) aa ( \"--pretrained-mfn-ckpt\" , type = str ) aa ( \"--pretrained-rpn-ckpt\" , type = str ) aa ( \"--pretrained-roi-ckpt\" , type = str ) aa ( \"--finetuned-rpn-ckpt\" , type = str ) aa ( \"--finetuned-roi-ckpt\" , type = str ) aa ( \"--resume-from-checkpoint\" , type = str ) aa ( \"--resume-sagemaker-from-checkpoint\" , type = str , default = os . getenv ( \"SM_CHANNEL_PRETRAINED_CHECKPOINT\" , None )) return parser load_from_checkpoint def load_from_checkpoint ( checkpoint_path : str , * args , map_location : Union [ Dict [ str , str ], str , torch . device , int , Callable , NoneType ] = None , hparams_file : Union [ str , NoneType ] = None , tags_csv : Union [ str , NoneType ] = None , ** kwargs ) Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to __init__ in the checkpoint under module_arguments Any arguments specified through *args and **kwargs will override args stored in hparams . Args: checkpoint_path: Path to checkpoint. This can also be a URL. args: Any positional args needed to init the model. map_location: If your checkpoint saved a GPU model and you now load on CPUs or a different number of GPUs, use this to map to the new setup. The behaviour is the same as in :func: torch.load . hparams_file: Optional path to a .yaml file with hierarchical structure as in this example:: drop_prob : 0.2 dataloader : batch_size : 32 You most likely won 't need this since Lightning will always save the hyperparameters to the checkpoint. However, if your checkpoint weights don' t have the hyperparameters saved , use this method to pass in a . yaml file with the hparams you 'd like to use. These will be converted into a :class:`~dict` and passed into your :class:`LightningModule` for use. If your model' s `hparams` argument is : class : `~argparse.Namespace` and . yaml file has hierarchical structure , you need to refactor your model to treat `hparams` as : class : `~dict` . . csv files are acceptable here till v0 .9.0 , see tags_csv argument for detailed usage . tags_csv : .. warning :: .. deprecated :: 0.7.6 `tags_csv` argument is deprecated in v0 .7.6 . Will be removed v0 .9.0 . Optional path to a . csv file with two columns ( key , value ) as in this example :: key , value drop_prob , 0.2 batch_size , 32 Use this method to pass in a . csv file with the hparams you 'd like to use. hparam_overrides: A dictionary with keys to override in the hparams kwargs: Any keyword args needed to init the model. Return: :class: LightningModule with loaded weights and hyperparameters (if available). Example: .. code-block:: python # load weights without mapping ... MyLightningModule . load_from_checkpoint ( 'path/to/checkpoint.ckpt' ) # or load weights mapping all weights from GPU 1 to GPU 0 ... map_location = { 'cuda:1' : 'cuda:0' } MyLightningModule . load_from_checkpoint ( 'path/to/checkpoint.ckpt' , map_location = map_location ) # or load weights and hyperparameters from separate files. MyLightningModule . load_from_checkpoint ( 'path/to/checkpoint.ckpt' , hparams_file = '/path/to/hparams_file.yaml' ) # override some of the params with new values MyLightningModule . load_from_checkpoint ( PATH , num_layers = 128 , pretrained_ckpt_path : NEW_PATH , ) # predict pretrained_model . eval () pretrained_model . freeze () y_hat = pretrained_model ( x ) View Source @classmethod def load_from_checkpoint ( cls , checkpoint_path : str , * args , map_location : Optional [ Union [ Dict [ str , str ] , str , torch . device , int , Callable ]] = None , hparams_file : Optional [ str ] = None , tags_csv : Optional [ str ] = None , # backward compatible, todo: remove in v0.9.0 ** kwargs ) : r \" \"\" Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to `__init__` in the checkpoint under `module_arguments` Any arguments specified through \\ *args and \\ * \\ *kwargs will override args stored in `hparams`. Args: checkpoint_path: Path to checkpoint. This can also be a URL. args: Any positional args needed to init the model. map_location: If your checkpoint saved a GPU model and you now load on CPUs or a different number of GPUs, use this to map to the new setup. The behaviour is the same as in :func:`torch.load`. hparams_file: Optional path to a .yaml file with hierarchical structure as in this example:: drop_prob: 0.2 dataloader: batch_size: 32 You most likely won't need this since Lightning will always save the hyperparameters to the checkpoint. However, if your checkpoint weights don't have the hyperparameters saved, use this method to pass in a .yaml file with the hparams you'd like to use. These will be converted into a :class:`~dict` and passed into your :class:`LightningModule` for use. If your model's `hparams` argument is :class:`~argparse.Namespace` and .yaml file has hierarchical structure, you need to refactor your model to treat `hparams` as :class:`~dict`. .csv files are acceptable here till v0.9.0, see tags_csv argument for detailed usage. tags_csv: .. warning:: .. deprecated:: 0.7.6 `tags_csv` argument is deprecated in v0.7.6. Will be removed v0.9.0. Optional path to a .csv file with two columns (key, value) as in this example:: key,value drop_prob,0.2 batch_size,32 Use this method to pass in a .csv file with the hparams you'd like to use. hparam_overrides: A dictionary with keys to override in the hparams kwargs: Any keyword args needed to init the model. Return: :class:`LightningModule` with loaded weights and hyperparameters (if available). Example: .. code-block:: python # load weights without mapping ... MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt') # or load weights mapping all weights from GPU 1 to GPU 0 ... map_location = {'cuda:1':'cuda:0'} MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', map_location=map_location ) # or load weights and hyperparameters from separate files. MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', hparams_file='/path/to/hparams_file.yaml' ) # override some of the params with new values MyLightningModule.load_from_checkpoint( PATH, num_layers=128, pretrained_ckpt_path: NEW_PATH, ) # predict pretrained_model.eval() pretrained_model.freeze() y_hat = pretrained_model(x) \"\" \" if map_location is not None : checkpoint = pl_load ( checkpoint_path , map_location = map_location ) else : checkpoint = pl_load ( checkpoint_path , map_location = lambda storage , loc : storage ) # add the hparams from csv file to checkpoint if tags_csv is not None : hparams_file = tags_csv rank_zero_warn ( '`tags_csv` argument is deprecated in v0.7.6. Will be removed v0.9.0' , DeprecationWarning ) if hparams_file is not None : extension = hparams_file . split ( '.' ) [ - 1 ] if extension . lower () in ( 'csv' ) : hparams = load_hparams_from_tags_csv ( hparams_file ) elif extension . lower () in ( 'yml' , 'yaml' ) : hparams = load_hparams_from_yaml ( hparams_file ) else : raise ValueError ( '.csv, .yml or .yaml is required for `hparams_file`' ) hparams [ 'on_gpu' ] = False # overwrite hparams by the given file checkpoint [ cls . CHECKPOINT_HYPER_PARAMS_KEY ] = hparams # for past checkpoint need to add the new key if cls . CHECKPOINT_HYPER_PARAMS_KEY not in checkpoint : checkpoint [ cls . CHECKPOINT_HYPER_PARAMS_KEY ] = {} # override the hparams with values that were passed in checkpoint [ cls . CHECKPOINT_HYPER_PARAMS_KEY ] . update ( kwargs ) model = cls . _load_model_state ( checkpoint , * args , ** kwargs ) return model load_from_metrics def load_from_metrics ( weights_path , tags_csv , map_location = None ) Warning: Deprecated in version 0.7.0. You should use :meth: load_from_checkpoint instead. Will be removed in v0.9.0. View Source @classmethod def load_from_metrics ( cls , weights_path , tags_csv , map_location = None ) : r \" \"\" Warning: Deprecated in version 0.7.0. You should use :meth:`load_from_checkpoint` instead. Will be removed in v0.9.0. \"\" \" rank_zero_warn ( \"`load_from_metrics` method has been unified with `load_from_checkpoint` in v0.7.0.\" \" The deprecated method will be removed in v0.9.0.\" , DeprecationWarning ) return cls . load_from_checkpoint ( weights_path , tags_csv = tags_csv , map_location = map_location ) Instance variables device dtype example_input_array hparams on_gpu True if your model is currently running on GPUs. Useful to set flags around the LightningModule for different CPU vs GPU behavior. Methods add_module def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module amp_scale_loss def amp_scale_loss ( self , unscaled_loss , optimizer , optimizer_idx ) View Source def amp_scale_loss ( self , unscaled_loss , optimizer , optimizer_idx ): if NATIVE_AMP_AVALAIBLE : scaled_loss = self . trainer . scaler . scale ( unscaled_loss ) else : scaled_loss = amp . scale_loss ( unscaled_loss , optimizer ) return scaled_loss apply def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self backward def backward ( self , trainer , loss : torch . Tensor , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) -> None Override backward with your own implementation if you need to. Args: trainer: Pointer to the trainer loss: Loss is already scaled by accumulated grads optimizer: Current optimizer being used optimizer_idx: Index of the current optimizer being used Called to perform backward step. Feel free to override as needed. The loss passed in has already been scaled for accumulated gradients if requested. Example:: def backward(self, trainer, loss, optimizer, optimizer_idx): loss.backward() View Source def backward ( self , trainer , loss : Tensor , optimizer : Optimizer , optimizer_idx : int ) -> None : \"\"\" Override backward with your own implementation if you need to. Args: trainer: Pointer to the trainer loss: Loss is already scaled by accumulated grads optimizer: Current optimizer being used optimizer_idx: Index of the current optimizer being used Called to perform backward step. Feel free to override as needed. The loss passed in has already been scaled for accumulated gradients if requested. Example:: def backward(self, trainer, loss, optimizer, optimizer_idx): loss.backward() \"\"\" loss . backward () bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf children def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module configure_apex def configure_apex ( self , amp : object , model : 'LightningModule' , optimizers : List [ torch . optim . optimizer . Optimizer ], amp_level : str ) -> Tuple [ _ForwardRef ( 'LightningModule' ), List [ torch . optim . optimizer . Optimizer ]] Override to init AMP your own way. Must return a model and list of optimizers. Args: amp: pointer to amp library object. model: pointer to current :class: LightningModule . optimizers: list of optimizers passed in :meth: configure_optimizers . amp_level: AMP mode chosen ('O1', 'O2', etc...) Return: Apex wrapped model and optimizers Examples: .. code-block:: python # Default implementation used by Trainer . def configure_apex ( self , amp , model , optimizers , amp_level ): model , optimizers = amp . initialize ( model , optimizers , opt_level = amp_level , ) return model , optimizers View Source def configure_apex ( self , amp : object , model : 'LightningModule' , optimizers : List [ Optimizer ] , amp_level : str ) -> Tuple [ 'LightningModule' , List [ Optimizer ]] : r \" \"\" Override to init AMP your own way. Must return a model and list of optimizers. Args: amp: pointer to amp library object. model: pointer to current :class:`LightningModule`. optimizers: list of optimizers passed in :meth:`configure_optimizers`. amp_level: AMP mode chosen ('O1', 'O2', etc...) Return: Apex wrapped model and optimizers Examples: .. code-block:: python # Default implementation used by Trainer. def configure_apex(self, amp, model, optimizers, amp_level): model, optimizers = amp.initialize( model, optimizers, opt_level=amp_level, ) return model, optimizers \"\" \" model , optimizers = amp . initialize ( model , optimizers , opt_level = amp_level ) return model , optimizers configure_ddp def configure_ddp ( self , model : 'LightningModule' , device_ids : List [ int ] ) -> torch . nn . parallel . distributed . DistributedDataParallel Override to init DDP in your own way or with your own wrapper. The only requirements are that: On a validation batch the call goes to model.validation_step . On a training batch the call goes to model.training_step . On a testing batch, the call goes to model.test_step .+ Args: model: the :class: LightningModule currently being optimized. device_ids: the list of GPU ids. Return: DDP wrapped model Examples: .. code-block:: python # default implementation used in Trainer def configure_ddp(self, model, device_ids): # Lightning DDP simply routes to test_step, val_step, etc... model = LightningDistributedDataParallel( model, device_ids=device_ids, find_unused_parameters=True ) return model View Source def configure_ddp ( self , model : 'LightningModule' , device_ids : List [ int ] ) -> DistributedDataParallel : r \" \"\" Override to init DDP in your own way or with your own wrapper. The only requirements are that: 1. On a validation batch the call goes to ``model.validation_step``. 2. On a training batch the call goes to ``model.training_step``. 3. On a testing batch, the call goes to ``model.test_step``.+ Args: model: the :class:`LightningModule` currently being optimized. device_ids: the list of GPU ids. Return: DDP wrapped model Examples: .. code-block:: python # default implementation used in Trainer def configure_ddp(self, model, device_ids): # Lightning DDP simply routes to test_step, val_step, etc... model = LightningDistributedDataParallel( model, device_ids=device_ids, find_unused_parameters=True ) return model \"\" \" model = LightningDistributedDataParallel ( model , device_ids = device_ids , find_unused_parameters = True ) return model configure_optimizers def configure_optimizers ( self ) View Source def configure_optimizers ( self ): params = [ p for p in self . parameters () if p . requires_grad ] optimizer = optim . SGD ( params , lr = self . learning_rate , momentum = self . momentum , weight_decay = self . weight_decay ) return optimizer cpu def cpu ( self ) -> torch . nn . modules . module . Module Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self ) -> Module : \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" self . _device = torch . device ( 'cpu') return super (). cpu () cuda def cuda ( self , device : Union [ int , NoneType ] = None ) -> torch . nn . modules . module . Module Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device: if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self , device : Optional [ int ] = None ) -> Module : \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device: if specified, all parameters will be copied to that device Returns: Module: self \"\"\" self . _device = torch . device ( 'cuda' , index = device ) return super (). cuda ( device = device ) double def double ( self ) -> torch . nn . modules . module . Module Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self ) -> Module : \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" self . _dtype = torch . double return super (). double () eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False ) extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float def float ( self ) -> torch . nn . modules . module . Module Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self ) -> Module : \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" self . _dtype = torch . float return super (). float () forward def forward ( self , images , * args , ** kwargs ) View Source @ auto_move_data def forward ( self , images , * args , ** kwargs ): if self . train_rpn : # step 2 images = torch . stack ( images ) features = self . mfn ( images ) features = OrderedDict ({ str ( i ): t . unsqueeze ( 0 ) for i , t in enumerate ( features )}) images = ImageList ( images , [( 224 , 224 )]) return self . rpn ( images , features , targets = kwargs . get ( \"targets\" )) elif self . train_roi : # step 3 self . mfn . eval () self . rpn . eval () images = torch . stack ( images ) features = self . mfn ( images ) features = OrderedDict ({ str ( i ): t . unsqueeze ( 0 ) for i , t in enumerate ( features )}) images = ImageList ( images , [( 224 , 224 )]) proposals , _ = self . rpn ( images , features , targets = None ) return self . roi ( features , proposals , [( 224 , 224 )], targets = kwargs . get ( \"targets\" )) elif self . finetune_rpn : self . model . backbone . eval () self . model . roi_heads . eval () return self . model ( images , targets = kwargs . get ( \"targets\" )) elif self . finetune_roi : self . model . backbone . eval () self . model . rpn . eval () return self . model ( images , targets = kwargs . get ( \"targets\" )) else : return self . model ( images , targets = kwargs . get ( \"targets\" )) freeze def freeze ( self ) -> None Freeze all params for inference. Example: .. code-block:: python model = MyLightningModule(...) model.freeze() View Source def freeze ( self ) -> None : r \"\"\" Freeze all params for inference. Example: .. code-block:: python model = MyLightningModule(...) model.freeze() \"\"\" for param in self . parameters (): param . requires_grad = False self . eval () get_progress_bar_dict def get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]] Additional items to be displayed in the progress bar. Return: Dictionary with the items to be displayed in the progress bar. View Source def get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]]: r \"\"\" Additional items to be displayed in the progress bar. Return: Dictionary with the items to be displayed in the progress bar. \"\"\" # call . item () only once but store elements without graphs running_train_loss = self . trainer . running_loss . mean () avg_training_loss = running_train_loss . cpu (). item () if running_train_loss is not None else float ( 'NaN' ) tqdm_dict = { 'loss' : '{:.3f}' . format ( avg_training_loss ) } if self . trainer . truncated_bptt_steps is not None : tqdm_dict [ 'split_idx' ] = self . trainer . split_idx if self . trainer . logger is not None and self . trainer . logger . version is not None : tqdm_dict [ 'v_num' ] = self . trainer . logger . version return tqdm_dict get_tqdm_dict def get_tqdm_dict ( self ) -> Dict [ str , Union [ int , str ]] Additional items to be displayed in the progress bar. Return: Dictionary with the items to be displayed in the progress bar. Warning: Deprecated since v0.7.3. Use :meth: get_progress_bar_dict instead. View Source def get_tqdm_dict ( self ) -> Dict [ str , Union [ int , str ]] : \" \"\" Additional items to be displayed in the progress bar. Return: Dictionary with the items to be displayed in the progress bar. Warning: Deprecated since v0.7.3. Use :meth:`get_progress_bar_dict` instead. \"\" \" rank_zero_warn ( \"`get_tqdm_dict` was renamed to `get_progress_bar_dict` in v0.7.3\" \" and this method will be removed in v1.0.0\" , DeprecationWarning ) return self . get_progress_bar_dict () grad_norm def grad_norm ( self , norm_type : Union [ float , int , str ] ) -> Dict [ str , float ] Compute each parameter's gradient's norm and their overall norm. The overall norm is computed over all gradients together, as if they were concatenated into a single vector. Args: norm_type: The type of the used p-norm, cast to float if necessary. Can be 'inf' for infinity norm. Return: norms: The dictionary of p-norms of each parameter's gradient and a special entry for the total p-norm of the gradients viewed as a single vector. View Source def grad_norm ( self , norm_type : Union [ float , int , str ]) -> Dict [ str , float ]: \"\"\"Compute each parameter's gradient's norm and their overall norm. The overall norm is computed over all gradients together, as if they were concatenated into a single vector. Args: norm_type: The type of the used p-norm, cast to float if necessary. Can be ``'inf'`` for infinity norm. Return: norms: The dictionary of p-norms of each parameter's gradient and a special entry for the total p-norm of the gradients viewed as a single vector. \"\"\" norm_type = float ( norm_type ) norms , all_norms = {} , [] for name , p in self . named_parameters (): if p . grad is None : continue param_norm = float ( p . grad . data . norm ( norm_type )) norms [ f 'grad_{norm_type}_norm_{name}' ] = round ( param_norm , 3 ) all_norms . append ( param_norm ) total_norm = float ( torch . tensor ( all_norms ). norm ( norm_type )) norms [ f 'grad_{norm_type}_norm_total' ] = round ( total_norm , 3 ) return norms half def half ( self ) -> torch . nn . modules . module . Module Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self ) -> Module : \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" self . _dtype = torch . half return super (). half () init_ddp_connection def init_ddp_connection ( self , global_rank : int , world_size : int , is_slurm_managing_tasks : bool = True ) -> None Override to define your custom way of setting up a distributed environment. Lightning's implementation uses env:// init by default and sets the first node as root for SLURM managed cluster. Args: global_rank: The global process idx. world_size: Number of GPUs being use across all nodes. (num_nodes * num_gpus). is_slurm_managing_tasks: is cluster managed by SLURM. View Source def init_ddp_connection ( self , global_rank : int , world_size : int , is_slurm_managing_tasks : bool = True ) -> None : \"\"\" Override to define your custom way of setting up a distributed environment. Lightning's implementation uses env:// init by default and sets the first node as root for SLURM managed cluster. Args: global_rank: The global process idx. world_size: Number of GPUs being use across all nodes. (num_nodes * num_gpus). is_slurm_managing_tasks: is cluster managed by SLURM. \"\"\" if is_slurm_managing_tasks : self . _init_slurm_connection () if 'MASTER_ADDR' not in os . environ : rank_zero_warn ( \"MASTER_ADDR environment variable is not defined. Set as localhost\" ) os . environ [ 'MASTER_ADDR' ] = '127.0.0.1' log . debug ( f \"MASTER_ADDR: {os.environ['MASTER_ADDR']}\" ) if 'MASTER_PORT' not in os . environ : rank_zero_warn ( \"MASTER_PORT environment variable is not defined. Set as 12910\" ) os . environ [ 'MASTER_PORT' ] = '12910' log . debug ( f \"MASTER_PORT: {os.environ['MASTER_PORT']}\" ) if 'WORLD_SIZE' in os . environ and int ( os . environ [ 'WORLD_SIZE' ]) != world_size : rank_zero_warn ( f \"WORLD_SIZE environment variable ({os.environ['WORLD_SIZE']}) \" f \"is not equal to the computed world size ({world_size}). Ignored.\" ) torch_backend = \"nccl\" if self . trainer . on_gpu else \"gloo\" log . info ( f \"initializing ddp: GLOBAL_RANK: {global_rank}, MEMBER: {global_rank+1}/{world_size}\" ) torch_distrib . init_process_group ( torch_backend , rank = global_rank , world_size = world_size ) load_state_dict def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys ) modules def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem on_after_backward def on_after_backward ( self ) -> None Called in the training loop after loss.backward() and before optimizers do anything. This is the ideal place to inspect or log gradient information. Example:: def on_after_backward(self): # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge params = self.state_dict() for k, v in params.items(): grads = v name = k self.logger.experiment.add_histogram(tag=name, values=grads, global_step=self.trainer.global_step) View Source def on_after_backward ( self ) -> None : \"\"\" Called in the training loop after loss.backward() and before optimizers do anything. This is the ideal place to inspect or log gradient information. Example:: def on_after_backward(self): # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge params = self.state_dict() for k, v in params.items(): grads = v name = k self.logger.experiment.add_histogram(tag=name, values=grads, global_step=self.trainer.global_step) \"\"\" on_batch_end def on_batch_end ( self ) -> None Called in the training loop after the batch. View Source def on_batch_end ( self ) -> None : \"\"\" Called in the training loop after the batch. \"\"\" on_batch_start def on_batch_start ( self , batch : Any ) -> None Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Args: batch: The batched data as it is returned by the training DataLoader. View Source def on_batch_start ( self , batch : Any ) -> None : \"\"\" Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Args: batch: The batched data as it is returned by the training DataLoader. \"\"\" on_before_zero_grad def on_before_zero_grad ( self , optimizer : torch . optim . optimizer . Optimizer ) -> None Called after optimizer.step() and before optimizer.zero_grad(). Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: optimizer.step() model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad Args: optimizer: The optimizer for which grads should be zeroed. View Source def on_before_zero_grad ( self , optimizer : Optimizer ) -> None : \"\"\" Called after optimizer.step() and before optimizer.zero_grad(). Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: optimizer.step() model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad Args: optimizer: The optimizer for which grads should be zeroed. \"\"\" on_epoch_end def on_epoch_end ( self ) -> None Called in the training loop at the very end of the epoch. View Source def on_epoch_end ( self ) -> None : \"\"\" Called in the training loop at the very end of the epoch. \"\"\" on_epoch_start def on_epoch_start ( self ) -> None Called in the training loop at the very beginning of the epoch. View Source def on_epoch_start ( self ) -> None : \"\"\" Called in the training loop at the very beginning of the epoch. \"\"\" on_fit_end def on_fit_end ( self ) Called at the very end of fit. If on DDP it is called on every process View Source def on_fit_end ( self ): \"\"\" Called at the very end of fit. If on DDP it is called on every process \"\"\" on_fit_start def on_fit_start ( self ) Called at the very beginning of fit. If on DDP it is called on every process View Source def on_fit_start ( self ): \"\"\" Called at the very beginning of fit. If on DDP it is called on every process \"\"\" on_hpc_load def on_hpc_load ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager loads the model. Args: checkpoint: A dictionary with variables from the checkpoint. View Source def on_hpc_load ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Hook to do whatever you need right before Slurm manager loads the model. Args: checkpoint: A dictionary with variables from the checkpoint. \"\"\" on_hpc_save def on_hpc_save ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager saves the model. Args: checkpoint: A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. View Source def on_hpc_save ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Hook to do whatever you need right before Slurm manager saves the model. Args: checkpoint: A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. \"\"\" on_load_checkpoint def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example: .. code-block:: python def on_load_checkpoint ( self , checkpoint ): # 99% of the time you don't need to implement this method self . something_cool_i_want_to_save = checkpoint [ 'something_cool_i_want_to_save' ] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. View Source def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None : r \" \"\" Called by Lightning to restore your model. If you saved something with :meth:`on_save_checkpoint` this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example: .. code-block:: python def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. \"\" \" on_post_performance_check def on_post_performance_check ( self ) -> None Called at the very end of the validation loop. View Source def on_post_performance_check ( self ) -> None : \"\"\" Called at the very end of the validation loop. \"\"\" on_pre_performance_check def on_pre_performance_check ( self ) -> None Called at the very beginning of the validation loop. View Source def on_pre_performance_check ( self ) -> None : \"\"\" Called at the very beginning of the validation loop. \"\"\" on_sanity_check_start def on_sanity_check_start ( self ) Called before starting evaluation. Warning: Deprecated. Will be removed in v0.9.0. View Source def on_sanity_check_start ( self ): \"\"\" Called before starting evaluation. Warning: Deprecated. Will be removed in v0.9.0. \"\"\" on_save_checkpoint def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: Checkpoint to be saved Example: .. code-block:: python def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. View Source def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None : r \"\"\" Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: Checkpoint to be saved Example: .. code-block:: python def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. \"\"\" on_train_end def on_train_end ( self ) -> None Called at the end of training before logger experiment is closed. View Source def on_train_end ( self ) -> None : \"\"\" Called at the end of training before logger experiment is closed. \"\"\" on_train_start def on_train_start ( self ) -> None Called at the beginning of training before sanity check. View Source def on_train_start ( self ) -> None : \"\"\" Called at the beginning of training before sanity check. \"\"\" optimizer_step def optimizer_step ( self , epoch : int , batch_idx : int , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int , second_order_closure : Union [ Callable , NoneType ] = None , on_tpu : bool = False , using_native_amp : bool = False , using_lbfgs : bool = False ) -> None Override this method to adjust the default way the :class: ~pytorch_lightning.trainer.trainer.Trainer calls each optimizer. By default, Lightning calls step() and zero_grad() as shown in the example once per optimizer. Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers this indexes into that list. second_order_closure: closure for second order methods on_tpu: true if TPU backward is required using_native_amp: True if using native amp using_lbfgs: True if the matching optimizer is lbfgs Examples: .. code-block:: python # DEFAULT def optimizer_step ( self , current_epoch , batch_idx , optimizer , optimizer_idx , second_order_closure , on_tpu , using_native_amp , using_lbfgs ): optimizer . step () # Alternating schedule for optimizer steps ( i . e .: GANs ) def optimizer_step ( self , current_epoch , batch_idx , optimizer , optimizer_idx , second_order_closure , on_tpu , using_native_amp , using_lbfgs ): # update generator opt every 2 steps if optimizer_idx == 0 : if batch_idx % 2 == 0 : optimizer . step () optimizer . zero_grad () # update discriminator opt every 4 steps if optimizer_idx == 1 : if batch_idx % 4 == 0 : optimizer . step () optimizer . zero_grad () # ... # add as many optimizers as you want Here 's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1., float(self.trainer.global_step + 1) / 500.) for pg in optimizer.param_groups: pg[' lr ' ] = lr_scale * self . learning_rate # update params optimizer . step () optimizer . zero_grad () Note: If you also override the :meth: ~pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad model hook don't forget to add the call to it before optimizer.zero_grad() yourself. View Source def optimizer_step ( self , epoch : int , batch_idx : int , optimizer : Optimizer , optimizer_idx : int , second_order_closure : Optional [ Callable ] = None , on_tpu : bool = False , using_native_amp : bool = False , using_lbfgs : bool = False , ) -> None : r \"\"\" Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls each optimizer. By default, Lightning calls ``step()`` and ``zero_grad()`` as shown in the example once per optimizer. Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers this indexes into that list. second_order_closure: closure for second order methods on_tpu: true if TPU backward is required using_native_amp: True if using native amp using_lbfgs: True if the matching optimizer is lbfgs Examples: .. code-block:: python # DEFAULT def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step() # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every 2 steps if optimizer_idx == 0: if batch_idx % 2 == 0 : optimizer.step() optimizer.zero_grad() # update discriminator opt every 4 steps if optimizer_idx == 1: if batch_idx % 4 == 0 : optimizer.step() optimizer.zero_grad() # ... # add as many optimizers as you want Here's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1., float(self.trainer.global_step + 1) / 500.) for pg in optimizer.param_groups: pg['lr'] = lr_scale * self.learning_rate # update params optimizer.step() optimizer.zero_grad() Note: If you also override the :meth:`~pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad` model hook don't forget to add the call to it before ``optimizer.zero_grad()`` yourself. \"\"\" if on_tpu : xm . optimizer_step ( optimizer ) elif using_native_amp : self . trainer . scaler . step ( optimizer ) elif using_lbfgs : optimizer . step ( second_order_closure ) else : optimizer . step () optimizer_zero_grad def optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) View Source def optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : Optimizer , optimizer_idx : int ): optimizer . zero_grad () parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param prepare_data def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data ( self ): # good download_data () tokenize () etc () # bad self . split = data_split self . some_state = some_other_state () In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK = 0 of that node Trainer ( prepare_data_per_node = True ) # call on GLOBAL_RANK = 0 ( great for shared file systems ) Trainer ( prepare_data_per_node = False ) This is called before requesting the dataloaders: .. code-block:: python model . prepare_data () if ddp / tpu : init () model . setup ( stage ) model . train_dataloader () model . val_dataloader () model . test_dataloader () View Source def prepare_data ( self ) -> None : \" \"\" Use this to download and prepare data. .. warning:: DO NOT set state to the model (use `setup` instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): 1. Once per node. This is the default and is only called on LOCAL_RANK=0. 2. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() if ddp/tpu: init() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() \"\" \" print def print ( self , * args , ** kwargs ) -> None Prints only from process 0. Use this in any distributed mode to log only once. Args: args: The thing to print. Will be passed to Python's built-in print function. *kwargs: Will be passed to Python's built-in print function. Example: .. code-block :: python def forward ( self , x ): self . print ( x , 'in forward' ) View Source def print ( self , * args , ** kwargs ) -> None : r \"\"\" Prints only from process 0. Use this in any distributed mode to log only once. Args: *args: The thing to print. Will be passed to Python's built-in print function. **kwargs: Will be passed to Python's built-in print function. Example: .. code-block:: python def forward(self, x): self.print(x, 'in forward') \"\"\" if self . trainer . is_global_zero : print ( * args , ** kwargs ) register_backward_hook def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_parameter def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self save_hyperparameters def save_hyperparameters ( self , * args , frame = None ) -> None Save all model arguments. Args: args: single object of dict , NameSpace or OmegaConf or string names or argumenst from class __init__ from collections import OrderedDict class ManuallyArgsModel(LightningModule): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assine arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... model = ManuallyArgsModel(1, 'abc', 3.14) model.hparams \"arg1\": 1 \"arg3\": 3.14 class AutomaticArgsModel(LightningModule): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, args, *kwargs): ... ... model = AutomaticArgsModel(1, 'abc', 3.14) model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 class SingleArgModel(LightningModule): ... def init (self, params): ... super(). init () ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, args, *kwargs): ... ... model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 View Source def save_hyperparameters ( self , * args , frame = None ) -> None : \"\"\"Save all model arguments. Args: args: single object of `dict`, `NameSpace` or `OmegaConf` or string names or argumenst from class `__init__` >>> from collections import OrderedDict >>> class ManuallyArgsModel(LightningModule): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # manually assine arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(LightningModule): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(LightningModule): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 \"\"\" if not frame : frame = inspect . currentframe () . f_back init_args = get_init_args ( frame ) assert init_args , 'failed to inspect the self init' if not args : hp = init_args self . _hparams_name = 'kwargs' if hp else None else : isx_non_str = [ i for i , arg in enumerate ( args ) if not isinstance ( arg , str )] if len ( isx_non_str ) == 1 : hp = args [ isx_non_str [ 0 ]] cand_names = [ k for k , v in init_args . items () if v == hp ] self . _hparams_name = cand_names [ 0 ] if cand_names else None else : hp = { arg : init_args [ arg ] for arg in args if isinstance ( arg , str )} self . _hparams_name = 'kwargs' # `hparams` are expected here if hp : self . _set_hparams ( hp ) setup def setup ( self , stage ) -> None View Source def setup ( self , stage ) -> None : if self . train_rpn : # step 2 self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . pretrained_mfn_ckpt , \"model.mfn\" ) self . rpn = RPN () elif self . train_roi : # step 3 self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . pretrained_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . pretrained_rpn_ckpt , prefix = \"rpn\" ) freeze ( self . rpn ) self . roi = RoI ( self . num_classes ) elif self . finetune_rpn : # step 4 or extra finetune rpn if self . finetuned_rpn_ckpt and self . finetuned_roi_ckpt : # extra finetune rpn self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . finetuned_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . finetuned_rpn_ckpt , prefix = \"rpn\" ) self . roi = load_checkpoint ( RoI ( self . num_classes ), self . finetuned_roi_ckpt , prefix = \"roi\" ) freeze ( self . roi ) self . model = Detection ( self . mfn , self . rpn , self . roi ) else : self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . pretrained_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . pretrained_rpn_ckpt , prefix = \"rpn\" ) self . roi = load_checkpoint ( RoI ( self . num_classes ), self . pretrained_roi_ckpt , prefix = \"roi\" ) freeze ( self . roi ) self . model = Detection ( self . mfn , self . rpn , self . roi ) elif self . finetune_roi : # step 5 or extra finetune roi if self . finetuned_rpn_ckpt and self . finetuned_roi_ckpt : # extra finetune roi self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . finetuned_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . finetuned_rpn_ckpt , prefix = \"rpn\" ) freeze ( self . rpn ) self . roi = load_checkpoint ( RoI ( self . num_classes ), self . finetuned_roi_ckpt , prefix = \"roi\" ) self . model = Detection ( self . mfn , self . rpn , self . roi ) else : self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . finetuned_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . finetuned_rpn_ckpt , prefix = \"rpn\" ) freeze ( self . rpn ) self . roi = load_checkpoint ( RoI ( self . num_classes ), self . pretrained_roi_ckpt , prefix = \"roi\" ) self . model = Detection ( self . mfn , self . rpn , self . roi ) else : # step 6: final/joint model load_checkpoint_fn = load_checkpoint if self . finetuned_roi_ckpt is not None : ckpt_path = self . finetuned_rpn_ckpt elif self . resume_sagemaker_from_checkpoint is not None : ckpt_path = self . resume_sagemaker_from_checkpoint else : ckpt_path = None # ignore load_checkpoint load_checkpoint_fn = lambda * args : args [ 0 ] self . mfn = load_checkpoint_fn ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , ckpt_path , \"mfn\" ) self . rpn = load_checkpoint_fn ( RPN (), ckpt_path , \"rpn\" ) self . roi = load_checkpoint_fn ( RoI ( self . num_classes ), ckpt_path , \"roi\" ) self . model = Detection ( self . mfn , self . rpn , self . roi ) return share_memory def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ()) state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination summarize def summarize ( self , mode : str = 'top' ) -> pytorch_lightning . core . memory . ModelSummary View Source def summarize ( self , mode : str = ModelSummary . MODE_DEFAULT ) -> ModelSummary : model_summary = ModelSummary ( self , mode = mode ) log . info ( '\\n' + str ( model_summary )) return model_summary tbptt_split_batch def tbptt_split_batch ( self , batch : torch . Tensor , split_size : int ) -> list When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Args: batch: Current batch split_size: The size of the split Return: List of batch splits. Each split will be passed to :meth: training_step to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples: .. code-block:: python def tbptt_split_batch ( self , batch , split_size ) : splits = [] for t in range ( 0 , time_dims [ 0 ] , split_size ) : batch_split = [] for i , x in enumerate ( batch ) : if isinstance ( x , torch . Tensor ) : split_x = x [ :, t:t + split_size ] elif isinstance ( x , collections . Sequence ) : split_x = [ None ] * len ( x ) for batch_idx in range ( len ( x )) : split_x [ batch_idx ] = x [ batch_idx ][ t:t + split_size ] batch_split . append ( split_x ) splits . append ( batch_split ) return splits Note: Called in the training loop after :meth: ~pytorch_lightning.callbacks.base.Callback.on_batch_start if :paramref: ~pytorch_lightning.trainer.Trainer.truncated_bptt_steps > 0. Each returned batch split is passed separately to :meth: training_step . View Source def tbptt_split_batch ( self , batch : Tensor , split_size : int ) -> list : r \"\"\" When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Args: batch: Current batch split_size: The size of the split Return: List of batch splits. Each split will be passed to :meth:`training_step` to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples: .. code-block:: python def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits Note: Called in the training loop after :meth:`~pytorch_lightning.callbacks.base.Callback.on_batch_start` if :paramref:`~pytorch_lightning.trainer.Trainer.truncated_bptt_steps` > 0. Each returned batch split is passed separately to :meth:`training_step`. \"\"\" time_dims = [ len(x[0 ] ) for x in batch if isinstance ( x , ( torch . Tensor , collections . Sequence )) ] assert len ( time_dims ) >= 1 , \"Unable to determine batch time dimension\" assert all ( x == time_dims [ 0 ] for x in time_dims ), \"Batch time dimension length is ambiguous\" splits = [] for t in range ( 0 , time_dims [ 0 ] , split_size ) : batch_split = [] for i , x in enumerate ( batch ) : if isinstance ( x , torch . Tensor ) : split_x = x [ :, t:t + split_size ] elif isinstance ( x , collections . Sequence ) : split_x = [ None ] * len ( x ) for batch_idx in range ( len ( x )) : split_x [ batch_idx ] = x [ batch_idx ][ t:t + split_size ] batch_split . append ( split_x ) splits . append ( batch_split ) return splits teardown def teardown ( self , stage : str ) Called at the end of fit and test. Args: stage: either 'fit' or 'test' View Source def teardown ( self , stage : str ): \"\"\" Called at the end of fit and test. Args: stage: either 'fit' or 'test' \"\"\" test_dataloader def test_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , List [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . For data processing use the following pattern: - download in : meth : `prepare_data` - process and split in : meth : `setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Return: Single or multiple PyTorch DataLoaders. Example: .. code-block:: python def test_dataloader ( self ): transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.5 ,), ( 1.0 ,))]) dataset = MNIST ( root = '/path/to/mnist/' , train = False , transform = transform , download = True ) loader = torch . utils . data . DataLoader ( dataset = dataset , batch_size = self . batch_size , shuffle = False ) return loader Note: If you don't need a test dataset and a :meth: test_step , you don't need to implement this method. View Source def test_dataloader ( self ) -> Union [ DataLoader , List [ DataLoader ]] : r \" \"\" Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be called every epoch unless you set :paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch` to ``True``. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data - :meth:`~pytorch_lightning.trainer.Trainer.fit` - ... - :meth:`prepare_data` - :meth:`setup` - :meth:`train_dataloader` - :meth:`val_dataloader` - :meth:`test_dataloader` Note: Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Return: Single or multiple PyTorch DataLoaders. Example: .. code-block:: python def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader Note: If you don't need a test dataset and a :meth:`test_step`, you don't need to implement this method. \"\" \" test_end def test_end ( self , outputs ) Warnings: Deprecated in v0.7.0. Use :meth: test_epoch_end instead. Will be removed in 1.0.0. View Source def test_end ( self , outputs ) : \" \"\" Warnings: Deprecated in v0.7.0. Use :meth:`test_epoch_end` instead. Will be removed in 1.0.0. \"\" \" test_epoch_end def test_epoch_end ( self , outputs : Union [ List [ Dict [ str , torch . Tensor ]], List [ List [ Dict [ str , torch . Tensor ]]]] ) -> Dict [ str , Dict [ str , torch . Tensor ]] Called at the end of a test epoch with the output of all test steps. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Args: outputs: List of outputs you defined in :meth: test_step_end , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader Return: Dict or OrderedDict: Dict has the following optional keys: - progress_bar -> Dict for progress bar display. Must have only tensors. - log -> Dict of metrics to add to logger. Must have only tensors (no images, etc). Note: If you didn't define a :meth: test_step , this won't be called. The outputs here are strictly for logging or progress bar. If you don't need to display anything, don't return anything. If you want to manually set current step, specify it with the 'step' key in the 'log' Dict Examples: With a single dataloader: .. code - block :: python def test_epoch_end ( self , outputs ) : test_acc_mean = 0 for output in outputs : test_acc_mean += output [ 'test_acc' ] test_acc_mean /= len ( outputs ) tqdm_dict = { 'test_acc' : test_acc_mean . item () } # show test_loss and test_acc in progress bar but only log test_loss results = { 'progress_bar' : tqdm_dict , 'log' : { 'test_acc' : test_acc_mean . item () } } return results With multiple dataloaders , `outputs` will be a list of lists . The outer list contains one entry per dataloader , while the inner list contains the individual outputs of each test step for that dataloader . .. code - block :: python def test_epoch_end ( self , outputs ) : test_acc_mean = 0 i = 0 for dataloader_outputs in outputs : for output in dataloader_outputs : test_acc_mean += output [ 'test_acc' ] i += 1 test_acc_mean /= i tqdm_dict = { 'test_acc' : test_acc_mean . item () } # show test_loss and test_acc in progress bar but only log test_loss results = { 'progress_bar' : tqdm_dict , 'log' : { 'test_acc' : test_acc_mean . item (), 'step' : self . current_epoch } } return results View Source def test_epoch_end ( self , outputs : Union [ List [ Dict [ str , Tensor ]] , List [ List [ Dict [ str , Tensor ]]]] ) -> Dict [ str , Dict [ str , Tensor ]] : \" \"\" Called at the end of a test epoch with the output of all test steps. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Args: outputs: List of outputs you defined in :meth:`test_step_end`, or if there are multiple dataloaders, a list containing a list of outputs for each dataloader Return: Dict or OrderedDict: Dict has the following optional keys: - progress_bar -> Dict for progress bar display. Must have only tensors. - log -> Dict of metrics to add to logger. Must have only tensors (no images, etc). Note: If you didn't define a :meth:`test_step`, this won't be called. - The outputs here are strictly for logging or progress bar. - If you don't need to display anything, don't return anything. - If you want to manually set current step, specify it with the 'step' key in the 'log' Dict Examples: With a single dataloader: .. code-block:: python def test_epoch_end(self, outputs): test_acc_mean = 0 for output in outputs: test_acc_mean += output['test_acc'] test_acc_mean /= len(outputs) tqdm_dict = {'test_acc': test_acc_mean.item()} # show test_loss and test_acc in progress bar but only log test_loss results = { 'progress_bar': tqdm_dict, 'log': {'test_acc': test_acc_mean.item()} } return results With multiple dataloaders, `outputs` will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each test step for that dataloader. .. code-block:: python def test_epoch_end(self, outputs): test_acc_mean = 0 i = 0 for dataloader_outputs in outputs: for output in dataloader_outputs: test_acc_mean += output['test_acc'] i += 1 test_acc_mean /= i tqdm_dict = {'test_acc': test_acc_mean.item()} # show test_loss and test_acc in progress bar but only log test_loss results = { 'progress_bar': tqdm_dict, 'log': {'test_acc': test_acc_mean.item(), 'step': self.current_epoch} } return results \"\" \" test_step def test_step ( self , * args , ** kwargs ) -> Dict [ str , torch . Tensor ] Operates on a single batch of data from the test set. In this step you'd normally generate examples or calculate anything of interest such as accuracy. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch. dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple test datasets used). Return: Dict or OrderedDict - passed to the :meth: test_epoch_end method. If you defined :meth: test_step_end it will go to that first. .. code-block:: python # if you have one test dataloader: def test_step ( self , batch , batch_idx ) # if you have multiple test dataloaders: def test_step ( self , batch , batch_idx , dataloader_idx ) Examples: .. code-block:: python # CASE 1: A single test dataset def test_step ( self , batch , batch_idx ) : x , y = batch # implement your own out = self ( x ) loss = self . loss ( out , y ) # log 6 example images # or generated text... or whatever sample_imgs = x [ : 6 ] grid = torchvision . utils . make_grid ( sample_imgs ) self . logger . experiment . add_image ( 'example_images' , grid , 0 ) # calculate acc labels_hat = torch . argmax ( out , dim = 1 ) val_acc = torch . sum ( y == labels_hat ). item () / ( len ( y ) * 1.0 ) # all optional... # return whatever you need for the collation function test_epoch_end output = OrderedDict ( { 'val_loss' : loss_val , 'val_acc' : torch . tensor ( val_acc ), # everything must be a tensor } ) # return an optional dict return output If you pass in multiple validation datasets , : meth : `test_step` will have an additional argument . .. code - block :: python # CASE 2: multiple test datasets def test_step ( self , batch , batch_idx , dataset_idx ) : # dataset_idx tells you which dataset this is. Note: If you don't need to validate you don't need to implement this method. Note: When the :meth: test_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of the test epoch, the model goes back to training mode and gradients are enabled. View Source def test_step ( self , * args , ** kwargs ) -> Dict [ str , Tensor ] : r \" \"\" Operates on a single batch of data from the test set. In this step you'd normally generate examples or calculate anything of interest such as accuracy. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Args: batch (:class:`~torch.Tensor` | (:class:`~torch.Tensor`, ...) | [:class:`~torch.Tensor`, ...]): The output of your :class:`~torch.utils.data.DataLoader`. A tensor, tuple or list. batch_idx (int): The index of this batch. dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple test datasets used). Return: Dict or OrderedDict - passed to the :meth:`test_epoch_end` method. If you defined :meth:`test_step_end` it will go to that first. .. code-block:: python # if you have one test dataloader: def test_step(self, batch, batch_idx) # if you have multiple test dataloaders: def test_step(self, batch, batch_idx, dataloader_idx) Examples: .. code-block:: python # CASE 1: A single test dataset def test_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # all optional... # return whatever you need for the collation function test_epoch_end output = OrderedDict({ 'val_loss': loss_val, 'val_acc': torch.tensor(val_acc), # everything must be a tensor }) # return an optional dict return output If you pass in multiple validation datasets, :meth:`test_step` will have an additional argument. .. code-block:: python # CASE 2: multiple test datasets def test_step(self, batch, batch_idx, dataset_idx): # dataset_idx tells you which dataset this is. Note: If you don't need to validate you don't need to implement this method. Note: When the :meth:`test_step` is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of the test epoch, the model goes back to training mode and gradients are enabled. \"\" \" test_step_end def test_step_end ( self , * args , ** kwargs ) -> Dict [ str , torch . Tensor ] Use this when testing with dp or ddp2 because :meth: test_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: test_step for each batch part. Return: Dict or OrderedDict - passed to the :meth: test_epoch_end . Examples: .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2 , this batch is 1 / num_gpus large def test_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) loss = self . softmax ( out ) loss = nce_loss ( loss ) return { 'loss' : loss } # -------------- # with test_step_end to do softmax over the full batch def test_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) return { 'out' : out } def test_step_end ( self , outputs ): # this out is now the full size of the batch out = outputs [ 'out' ] # this softmax now uses the full batch size loss = nce_loss ( loss ) return { 'loss' : loss } See Also: See the :ref: multi-gpu-training guide for more details. View Source def test_step_end ( self , * args , ** kwargs ) -> Dict [ str , Tensor ] : \" \"\" Use this when testing with dp or ddp2 because :meth:`test_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth:`test_step` for each batch part. Return: Dict or OrderedDict - passed to the :meth:`test_epoch_end`. Examples: .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) loss = nce_loss(loss) return {'loss': loss} # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return {'out': out} def test_step_end(self, outputs): # this out is now the full size of the batch out = outputs['out'] # this softmax now uses the full batch size loss = nce_loss(loss) return {'loss': loss} See Also: See the :ref:`multi-gpu-training` guide for more details. \"\" \" tng_dataloader def tng_dataloader ( self ) Warnings: Deprecated in v0.5.0. Use :meth: train_dataloader instead. Will be removed in 1.0.0. View Source def tng_dataloader ( self ) : # todo: remove in v1.0.0 \" \"\" Warnings: Deprecated in v0.5.0. Use :meth:`train_dataloader` instead. Will be removed in 1.0.0. \"\" \" output = self . train_dataloader () rank_zero_warn ( \"`tng_dataloader` has been renamed to `train_dataloader` since v0.5.0.\" \" and this method will be removed in v1.0.0\" , DeprecationWarning ) return output to def to ( self , * args , ** kwargs ) -> torch . nn . modules . module . Module Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note: This method modifies the module in-place. Args: device: the desired device of the parameters and buffers in this module dtype: the desired floating point type of the floating point parameters and buffers in this module tensor: Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module Returns: Module: self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def init (self, weight: torch.Tensor): ... super(). init () ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) -> Module : \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note: This method modifies the module in-place. Args: device: the desired device of the parameters and buffers in this module dtype: the desired floating point type of the floating point parameters and buffers in this module tensor: Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module Returns: Module: self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def __init__(self, weight: torch.Tensor): ... super().__init__() ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) \"\" \" # there is diff nb vars in PT 1.5 out = torch . _C . _nn . _parse_to ( * args , ** kwargs ) device = out [ 0 ] dtype = out [ 1 ] if device is not None : self . _device = device if dtype is not None : self . _dtype = dtype return super (). to ( * args , ** kwargs ) train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self train_dataloader def train_dataloader ( self ) View Source def train_dataloader ( self ): train_loader = DataLoader ( dataset = self . train_dataset , batch_size = self . batch_size , collate_fn = self . train_dataset . collate_fn , shuffle = True , num_workers = cpu_count (), ) return train_loader training_end def training_end ( self , * args , ** kwargs ) Warnings: Deprecated in v0.7.0. Use :meth: training_step_end instead. View Source def training_end ( self , * args , ** kwargs ) : \" \"\" Warnings: Deprecated in v0.7.0. Use :meth:`training_step_end` instead. \"\" \" training_epoch_end def training_epoch_end ( self , outputs : Union [ List [ Dict [ str , torch . Tensor ]], List [ List [ Dict [ str , torch . Tensor ]]]] ) -> Dict [ str , Dict [ str , torch . Tensor ]] Called at the end of the training epoch with the outputs of all training steps. .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Args: outputs: List of outputs you defined in :meth: training_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: Dict or OrderedDict. May contain the following optional keys: - log (metrics to be added to the logger; only tensors) - progress_bar (dict for progress bar display) - any metric used in a callback (e.g. early stopping). Note: If this method is not overridden, this won't be called. The outputs here are strictly for logging or progress bar. If you don't need to display anything, don't return anything. If you want to manually set current step, you can specify the 'step' key in the 'log' dict. Examples: With a single dataloader: .. code - block :: python def training_epoch_end ( self , outputs ) : train_acc_mean = 0 for output in outputs : train_acc_mean += output [ 'train_acc' ] train_acc_mean /= len ( outputs ) # log training accuracy at the end of an epoch results = { 'log' : { 'train_acc' : train_acc_mean . item () } , 'progress_bar' : { 'train_acc' : train_acc_mean } , } return results With multiple dataloaders , ``outputs`` will be a list of lists . The outer list contains one entry per dataloader , while the inner list contains the individual outputs of each training step for that dataloader . .. code - block :: python def training_epoch_end ( self , outputs ) : train_acc_mean = 0 i = 0 for dataloader_outputs in outputs : for output in dataloader_outputs : train_acc_mean += output [ 'train_acc' ] i += 1 train_acc_mean /= i # log training accuracy at the end of an epoch results = { 'log' : { 'train_acc' : train_acc_mean . item (), 'step' : self . current_epoch } 'progress_bar' : { 'train_acc' : train_acc_mean } , } return results View Source def training_epoch_end ( self , outputs : Union [ List [ Dict [ str , Tensor ]] , List [ List [ Dict [ str , Tensor ]]]] ) -> Dict [ str , Dict [ str , Tensor ]] : \" \"\" Called at the end of the training epoch with the outputs of all training steps. .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Args: outputs: List of outputs you defined in :meth:`training_step`, or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: Dict or OrderedDict. May contain the following optional keys: - log (metrics to be added to the logger; only tensors) - progress_bar (dict for progress bar display) - any metric used in a callback (e.g. early stopping). Note: If this method is not overridden, this won't be called. - The outputs here are strictly for logging or progress bar. - If you don't need to display anything, don't return anything. - If you want to manually set current step, you can specify the 'step' key in the 'log' dict. Examples: With a single dataloader: .. code-block:: python def training_epoch_end(self, outputs): train_acc_mean = 0 for output in outputs: train_acc_mean += output['train_acc'] train_acc_mean /= len(outputs) # log training accuracy at the end of an epoch results = { 'log': {'train_acc': train_acc_mean.item()}, 'progress_bar': {'train_acc': train_acc_mean}, } return results With multiple dataloaders, ``outputs`` will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each training step for that dataloader. .. code-block:: python def training_epoch_end(self, outputs): train_acc_mean = 0 i = 0 for dataloader_outputs in outputs: for output in dataloader_outputs: train_acc_mean += output['train_acc'] i += 1 train_acc_mean /= i # log training accuracy at the end of an epoch results = { 'log': {'train_acc': train_acc_mean.item(), 'step': self.current_epoch} 'progress_bar': {'train_acc': train_acc_mean}, } return results \"\" \" training_step def training_step ( self , batch , batch_idx ) View Source def training_step ( self , batch , batch_idx ) : images , targets , _ = batch if self . train_rpn: targets = [{ \"boxes\" : t [ \"boxes\" ]} for t in targets ] _ , loss_dict = self ( images , targets = targets ) loss = sum ( loss for loss in loss_dict . values ()) return OrderedDict ({ \"loss\" : loss , \"progress_bar\" : loss_dict , \"log\" : loss_dict }) elif self . train_roi: _ , loss_dict = self ( images , targets = targets ) loss = sum ( loss for loss in loss_dict . values ()) return OrderedDict ({ \"loss\" : loss , \"progress_bar\" : loss_dict , \"log\" : loss_dict }) else : images = list ( image for image in images ) targets = [{ k: v for k , v in t . items ()} for t in targets ] loss_dict = self ( images , targets = targets ) # loss keys: [' loss_classifier ', ' loss_box_reg ', ' loss_objectness ', ' loss_rpn_box_reg '] loss = sum ( loss for loss in loss_dict . values ()) if not math . isfinite ( loss . item ()) : sys . exit ( 1 ) return OrderedDict ({ \"loss\" : loss , \"progress_bar\" : loss_dict , \"log\" : loss_dict }) training_step_end def training_step_end ( self , * args , ** kwargs ) -> Dict [ str , Union [ torch . Tensor , Dict [ str , torch . Tensor ]]] Use this when training with dp or ddp2 because :meth: training_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in training_step for each batch part. Return: Dict with loss key and optional log or progress bar keys. - loss -> tensor scalar **REQUIRED** - progress_bar -> Dict for progress bar display. Must have only tensors - log -> Dict of metrics to add to logger. Must have only tensors (no images, etc) Examples: .. code-block:: python # WITHOUT training_step_end # if used in DP or DDP2 , this batch is 1 / num_gpus large def training_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) loss = self . softmax ( out ) loss = nce_loss ( loss ) return { 'loss' : loss } # -------------- # with training_step_end to do softmax over the full batch def training_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) return { 'out' : out } def training_step_end ( self , outputs ): # this out is now the full size of the batch out = outputs [ 'out' ] # this softmax now uses the full batch size loss = nce_loss ( loss ) return { 'loss' : loss } See Also: See the :ref: multi-gpu-training guide for more details. View Source def training_step_end ( self , * args , ** kwargs ) -> Dict [ str , Union [ Tensor , Dict [ str , Tensor ]] ] : \" \"\" Use this when training with dp or ddp2 because :meth:`training_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in `training_step` for each batch part. Return: Dict with loss key and optional log or progress bar keys. - loss -> tensor scalar **REQUIRED** - progress_bar -> Dict for progress bar display. Must have only tensors - log -> Dict of metrics to add to logger. Must have only tensors (no images, etc) Examples: .. code-block:: python # WITHOUT training_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) loss = nce_loss(loss) return {'loss': loss} # -------------- # with training_step_end to do softmax over the full batch def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return {'out': out} def training_step_end(self, outputs): # this out is now the full size of the batch out = outputs['out'] # this softmax now uses the full batch size loss = nce_loss(loss) return {'loss': loss} See Also: See the :ref:`multi-gpu-training` guide for more details. \"\" \" transfer_batch_to_device def transfer_batch_to_device ( self , batch : Any , device : torch . device ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Example:: def transfer_batch_to_device(self, batch, device) if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) else: batch = super().transfer_batch_to_device(data, device) return batch Args: batch: A batch of data that needs to be transferred to a new device. device: The target device as defined in PyTorch. Returns: A reference to the data on the new device. Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). The :class: ~pytorch_lightning.trainer.trainer.Trainer already takes care of splitting the batch and determines the target devices. See Also: - :func: ~pytorch_lightning.utilities.apply_func.move_data_to_device - :func: ~pytorch_lightning.utilities.apply_func.apply_to_collection View Source def transfer_batch_to_device ( self , batch : Any , device : torch . device ) -> Any : \" \"\" Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: - :class:`torch.Tensor` or anything that implements `.to(...)` - :class:`list` - :class:`dict` - :class:`tuple` - :class:`torchtext.data.batch.Batch` For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Example:: def transfer_batch_to_device(self, batch, device) if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) else: batch = super().transfer_batch_to_device(data, device) return batch Args: batch: A batch of data that needs to be transferred to a new device. device: The target device as defined in PyTorch. Returns: A reference to the data on the new device. Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). The :class:`~pytorch_lightning.trainer.trainer.Trainer` already takes care of splitting the batch and determines the target devices. See Also: - :func:`~pytorch_lightning.utilities.apply_func.move_data_to_device` - :func:`~pytorch_lightning.utilities.apply_func.apply_to_collection` \"\" \" return move_data_to_device ( batch , device ) type def type ( self , dst_type : Union [ str , torch . dtype ] ) -> torch . nn . modules . module . Module Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self , dst_type : Union [ str , torch . dtype ] ) -> Module : \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" self . _dtype = dst_type return super (). type ( dst_type = dst_type ) unfreeze def unfreeze ( self ) -> None Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() View Source def unfreeze ( self ) -> None : \"\"\" Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() \"\"\" for param in self . parameters (): param . requires_grad = True self . train () val_dataloader def val_dataloader ( self ) View Source def val_dataloader ( self ): val_loader = DataLoader ( self . val_dataset , batch_size = self . batch_size , collate_fn = self . val_dataset . collate_fn , shuffle = False , num_workers = cpu_count () // 2 , ) self . coco_evaluator = self . _get_evaluator ( val_loader . dataset ) return val_loader validation_end def validation_end ( self , outputs ) Warnings: Deprecated in v0.7.0. Use :meth: validation_epoch_end instead. Will be removed in 1.0.0. View Source def validation_end ( self , outputs ) : \" \"\" Warnings: Deprecated in v0.7.0. Use :meth:`validation_epoch_end` instead. Will be removed in 1.0.0. \"\" \" validation_epoch_end def validation_epoch_end ( self , outputs ) View Source @auto_move_data def validation_epoch_end ( self , outputs ) : if self . train_rpn : return {} elif self . train_roi : # TODO : above return {} else : self . coco_evaluator . synchronize_between_processes () self . coco_evaluator . accumulate () self . coco_evaluator . summarize () metric = self . coco_evaluator . coco_eval [ \"bbox\" ] . stats [ 0 ] metric = torch . as_tensor ( metric ) tensorboard_logs = { \"main_score\" : metric } self . coco_evaluator = self . _get_evaluator ( self . val_dataset ) # need to update for the new evaluation return { \"val_loss\" : metric , \"log\" : tensorboard_logs , \"progress_bar\" : tensorboard_logs } validation_step def validation_step ( self , batch , batch_idx ) View Source @auto_move_data def validation_step ( self , batch , batch_idx ) : images , targets , _ = batch if self . train_rpn : # rpn doesn ' t compute loss for val return {} elif self . train_roi : # TODO : scores are predictions scores , not a metric ! iou ? + acc ? return {} else : images = list ( image for image in images ) targets = [ {k: v for k, v in t.items()} for t in targets ] outputs = self ( images , targets = targets ) ret = { target [ \"image_id\" ] . item () : output for target , output in zip ( targets , outputs ) } self . coco_evaluator . update ( ret ) return {} validation_step_end def validation_step_end ( self , * args , ** kwargs ) -> Dict [ str , torch . Tensor ] Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: validation_step for each batch part. Return: Dict or OrderedDict - passed to the :meth: validation_epoch_end method. Examples: .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2 , this batch is 1 / num_gpus large def validation_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) loss = self . softmax ( out ) loss = nce_loss ( loss ) return { 'loss' : loss } # -------------- # with validation_step_end to do softmax over the full batch def validation_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) return { 'out' : out } def validation_epoch_end ( self , outputs ): # this out is now the full size of the batch out = outputs [ 'out' ] # this softmax now uses the full batch size loss = nce_loss ( loss ) return { 'loss' : loss } See Also: See the :ref: multi-gpu-training guide for more details. View Source def validation_step_end ( self , * args , ** kwargs ) -> Dict [ str , Tensor ] : \" \"\" Use this when validating with dp or ddp2 because :meth:`validation_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth:`validation_step` for each batch part. Return: Dict or OrderedDict - passed to the :meth:`validation_epoch_end` method. Examples: .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) loss = nce_loss(loss) return {'loss': loss} # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return {'out': out} def validation_epoch_end(self, outputs): # this out is now the full size of the batch out = outputs['out'] # this softmax now uses the full batch size loss = nce_loss(loss) return {'loss': loss} See Also: See the :ref:`multi-gpu-training` guide for more details. \"\" \" zero_grad def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ ()","title":"Detector"},{"location":"reference/sagemaker_defect_detection/detector/#module-sagemaker_defect_detectiondetector","text":"View Source # mypy: ignore-errors from typing import Optional from pathlib import Path from os import path as osp from collections import OrderedDict from argparse import ArgumentParser , Namespace from multiprocessing import cpu_count import os import math import sys import numpy as np import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler from torch.utils.data import DataLoader from torchvision.models.detection.image_list import ImageList import pytorch_lightning as pl from pytorch_lightning.core.decorators import auto_move_data from pytorch_lightning.callbacks import EarlyStopping , ModelCheckpoint from sagemaker_defect_detection import Detection , NEUDET , Classification , RPN , RoI , get_augmentation , get_preprocess from sagemaker_defect_detection.utils.coco_eval import CocoEvaluator from sagemaker_defect_detection.utils.coco_utils import convert_to_coco_api from sagemaker_defect_detection.utils import freeze , load_checkpoint class DDNDetection ( pl . LightningModule ): def __init__ ( self , train_rpn : bool , train_roi : bool , finetune_rpn : bool , finetune_roi : bool , data_path : str , backbone : str , num_classes : int , learning_rate : float , batch_size : int , momentum : float , weight_decay : float , seed : int , pretrained_mfn_ckpt : Optional [ str ] = None , pretrained_rpn_ckpt : Optional [ str ] = None , pretrained_roi_ckpt : Optional [ str ] = None , finetuned_rpn_ckpt : Optional [ str ] = None , finetuned_roi_ckpt : Optional [ str ] = None , resume_sagemaker_from_checkpoint : Optional [ str ] = None , ** kwargs , ) -> None : super () . __init__ () self . train_rpn = train_rpn self . train_roi = train_roi self . finetune_rpn = finetune_rpn self . finetune_roi = finetune_roi self . data_path = data_path self . backbone = backbone self . num_classes = num_classes self . learning_rate = learning_rate self . batch_size = batch_size self . momentum = momentum self . weight_decay = weight_decay self . seed = seed self . train_dataset = NEUDET ( self . data_path , split = \"train\" , augmentation = get_augmentation ( \"train\" ), preprocess = get_preprocess (), seed = self . seed , ) self . val_dataset = NEUDET ( self . data_path , split = \"val\" , augmentation = get_augmentation ( \"val\" ), preprocess = get_preprocess (), seed = self . seed , ) self . pretrained_mfn_ckpt = pretrained_mfn_ckpt self . pretrained_rpn_ckpt = pretrained_rpn_ckpt self . pretrained_roi_ckpt = pretrained_roi_ckpt self . finetuned_rpn_ckpt = finetuned_rpn_ckpt self . finetuned_roi_ckpt = finetuned_roi_ckpt self . resume_sagemaker_from_checkpoint = resume_sagemaker_from_checkpoint self . coco_evaluator = self . _get_evaluator ( self . val_dataset ) def setup ( self , stage ) -> None : if self . train_rpn : # step 2 self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . pretrained_mfn_ckpt , \"model.mfn\" ) self . rpn = RPN () elif self . train_roi : # step 3 self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . pretrained_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . pretrained_rpn_ckpt , prefix = \"rpn\" ) freeze ( self . rpn ) self . roi = RoI ( self . num_classes ) elif self . finetune_rpn : # step 4 or extra finetune rpn if self . finetuned_rpn_ckpt and self . finetuned_roi_ckpt : # extra finetune rpn self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . finetuned_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . finetuned_rpn_ckpt , prefix = \"rpn\" ) self . roi = load_checkpoint ( RoI ( self . num_classes ), self . finetuned_roi_ckpt , prefix = \"roi\" ) freeze ( self . roi ) self . model = Detection ( self . mfn , self . rpn , self . roi ) else : self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . pretrained_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . pretrained_rpn_ckpt , prefix = \"rpn\" ) self . roi = load_checkpoint ( RoI ( self . num_classes ), self . pretrained_roi_ckpt , prefix = \"roi\" ) freeze ( self . roi ) self . model = Detection ( self . mfn , self . rpn , self . roi ) elif self . finetune_roi : # step 5 or extra finetune roi if self . finetuned_rpn_ckpt and self . finetuned_roi_ckpt : # extra finetune roi self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . finetuned_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . finetuned_rpn_ckpt , prefix = \"rpn\" ) freeze ( self . rpn ) self . roi = load_checkpoint ( RoI ( self . num_classes ), self . finetuned_roi_ckpt , prefix = \"roi\" ) self . model = Detection ( self . mfn , self . rpn , self . roi ) else : self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . finetuned_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . finetuned_rpn_ckpt , prefix = \"rpn\" ) freeze ( self . rpn ) self . roi = load_checkpoint ( RoI ( self . num_classes ), self . pretrained_roi_ckpt , prefix = \"roi\" ) self . model = Detection ( self . mfn , self . rpn , self . roi ) else : # step 6: final/joint model load_checkpoint_fn = load_checkpoint if self . finetuned_roi_ckpt is not None : ckpt_path = self . finetuned_rpn_ckpt elif self . resume_sagemaker_from_checkpoint is not None : ckpt_path = self . resume_sagemaker_from_checkpoint else : ckpt_path = None # ignore load_checkpoint load_checkpoint_fn = lambda * args : args [ 0 ] self . mfn = load_checkpoint_fn ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , ckpt_path , \"mfn\" ) self . rpn = load_checkpoint_fn ( RPN (), ckpt_path , \"rpn\" ) self . roi = load_checkpoint_fn ( RoI ( self . num_classes ), ckpt_path , \"roi\" ) self . model = Detection ( self . mfn , self . rpn , self . roi ) return @auto_move_data def forward ( self , images , * args , ** kwargs ): if self . train_rpn : # step 2 images = torch . stack ( images ) features = self . mfn ( images ) features = OrderedDict ({ str ( i ): t . unsqueeze ( 0 ) for i , t in enumerate ( features )}) images = ImageList ( images , [( 224 , 224 )]) return self . rpn ( images , features , targets = kwargs . get ( \"targets\" )) elif self . train_roi : # step 3 self . mfn . eval () self . rpn . eval () images = torch . stack ( images ) features = self . mfn ( images ) features = OrderedDict ({ str ( i ): t . unsqueeze ( 0 ) for i , t in enumerate ( features )}) images = ImageList ( images , [( 224 , 224 )]) proposals , _ = self . rpn ( images , features , targets = None ) return self . roi ( features , proposals , [( 224 , 224 )], targets = kwargs . get ( \"targets\" )) elif self . finetune_rpn : self . model . backbone . eval () self . model . roi_heads . eval () return self . model ( images , targets = kwargs . get ( \"targets\" )) elif self . finetune_roi : self . model . backbone . eval () self . model . rpn . eval () return self . model ( images , targets = kwargs . get ( \"targets\" )) else : return self . model ( images , targets = kwargs . get ( \"targets\" )) def _get_evaluator ( self , dataset ): coco = convert_to_coco_api ( dataset ) return CocoEvaluator ( coco , [ \"bbox\" ]) def train_dataloader ( self ): train_loader = DataLoader ( dataset = self . train_dataset , batch_size = self . batch_size , collate_fn = self . train_dataset . collate_fn , shuffle = True , num_workers = cpu_count (), ) return train_loader def val_dataloader ( self ): val_loader = DataLoader ( self . val_dataset , batch_size = self . batch_size , collate_fn = self . val_dataset . collate_fn , shuffle = False , num_workers = cpu_count () // 2 , ) self . coco_evaluator = self . _get_evaluator ( val_loader . dataset ) return val_loader def configure_optimizers ( self ): params = [ p for p in self . parameters () if p . requires_grad ] optimizer = optim . SGD ( params , lr = self . learning_rate , momentum = self . momentum , weight_decay = self . weight_decay ) return optimizer def training_step ( self , batch , batch_idx ): images , targets , _ = batch if self . train_rpn : targets = [{ \"boxes\" : t [ \"boxes\" ]} for t in targets ] _ , loss_dict = self ( images , targets = targets ) loss = sum ( loss for loss in loss_dict . values ()) return OrderedDict ({ \"loss\" : loss , \"progress_bar\" : loss_dict , \"log\" : loss_dict }) elif self . train_roi : _ , loss_dict = self ( images , targets = targets ) loss = sum ( loss for loss in loss_dict . values ()) return OrderedDict ({ \"loss\" : loss , \"progress_bar\" : loss_dict , \"log\" : loss_dict }) else : images = list ( image for image in images ) targets = [{ k : v for k , v in t . items ()} for t in targets ] loss_dict = self ( images , targets = targets ) # loss keys: ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg'] loss = sum ( loss for loss in loss_dict . values ()) if not math . isfinite ( loss . item ()): sys . exit ( 1 ) return OrderedDict ({ \"loss\" : loss , \"progress_bar\" : loss_dict , \"log\" : loss_dict }) @auto_move_data def validation_step ( self , batch , batch_idx ): images , targets , _ = batch if self . train_rpn : # rpn doesn't compute loss for val return {} elif self . train_roi : # TODO: scores are predictions scores, not a metric! iou? + acc? return {} else : images = list ( image for image in images ) targets = [{ k : v for k , v in t . items ()} for t in targets ] outputs = self ( images , targets = targets ) ret = { target [ \"image_id\" ] . item (): output for target , output in zip ( targets , outputs )} self . coco_evaluator . update ( ret ) return {} @auto_move_data def validation_epoch_end ( self , outputs ): if self . train_rpn : return {} elif self . train_roi : # TODO: above return {} else : self . coco_evaluator . synchronize_between_processes () self . coco_evaluator . accumulate () self . coco_evaluator . summarize () metric = self . coco_evaluator . coco_eval [ \"bbox\" ] . stats [ 0 ] metric = torch . as_tensor ( metric ) tensorboard_logs = { \"main_score\" : metric } self . coco_evaluator = self . _get_evaluator ( self . val_dataset ) # need to update for the new evaluation return { \"val_loss\" : metric , \"log\" : tensorboard_logs , \"progress_bar\" : tensorboard_logs } @staticmethod def add_model_specific_args ( parent_parser ): # pragma: no-cover parser = ArgumentParser ( parents = [ parent_parser ], add_help = False ) aa = parser . add_argument aa ( \"--train-rpn\" , action = \"store_true\" ) aa ( \"--train-roi\" , action = \"store_true\" ) aa ( \"--finetune-rpn\" , action = \"store_true\" ) aa ( \"--finetune-roi\" , action = \"store_true\" ) aa ( \"--data-path\" , metavar = \"DIR\" , type = str , default = os . environ [ \"SM_CHANNEL_TRAINING\" ]) aa ( \"--backbone\" , default = \"resnet34\" , help = \"backbone model either resnet34 (default) or resnet50\" ) aa ( \"--num-classes\" , default = 7 , type = int , metavar = \"N\" , help = \"number of classes including the background\" ) aa ( \"-b\" , \"--batch-size\" , default = 16 , type = int , metavar = \"N\" , help = \"mini-batch size (default: 16), this is the total \" \"batch size of all GPUs on the current node when \" \"using Data Parallel or Distributed Data Parallel\" , ) aa ( \"--lr\" , \"--learning-rate\" , default = 1e-3 , type = float , metavar = \"LR\" , help = \"initial learning rate\" , dest = \"learning_rate\" , ) aa ( \"--momentum\" , default = 0.9 , type = float , metavar = \"M\" , help = \"momentum\" ) aa ( \"--wd\" , \"--weight-decay\" , default = 1e-4 , type = float , metavar = \"W\" , help = \"weight decay (default: 1e-4)\" , dest = \"weight_decay\" , ) aa ( \"--seed\" , type = int , default = 123 , help = \"seed for initializing training\" ) aa ( \"--pretrained-mfn-ckpt\" , type = str ) aa ( \"--pretrained-rpn-ckpt\" , type = str ) aa ( \"--pretrained-roi-ckpt\" , type = str ) aa ( \"--finetuned-rpn-ckpt\" , type = str ) aa ( \"--finetuned-roi-ckpt\" , type = str ) aa ( \"--resume-from-checkpoint\" , type = str ) aa ( \"--resume-sagemaker-from-checkpoint\" , type = str , default = os . getenv ( \"SM_CHANNEL_PRETRAINED_CHECKPOINT\" , None )) return parser def get_args (): parent_parser = ArgumentParser ( add_help = False ) aa = parent_parser . add_argument aa ( \"--epochs\" , type = int , default = 1 , help = \"number of training epochs\" ) aa ( \"--save-path\" , metavar = \"DIR\" , default = os . environ [ \"SM_MODEL_DIR\" ], type = str , help = \"path to save output\" ) aa ( \"--gpus\" , type = int , default = os . getenv ( \"SM_NUM_GPUS\" , 1 ), help = \"how many gpus\" ) aa ( \"--distributed-backend\" , type = str , default = \"\" , choices = ( \"dp\" , \"ddp\" , \"ddp2\" ), help = \"supports three options dp, ddp, ddp2\" , ) # aa(\"--use-16bit\", dest=\"use_16bit\", action=\"store_true\", help=\"if true uses 16 bit precision\") parser = DDNDetection . add_model_specific_args ( parent_parser ) return parser . parse_args () def model_fn ( model_dir ): # TODO: `model_fn` doesn't get more args # see: https://github.com/aws/sagemaker-inference-toolkit/issues/65 backbone = \"resnet34\" num_classes = 7 # including the background mfn = load_checkpoint ( Classification ( backbone , num_classes - 1 ) . mfn , model_dir , \"mfn\" ) rpn = load_checkpoint ( RPN (), model_dir , \"rpn\" ) roi = load_checkpoint ( RoI ( num_classes ), model_dir , \"roi\" ) model = Detection ( mfn , rpn , roi ) model = model . eval () freeze ( model ) return model def main ( args : Namespace ) -> None : ddn = DDNDetection ( ** vars ( args )) if args . seed is not None : pl . seed_everything ( args . seed ) # doesn't do multi-gpu if torch . cuda . device_count () > 1 : torch . cuda . manual_seed_all ( args . seed ) # TODO: add deterministic training # torch.backends.cudnn.deterministic = True if ddn . train_rpn : checkpoint_callback = ModelCheckpoint ( filepath = os . path . join ( args . save_path , \" {epoch} - {loss:.3f} \" ), save_top_k = 1 , verbose = True , monitor = \"loss\" , mode = \"min\" , ) early_stop_callback = None elif ddn . train_roi : checkpoint_callback = ModelCheckpoint ( filepath = os . path . join ( args . save_path , \" {epoch} - {loss:.3f} \" ), save_top_k = 1 , verbose = True , monitor = \"loss\" , mode = \"min\" , ) early_stop_callback = None else : checkpoint_callback = ModelCheckpoint ( filepath = os . path . join ( args . save_path , \" {epoch} - {loss:.3f} - {main_score:.3f} \" ), save_top_k = 1 , verbose = True , monitor = \"main_score\" , mode = \"max\" , ) early_stop_callback = EarlyStopping ( \"main_score\" , patience = 50 , mode = \"max\" ) trainer = pl . Trainer ( default_root_dir = args . save_path , num_sanity_val_steps = 1 , limit_val_batches = 1.0 , gpus = args . gpus , max_epochs = args . epochs , early_stop_callback = early_stop_callback , checkpoint_callback = checkpoint_callback , distributed_backend = args . distributed_backend or None , # precision=16 if args.use_16bit else 32, # TODO: apex weights_summary = \"top\" , resume_from_checkpoint = None if args . resume_from_checkpoint == \"\" else args . resume_from_checkpoint , ) trainer . fit ( ddn ) return if __name__ == \"__main__\" : main ( get_args ())","title":"Module sagemaker_defect_detection.detector"},{"location":"reference/sagemaker_defect_detection/detector/#functions","text":"","title":"Functions"},{"location":"reference/sagemaker_defect_detection/detector/#get_args","text":"def get_args ( ) View Source def get_args (): parent_parser = ArgumentParser ( add_help = False ) aa = parent_parser . add_argument aa ( \"--epochs\" , type = int , default = 1 , help = \"number of training epochs\" ) aa ( \"--save-path\" , metavar = \"DIR\" , default = os . environ [ \"SM_MODEL_DIR\" ], type = str , help = \"path to save output\" ) aa ( \"--gpus\" , type = int , default = os . getenv ( \"SM_NUM_GPUS\" , 1 ), help = \"how many gpus\" ) aa ( \"--distributed-backend\" , type = str , default = \"\" , choices = ( \"dp\" , \"ddp\" , \"ddp2\" ), help = \"supports three options dp, ddp, ddp2\" , ) # aa(\"--use-16bit\", dest=\"use_16bit\", action=\"store_true\", help=\"if true uses 16 bit precision\") parser = DDNDetection . add_model_specific_args ( parent_parser ) return parser . parse_args ()","title":"get_args"},{"location":"reference/sagemaker_defect_detection/detector/#main","text":"def main ( args : argparse . Namespace ) -> None View Source def main ( args : Namespace ) -> None : ddn = DDNDetection ( ** vars ( args )) if args . seed is not None : pl . seed_everything ( args . seed ) # doesn't do multi-gpu if torch . cuda . device_count () > 1 : torch . cuda . manual_seed_all ( args . seed ) # TODO: add deterministic training # torch.backends.cudnn.deterministic = True if ddn . train_rpn : checkpoint_callback = ModelCheckpoint ( filepath = os . path . join ( args . save_path , \"{epoch}-{loss:.3f}\" ), save_top_k = 1 , verbose = True , monitor = \"loss\" , mode = \"min\" , ) early_stop_callback = None elif ddn . train_roi : checkpoint_callback = ModelCheckpoint ( filepath = os . path . join ( args . save_path , \"{epoch}-{loss:.3f}\" ), save_top_k = 1 , verbose = True , monitor = \"loss\" , mode = \"min\" , ) early_stop_callback = None else : checkpoint_callback = ModelCheckpoint ( filepath = os . path . join ( args . save_path , \"{epoch}-{loss:.3f}-{main_score:.3f}\" ), save_top_k = 1 , verbose = True , monitor = \"main_score\" , mode = \"max\" , ) early_stop_callback = EarlyStopping ( \"main_score\" , patience = 50 , mode = \"max\" ) trainer = pl . Trainer ( default_root_dir = args . save_path , num_sanity_val_steps = 1 , limit_val_batches = 1.0 , gpus = args . gpus , max_epochs = args . epochs , early_stop_callback = early_stop_callback , checkpoint_callback = checkpoint_callback , distributed_backend = args . distributed_backend or None , # precision=16 if args.use_16bit else 32, # TODO: apex weights_summary = \"top\" , resume_from_checkpoint = None if args . resume_from_checkpoint == \"\" else args . resume_from_checkpoint , ) trainer . fit ( ddn ) return","title":"main"},{"location":"reference/sagemaker_defect_detection/detector/#model_fn","text":"def model_fn ( model_dir ) View Source def model_fn ( model_dir ): # TODO: `model_fn` doesn't get more args # see: https://github.com/aws/sagemaker-inference-toolkit/issues/65 backbone = \"resnet34\" num_classes = 7 # including the background mfn = load_checkpoint ( Classification ( backbone , num_classes - 1 ) . mfn , model_dir , \"mfn\" ) rpn = load_checkpoint ( RPN (), model_dir , \"rpn\" ) roi = load_checkpoint ( RoI ( num_classes ), model_dir , \"roi\" ) model = Detection ( mfn , rpn , roi ) model = model . eval () freeze ( model ) return model","title":"model_fn"},{"location":"reference/sagemaker_defect_detection/detector/#classes","text":"","title":"Classes"},{"location":"reference/sagemaker_defect_detection/detector/#ddndetection","text":"class DDNDetection ( train_rpn : bool , train_roi : bool , finetune_rpn : bool , finetune_roi : bool , data_path : str , backbone : str , num_classes : int , learning_rate : float , batch_size : int , momentum : float , weight_decay : float , seed : int , pretrained_mfn_ckpt : Union [ str , NoneType ] = None , pretrained_rpn_ckpt : Union [ str , NoneType ] = None , pretrained_roi_ckpt : Union [ str , NoneType ] = None , finetuned_rpn_ckpt : Union [ str , NoneType ] = None , finetuned_roi_ckpt : Union [ str , NoneType ] = None , resume_sagemaker_from_checkpoint : Union [ str , NoneType ] = None , ** kwargs ) Helper class that provides a standard way to create an ABC using inheritance.","title":"DDNDetection"},{"location":"reference/sagemaker_defect_detection/detector/#ancestors-in-mro","text":"pytorch_lightning.core.lightning.LightningModule abc.ABC pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin pytorch_lightning.core.grads.GradInformation pytorch_lightning.core.saving.ModelIO pytorch_lightning.core.hooks.ModelHooks torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/sagemaker_defect_detection/detector/#class-variables","text":"CHECKPOINT_HYPER_PARAMS_KEY CHECKPOINT_HYPER_PARAMS_NAME CHECKPOINT_HYPER_PARAMS_TYPE T_destination dump_patches","title":"Class variables"},{"location":"reference/sagemaker_defect_detection/detector/#static-methods","text":"","title":"Static methods"},{"location":"reference/sagemaker_defect_detection/detector/#add_model_specific_args","text":"def add_model_specific_args ( parent_parser ) View Source @staticmethod def add_model_specific_args ( parent_parser ) : # pragma : no - cover parser = ArgumentParser ( parents =[ parent_parser ] , add_help = False ) aa = parser . add_argument aa ( \"--train-rpn\" , action = \"store_true\" ) aa ( \"--train-roi\" , action = \"store_true\" ) aa ( \"--finetune-rpn\" , action = \"store_true\" ) aa ( \"--finetune-roi\" , action = \"store_true\" ) aa ( \"--data-path\" , metavar = \"DIR\" , type = str , default = os . environ [ \"SM_CHANNEL_TRAINING\" ] ) aa ( \"--backbone\" , default = \"resnet34\" , help = \"backbone model either resnet34 (default) or resnet50\" ) aa ( \"--num-classes\" , default = 7 , type = int , metavar = \"N\" , help = \"number of classes including the background\" ) aa ( \"-b\" , \"--batch-size\" , default = 16 , type = int , metavar = \"N\" , help = \"mini-batch size (default: 16), this is the total \" \"batch size of all GPUs on the current node when \" \"using Data Parallel or Distributed Data Parallel\" , ) aa ( \"--lr\" , \"--learning-rate\" , default = 1e-3 , type = float , metavar = \"LR\" , help = \"initial learning rate\" , dest = \"learning_rate\" , ) aa ( \"--momentum\" , default = 0.9 , type = float , metavar = \"M\" , help = \"momentum\" ) aa ( \"--wd\" , \"--weight-decay\" , default = 1e-4 , type = float , metavar = \"W\" , help = \"weight decay (default: 1e-4)\" , dest = \"weight_decay\" , ) aa ( \"--seed\" , type = int , default = 123 , help = \"seed for initializing training\" ) aa ( \"--pretrained-mfn-ckpt\" , type = str ) aa ( \"--pretrained-rpn-ckpt\" , type = str ) aa ( \"--pretrained-roi-ckpt\" , type = str ) aa ( \"--finetuned-rpn-ckpt\" , type = str ) aa ( \"--finetuned-roi-ckpt\" , type = str ) aa ( \"--resume-from-checkpoint\" , type = str ) aa ( \"--resume-sagemaker-from-checkpoint\" , type = str , default = os . getenv ( \"SM_CHANNEL_PRETRAINED_CHECKPOINT\" , None )) return parser","title":"add_model_specific_args"},{"location":"reference/sagemaker_defect_detection/detector/#load_from_checkpoint","text":"def load_from_checkpoint ( checkpoint_path : str , * args , map_location : Union [ Dict [ str , str ], str , torch . device , int , Callable , NoneType ] = None , hparams_file : Union [ str , NoneType ] = None , tags_csv : Union [ str , NoneType ] = None , ** kwargs ) Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to __init__ in the checkpoint under module_arguments Any arguments specified through *args and **kwargs will override args stored in hparams . Args: checkpoint_path: Path to checkpoint. This can also be a URL. args: Any positional args needed to init the model. map_location: If your checkpoint saved a GPU model and you now load on CPUs or a different number of GPUs, use this to map to the new setup. The behaviour is the same as in :func: torch.load . hparams_file: Optional path to a .yaml file with hierarchical structure as in this example:: drop_prob : 0.2 dataloader : batch_size : 32 You most likely won 't need this since Lightning will always save the hyperparameters to the checkpoint. However, if your checkpoint weights don' t have the hyperparameters saved , use this method to pass in a . yaml file with the hparams you 'd like to use. These will be converted into a :class:`~dict` and passed into your :class:`LightningModule` for use. If your model' s `hparams` argument is : class : `~argparse.Namespace` and . yaml file has hierarchical structure , you need to refactor your model to treat `hparams` as : class : `~dict` . . csv files are acceptable here till v0 .9.0 , see tags_csv argument for detailed usage . tags_csv : .. warning :: .. deprecated :: 0.7.6 `tags_csv` argument is deprecated in v0 .7.6 . Will be removed v0 .9.0 . Optional path to a . csv file with two columns ( key , value ) as in this example :: key , value drop_prob , 0.2 batch_size , 32 Use this method to pass in a . csv file with the hparams you 'd like to use. hparam_overrides: A dictionary with keys to override in the hparams kwargs: Any keyword args needed to init the model. Return: :class: LightningModule with loaded weights and hyperparameters (if available). Example: .. code-block:: python # load weights without mapping ... MyLightningModule . load_from_checkpoint ( 'path/to/checkpoint.ckpt' ) # or load weights mapping all weights from GPU 1 to GPU 0 ... map_location = { 'cuda:1' : 'cuda:0' } MyLightningModule . load_from_checkpoint ( 'path/to/checkpoint.ckpt' , map_location = map_location ) # or load weights and hyperparameters from separate files. MyLightningModule . load_from_checkpoint ( 'path/to/checkpoint.ckpt' , hparams_file = '/path/to/hparams_file.yaml' ) # override some of the params with new values MyLightningModule . load_from_checkpoint ( PATH , num_layers = 128 , pretrained_ckpt_path : NEW_PATH , ) # predict pretrained_model . eval () pretrained_model . freeze () y_hat = pretrained_model ( x ) View Source @classmethod def load_from_checkpoint ( cls , checkpoint_path : str , * args , map_location : Optional [ Union [ Dict [ str , str ] , str , torch . device , int , Callable ]] = None , hparams_file : Optional [ str ] = None , tags_csv : Optional [ str ] = None , # backward compatible, todo: remove in v0.9.0 ** kwargs ) : r \" \"\" Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to `__init__` in the checkpoint under `module_arguments` Any arguments specified through \\ *args and \\ * \\ *kwargs will override args stored in `hparams`. Args: checkpoint_path: Path to checkpoint. This can also be a URL. args: Any positional args needed to init the model. map_location: If your checkpoint saved a GPU model and you now load on CPUs or a different number of GPUs, use this to map to the new setup. The behaviour is the same as in :func:`torch.load`. hparams_file: Optional path to a .yaml file with hierarchical structure as in this example:: drop_prob: 0.2 dataloader: batch_size: 32 You most likely won't need this since Lightning will always save the hyperparameters to the checkpoint. However, if your checkpoint weights don't have the hyperparameters saved, use this method to pass in a .yaml file with the hparams you'd like to use. These will be converted into a :class:`~dict` and passed into your :class:`LightningModule` for use. If your model's `hparams` argument is :class:`~argparse.Namespace` and .yaml file has hierarchical structure, you need to refactor your model to treat `hparams` as :class:`~dict`. .csv files are acceptable here till v0.9.0, see tags_csv argument for detailed usage. tags_csv: .. warning:: .. deprecated:: 0.7.6 `tags_csv` argument is deprecated in v0.7.6. Will be removed v0.9.0. Optional path to a .csv file with two columns (key, value) as in this example:: key,value drop_prob,0.2 batch_size,32 Use this method to pass in a .csv file with the hparams you'd like to use. hparam_overrides: A dictionary with keys to override in the hparams kwargs: Any keyword args needed to init the model. Return: :class:`LightningModule` with loaded weights and hyperparameters (if available). Example: .. code-block:: python # load weights without mapping ... MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt') # or load weights mapping all weights from GPU 1 to GPU 0 ... map_location = {'cuda:1':'cuda:0'} MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', map_location=map_location ) # or load weights and hyperparameters from separate files. MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', hparams_file='/path/to/hparams_file.yaml' ) # override some of the params with new values MyLightningModule.load_from_checkpoint( PATH, num_layers=128, pretrained_ckpt_path: NEW_PATH, ) # predict pretrained_model.eval() pretrained_model.freeze() y_hat = pretrained_model(x) \"\" \" if map_location is not None : checkpoint = pl_load ( checkpoint_path , map_location = map_location ) else : checkpoint = pl_load ( checkpoint_path , map_location = lambda storage , loc : storage ) # add the hparams from csv file to checkpoint if tags_csv is not None : hparams_file = tags_csv rank_zero_warn ( '`tags_csv` argument is deprecated in v0.7.6. Will be removed v0.9.0' , DeprecationWarning ) if hparams_file is not None : extension = hparams_file . split ( '.' ) [ - 1 ] if extension . lower () in ( 'csv' ) : hparams = load_hparams_from_tags_csv ( hparams_file ) elif extension . lower () in ( 'yml' , 'yaml' ) : hparams = load_hparams_from_yaml ( hparams_file ) else : raise ValueError ( '.csv, .yml or .yaml is required for `hparams_file`' ) hparams [ 'on_gpu' ] = False # overwrite hparams by the given file checkpoint [ cls . CHECKPOINT_HYPER_PARAMS_KEY ] = hparams # for past checkpoint need to add the new key if cls . CHECKPOINT_HYPER_PARAMS_KEY not in checkpoint : checkpoint [ cls . CHECKPOINT_HYPER_PARAMS_KEY ] = {} # override the hparams with values that were passed in checkpoint [ cls . CHECKPOINT_HYPER_PARAMS_KEY ] . update ( kwargs ) model = cls . _load_model_state ( checkpoint , * args , ** kwargs ) return model","title":"load_from_checkpoint"},{"location":"reference/sagemaker_defect_detection/detector/#load_from_metrics","text":"def load_from_metrics ( weights_path , tags_csv , map_location = None ) Warning: Deprecated in version 0.7.0. You should use :meth: load_from_checkpoint instead. Will be removed in v0.9.0. View Source @classmethod def load_from_metrics ( cls , weights_path , tags_csv , map_location = None ) : r \" \"\" Warning: Deprecated in version 0.7.0. You should use :meth:`load_from_checkpoint` instead. Will be removed in v0.9.0. \"\" \" rank_zero_warn ( \"`load_from_metrics` method has been unified with `load_from_checkpoint` in v0.7.0.\" \" The deprecated method will be removed in v0.9.0.\" , DeprecationWarning ) return cls . load_from_checkpoint ( weights_path , tags_csv = tags_csv , map_location = map_location )","title":"load_from_metrics"},{"location":"reference/sagemaker_defect_detection/detector/#instance-variables","text":"device dtype example_input_array hparams on_gpu True if your model is currently running on GPUs. Useful to set flags around the LightningModule for different CPU vs GPU behavior.","title":"Instance variables"},{"location":"reference/sagemaker_defect_detection/detector/#methods","text":"","title":"Methods"},{"location":"reference/sagemaker_defect_detection/detector/#add_module","text":"def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module","title":"add_module"},{"location":"reference/sagemaker_defect_detection/detector/#amp_scale_loss","text":"def amp_scale_loss ( self , unscaled_loss , optimizer , optimizer_idx ) View Source def amp_scale_loss ( self , unscaled_loss , optimizer , optimizer_idx ): if NATIVE_AMP_AVALAIBLE : scaled_loss = self . trainer . scaler . scale ( unscaled_loss ) else : scaled_loss = amp . scale_loss ( unscaled_loss , optimizer ) return scaled_loss","title":"amp_scale_loss"},{"location":"reference/sagemaker_defect_detection/detector/#apply","text":"def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self","title":"apply"},{"location":"reference/sagemaker_defect_detection/detector/#backward","text":"def backward ( self , trainer , loss : torch . Tensor , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) -> None Override backward with your own implementation if you need to. Args: trainer: Pointer to the trainer loss: Loss is already scaled by accumulated grads optimizer: Current optimizer being used optimizer_idx: Index of the current optimizer being used Called to perform backward step. Feel free to override as needed. The loss passed in has already been scaled for accumulated gradients if requested. Example:: def backward(self, trainer, loss, optimizer, optimizer_idx): loss.backward() View Source def backward ( self , trainer , loss : Tensor , optimizer : Optimizer , optimizer_idx : int ) -> None : \"\"\" Override backward with your own implementation if you need to. Args: trainer: Pointer to the trainer loss: Loss is already scaled by accumulated grads optimizer: Current optimizer being used optimizer_idx: Index of the current optimizer being used Called to perform backward step. Feel free to override as needed. The loss passed in has already been scaled for accumulated gradients if requested. Example:: def backward(self, trainer, loss, optimizer, optimizer_idx): loss.backward() \"\"\" loss . backward ()","title":"backward"},{"location":"reference/sagemaker_defect_detection/detector/#bfloat16","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t )","title":"bfloat16"},{"location":"reference/sagemaker_defect_detection/detector/#buffers","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf","title":"buffers"},{"location":"reference/sagemaker_defect_detection/detector/#children","text":"def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module","title":"children"},{"location":"reference/sagemaker_defect_detection/detector/#configure_apex","text":"def configure_apex ( self , amp : object , model : 'LightningModule' , optimizers : List [ torch . optim . optimizer . Optimizer ], amp_level : str ) -> Tuple [ _ForwardRef ( 'LightningModule' ), List [ torch . optim . optimizer . Optimizer ]] Override to init AMP your own way. Must return a model and list of optimizers. Args: amp: pointer to amp library object. model: pointer to current :class: LightningModule . optimizers: list of optimizers passed in :meth: configure_optimizers . amp_level: AMP mode chosen ('O1', 'O2', etc...) Return: Apex wrapped model and optimizers Examples: .. code-block:: python # Default implementation used by Trainer . def configure_apex ( self , amp , model , optimizers , amp_level ): model , optimizers = amp . initialize ( model , optimizers , opt_level = amp_level , ) return model , optimizers View Source def configure_apex ( self , amp : object , model : 'LightningModule' , optimizers : List [ Optimizer ] , amp_level : str ) -> Tuple [ 'LightningModule' , List [ Optimizer ]] : r \" \"\" Override to init AMP your own way. Must return a model and list of optimizers. Args: amp: pointer to amp library object. model: pointer to current :class:`LightningModule`. optimizers: list of optimizers passed in :meth:`configure_optimizers`. amp_level: AMP mode chosen ('O1', 'O2', etc...) Return: Apex wrapped model and optimizers Examples: .. code-block:: python # Default implementation used by Trainer. def configure_apex(self, amp, model, optimizers, amp_level): model, optimizers = amp.initialize( model, optimizers, opt_level=amp_level, ) return model, optimizers \"\" \" model , optimizers = amp . initialize ( model , optimizers , opt_level = amp_level ) return model , optimizers","title":"configure_apex"},{"location":"reference/sagemaker_defect_detection/detector/#configure_ddp","text":"def configure_ddp ( self , model : 'LightningModule' , device_ids : List [ int ] ) -> torch . nn . parallel . distributed . DistributedDataParallel Override to init DDP in your own way or with your own wrapper. The only requirements are that: On a validation batch the call goes to model.validation_step . On a training batch the call goes to model.training_step . On a testing batch, the call goes to model.test_step .+ Args: model: the :class: LightningModule currently being optimized. device_ids: the list of GPU ids. Return: DDP wrapped model Examples: .. code-block:: python # default implementation used in Trainer def configure_ddp(self, model, device_ids): # Lightning DDP simply routes to test_step, val_step, etc... model = LightningDistributedDataParallel( model, device_ids=device_ids, find_unused_parameters=True ) return model View Source def configure_ddp ( self , model : 'LightningModule' , device_ids : List [ int ] ) -> DistributedDataParallel : r \" \"\" Override to init DDP in your own way or with your own wrapper. The only requirements are that: 1. On a validation batch the call goes to ``model.validation_step``. 2. On a training batch the call goes to ``model.training_step``. 3. On a testing batch, the call goes to ``model.test_step``.+ Args: model: the :class:`LightningModule` currently being optimized. device_ids: the list of GPU ids. Return: DDP wrapped model Examples: .. code-block:: python # default implementation used in Trainer def configure_ddp(self, model, device_ids): # Lightning DDP simply routes to test_step, val_step, etc... model = LightningDistributedDataParallel( model, device_ids=device_ids, find_unused_parameters=True ) return model \"\" \" model = LightningDistributedDataParallel ( model , device_ids = device_ids , find_unused_parameters = True ) return model","title":"configure_ddp"},{"location":"reference/sagemaker_defect_detection/detector/#configure_optimizers","text":"def configure_optimizers ( self ) View Source def configure_optimizers ( self ): params = [ p for p in self . parameters () if p . requires_grad ] optimizer = optim . SGD ( params , lr = self . learning_rate , momentum = self . momentum , weight_decay = self . weight_decay ) return optimizer","title":"configure_optimizers"},{"location":"reference/sagemaker_defect_detection/detector/#cpu","text":"def cpu ( self ) -> torch . nn . modules . module . Module Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self ) -> Module : \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" self . _device = torch . device ( 'cpu') return super (). cpu ()","title":"cpu"},{"location":"reference/sagemaker_defect_detection/detector/#cuda","text":"def cuda ( self , device : Union [ int , NoneType ] = None ) -> torch . nn . modules . module . Module Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device: if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self , device : Optional [ int ] = None ) -> Module : \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device: if specified, all parameters will be copied to that device Returns: Module: self \"\"\" self . _device = torch . device ( 'cuda' , index = device ) return super (). cuda ( device = device )","title":"cuda"},{"location":"reference/sagemaker_defect_detection/detector/#double","text":"def double ( self ) -> torch . nn . modules . module . Module Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self ) -> Module : \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" self . _dtype = torch . double return super (). double ()","title":"double"},{"location":"reference/sagemaker_defect_detection/detector/#eval","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False )","title":"eval"},{"location":"reference/sagemaker_defect_detection/detector/#extra_repr","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return ''","title":"extra_repr"},{"location":"reference/sagemaker_defect_detection/detector/#float","text":"def float ( self ) -> torch . nn . modules . module . Module Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self ) -> Module : \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" self . _dtype = torch . float return super (). float ()","title":"float"},{"location":"reference/sagemaker_defect_detection/detector/#forward","text":"def forward ( self , images , * args , ** kwargs ) View Source @ auto_move_data def forward ( self , images , * args , ** kwargs ): if self . train_rpn : # step 2 images = torch . stack ( images ) features = self . mfn ( images ) features = OrderedDict ({ str ( i ): t . unsqueeze ( 0 ) for i , t in enumerate ( features )}) images = ImageList ( images , [( 224 , 224 )]) return self . rpn ( images , features , targets = kwargs . get ( \"targets\" )) elif self . train_roi : # step 3 self . mfn . eval () self . rpn . eval () images = torch . stack ( images ) features = self . mfn ( images ) features = OrderedDict ({ str ( i ): t . unsqueeze ( 0 ) for i , t in enumerate ( features )}) images = ImageList ( images , [( 224 , 224 )]) proposals , _ = self . rpn ( images , features , targets = None ) return self . roi ( features , proposals , [( 224 , 224 )], targets = kwargs . get ( \"targets\" )) elif self . finetune_rpn : self . model . backbone . eval () self . model . roi_heads . eval () return self . model ( images , targets = kwargs . get ( \"targets\" )) elif self . finetune_roi : self . model . backbone . eval () self . model . rpn . eval () return self . model ( images , targets = kwargs . get ( \"targets\" )) else : return self . model ( images , targets = kwargs . get ( \"targets\" ))","title":"forward"},{"location":"reference/sagemaker_defect_detection/detector/#freeze","text":"def freeze ( self ) -> None Freeze all params for inference. Example: .. code-block:: python model = MyLightningModule(...) model.freeze() View Source def freeze ( self ) -> None : r \"\"\" Freeze all params for inference. Example: .. code-block:: python model = MyLightningModule(...) model.freeze() \"\"\" for param in self . parameters (): param . requires_grad = False self . eval ()","title":"freeze"},{"location":"reference/sagemaker_defect_detection/detector/#get_progress_bar_dict","text":"def get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]] Additional items to be displayed in the progress bar. Return: Dictionary with the items to be displayed in the progress bar. View Source def get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]]: r \"\"\" Additional items to be displayed in the progress bar. Return: Dictionary with the items to be displayed in the progress bar. \"\"\" # call . item () only once but store elements without graphs running_train_loss = self . trainer . running_loss . mean () avg_training_loss = running_train_loss . cpu (). item () if running_train_loss is not None else float ( 'NaN' ) tqdm_dict = { 'loss' : '{:.3f}' . format ( avg_training_loss ) } if self . trainer . truncated_bptt_steps is not None : tqdm_dict [ 'split_idx' ] = self . trainer . split_idx if self . trainer . logger is not None and self . trainer . logger . version is not None : tqdm_dict [ 'v_num' ] = self . trainer . logger . version return tqdm_dict","title":"get_progress_bar_dict"},{"location":"reference/sagemaker_defect_detection/detector/#get_tqdm_dict","text":"def get_tqdm_dict ( self ) -> Dict [ str , Union [ int , str ]] Additional items to be displayed in the progress bar. Return: Dictionary with the items to be displayed in the progress bar. Warning: Deprecated since v0.7.3. Use :meth: get_progress_bar_dict instead. View Source def get_tqdm_dict ( self ) -> Dict [ str , Union [ int , str ]] : \" \"\" Additional items to be displayed in the progress bar. Return: Dictionary with the items to be displayed in the progress bar. Warning: Deprecated since v0.7.3. Use :meth:`get_progress_bar_dict` instead. \"\" \" rank_zero_warn ( \"`get_tqdm_dict` was renamed to `get_progress_bar_dict` in v0.7.3\" \" and this method will be removed in v1.0.0\" , DeprecationWarning ) return self . get_progress_bar_dict ()","title":"get_tqdm_dict"},{"location":"reference/sagemaker_defect_detection/detector/#grad_norm","text":"def grad_norm ( self , norm_type : Union [ float , int , str ] ) -> Dict [ str , float ] Compute each parameter's gradient's norm and their overall norm. The overall norm is computed over all gradients together, as if they were concatenated into a single vector. Args: norm_type: The type of the used p-norm, cast to float if necessary. Can be 'inf' for infinity norm. Return: norms: The dictionary of p-norms of each parameter's gradient and a special entry for the total p-norm of the gradients viewed as a single vector. View Source def grad_norm ( self , norm_type : Union [ float , int , str ]) -> Dict [ str , float ]: \"\"\"Compute each parameter's gradient's norm and their overall norm. The overall norm is computed over all gradients together, as if they were concatenated into a single vector. Args: norm_type: The type of the used p-norm, cast to float if necessary. Can be ``'inf'`` for infinity norm. Return: norms: The dictionary of p-norms of each parameter's gradient and a special entry for the total p-norm of the gradients viewed as a single vector. \"\"\" norm_type = float ( norm_type ) norms , all_norms = {} , [] for name , p in self . named_parameters (): if p . grad is None : continue param_norm = float ( p . grad . data . norm ( norm_type )) norms [ f 'grad_{norm_type}_norm_{name}' ] = round ( param_norm , 3 ) all_norms . append ( param_norm ) total_norm = float ( torch . tensor ( all_norms ). norm ( norm_type )) norms [ f 'grad_{norm_type}_norm_total' ] = round ( total_norm , 3 ) return norms","title":"grad_norm"},{"location":"reference/sagemaker_defect_detection/detector/#half","text":"def half ( self ) -> torch . nn . modules . module . Module Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self ) -> Module : \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" self . _dtype = torch . half return super (). half ()","title":"half"},{"location":"reference/sagemaker_defect_detection/detector/#init_ddp_connection","text":"def init_ddp_connection ( self , global_rank : int , world_size : int , is_slurm_managing_tasks : bool = True ) -> None Override to define your custom way of setting up a distributed environment. Lightning's implementation uses env:// init by default and sets the first node as root for SLURM managed cluster. Args: global_rank: The global process idx. world_size: Number of GPUs being use across all nodes. (num_nodes * num_gpus). is_slurm_managing_tasks: is cluster managed by SLURM. View Source def init_ddp_connection ( self , global_rank : int , world_size : int , is_slurm_managing_tasks : bool = True ) -> None : \"\"\" Override to define your custom way of setting up a distributed environment. Lightning's implementation uses env:// init by default and sets the first node as root for SLURM managed cluster. Args: global_rank: The global process idx. world_size: Number of GPUs being use across all nodes. (num_nodes * num_gpus). is_slurm_managing_tasks: is cluster managed by SLURM. \"\"\" if is_slurm_managing_tasks : self . _init_slurm_connection () if 'MASTER_ADDR' not in os . environ : rank_zero_warn ( \"MASTER_ADDR environment variable is not defined. Set as localhost\" ) os . environ [ 'MASTER_ADDR' ] = '127.0.0.1' log . debug ( f \"MASTER_ADDR: {os.environ['MASTER_ADDR']}\" ) if 'MASTER_PORT' not in os . environ : rank_zero_warn ( \"MASTER_PORT environment variable is not defined. Set as 12910\" ) os . environ [ 'MASTER_PORT' ] = '12910' log . debug ( f \"MASTER_PORT: {os.environ['MASTER_PORT']}\" ) if 'WORLD_SIZE' in os . environ and int ( os . environ [ 'WORLD_SIZE' ]) != world_size : rank_zero_warn ( f \"WORLD_SIZE environment variable ({os.environ['WORLD_SIZE']}) \" f \"is not equal to the computed world size ({world_size}). Ignored.\" ) torch_backend = \"nccl\" if self . trainer . on_gpu else \"gloo\" log . info ( f \"initializing ddp: GLOBAL_RANK: {global_rank}, MEMBER: {global_rank+1}/{world_size}\" ) torch_distrib . init_process_group ( torch_backend , rank = global_rank , world_size = world_size )","title":"init_ddp_connection"},{"location":"reference/sagemaker_defect_detection/detector/#load_state_dict","text":"def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys )","title":"load_state_dict"},{"location":"reference/sagemaker_defect_detection/detector/#modules","text":"def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module","title":"modules"},{"location":"reference/sagemaker_defect_detection/detector/#named_buffers","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_buffers"},{"location":"reference/sagemaker_defect_detection/detector/#named_children","text":"def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module","title":"named_children"},{"location":"reference/sagemaker_defect_detection/detector/#named_modules","text":"def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m","title":"named_modules"},{"location":"reference/sagemaker_defect_detection/detector/#named_parameters","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_parameters"},{"location":"reference/sagemaker_defect_detection/detector/#on_after_backward","text":"def on_after_backward ( self ) -> None Called in the training loop after loss.backward() and before optimizers do anything. This is the ideal place to inspect or log gradient information. Example:: def on_after_backward(self): # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge params = self.state_dict() for k, v in params.items(): grads = v name = k self.logger.experiment.add_histogram(tag=name, values=grads, global_step=self.trainer.global_step) View Source def on_after_backward ( self ) -> None : \"\"\" Called in the training loop after loss.backward() and before optimizers do anything. This is the ideal place to inspect or log gradient information. Example:: def on_after_backward(self): # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge params = self.state_dict() for k, v in params.items(): grads = v name = k self.logger.experiment.add_histogram(tag=name, values=grads, global_step=self.trainer.global_step) \"\"\"","title":"on_after_backward"},{"location":"reference/sagemaker_defect_detection/detector/#on_batch_end","text":"def on_batch_end ( self ) -> None Called in the training loop after the batch. View Source def on_batch_end ( self ) -> None : \"\"\" Called in the training loop after the batch. \"\"\"","title":"on_batch_end"},{"location":"reference/sagemaker_defect_detection/detector/#on_batch_start","text":"def on_batch_start ( self , batch : Any ) -> None Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Args: batch: The batched data as it is returned by the training DataLoader. View Source def on_batch_start ( self , batch : Any ) -> None : \"\"\" Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Args: batch: The batched data as it is returned by the training DataLoader. \"\"\"","title":"on_batch_start"},{"location":"reference/sagemaker_defect_detection/detector/#on_before_zero_grad","text":"def on_before_zero_grad ( self , optimizer : torch . optim . optimizer . Optimizer ) -> None Called after optimizer.step() and before optimizer.zero_grad(). Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: optimizer.step() model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad Args: optimizer: The optimizer for which grads should be zeroed. View Source def on_before_zero_grad ( self , optimizer : Optimizer ) -> None : \"\"\" Called after optimizer.step() and before optimizer.zero_grad(). Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: optimizer.step() model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad Args: optimizer: The optimizer for which grads should be zeroed. \"\"\"","title":"on_before_zero_grad"},{"location":"reference/sagemaker_defect_detection/detector/#on_epoch_end","text":"def on_epoch_end ( self ) -> None Called in the training loop at the very end of the epoch. View Source def on_epoch_end ( self ) -> None : \"\"\" Called in the training loop at the very end of the epoch. \"\"\"","title":"on_epoch_end"},{"location":"reference/sagemaker_defect_detection/detector/#on_epoch_start","text":"def on_epoch_start ( self ) -> None Called in the training loop at the very beginning of the epoch. View Source def on_epoch_start ( self ) -> None : \"\"\" Called in the training loop at the very beginning of the epoch. \"\"\"","title":"on_epoch_start"},{"location":"reference/sagemaker_defect_detection/detector/#on_fit_end","text":"def on_fit_end ( self ) Called at the very end of fit. If on DDP it is called on every process View Source def on_fit_end ( self ): \"\"\" Called at the very end of fit. If on DDP it is called on every process \"\"\"","title":"on_fit_end"},{"location":"reference/sagemaker_defect_detection/detector/#on_fit_start","text":"def on_fit_start ( self ) Called at the very beginning of fit. If on DDP it is called on every process View Source def on_fit_start ( self ): \"\"\" Called at the very beginning of fit. If on DDP it is called on every process \"\"\"","title":"on_fit_start"},{"location":"reference/sagemaker_defect_detection/detector/#on_hpc_load","text":"def on_hpc_load ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager loads the model. Args: checkpoint: A dictionary with variables from the checkpoint. View Source def on_hpc_load ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Hook to do whatever you need right before Slurm manager loads the model. Args: checkpoint: A dictionary with variables from the checkpoint. \"\"\"","title":"on_hpc_load"},{"location":"reference/sagemaker_defect_detection/detector/#on_hpc_save","text":"def on_hpc_save ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager saves the model. Args: checkpoint: A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. View Source def on_hpc_save ( self , checkpoint : Dict [ str , Any ]) -> None : \"\"\" Hook to do whatever you need right before Slurm manager saves the model. Args: checkpoint: A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. \"\"\"","title":"on_hpc_save"},{"location":"reference/sagemaker_defect_detection/detector/#on_load_checkpoint","text":"def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example: .. code-block:: python def on_load_checkpoint ( self , checkpoint ): # 99% of the time you don't need to implement this method self . something_cool_i_want_to_save = checkpoint [ 'something_cool_i_want_to_save' ] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. View Source def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None : r \" \"\" Called by Lightning to restore your model. If you saved something with :meth:`on_save_checkpoint` this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example: .. code-block:: python def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. \"\" \"","title":"on_load_checkpoint"},{"location":"reference/sagemaker_defect_detection/detector/#on_post_performance_check","text":"def on_post_performance_check ( self ) -> None Called at the very end of the validation loop. View Source def on_post_performance_check ( self ) -> None : \"\"\" Called at the very end of the validation loop. \"\"\"","title":"on_post_performance_check"},{"location":"reference/sagemaker_defect_detection/detector/#on_pre_performance_check","text":"def on_pre_performance_check ( self ) -> None Called at the very beginning of the validation loop. View Source def on_pre_performance_check ( self ) -> None : \"\"\" Called at the very beginning of the validation loop. \"\"\"","title":"on_pre_performance_check"},{"location":"reference/sagemaker_defect_detection/detector/#on_sanity_check_start","text":"def on_sanity_check_start ( self ) Called before starting evaluation. Warning: Deprecated. Will be removed in v0.9.0. View Source def on_sanity_check_start ( self ): \"\"\" Called before starting evaluation. Warning: Deprecated. Will be removed in v0.9.0. \"\"\"","title":"on_sanity_check_start"},{"location":"reference/sagemaker_defect_detection/detector/#on_save_checkpoint","text":"def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: Checkpoint to be saved Example: .. code-block:: python def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. View Source def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ]) -> None : r \"\"\" Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: Checkpoint to be saved Example: .. code-block:: python def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. \"\"\"","title":"on_save_checkpoint"},{"location":"reference/sagemaker_defect_detection/detector/#on_train_end","text":"def on_train_end ( self ) -> None Called at the end of training before logger experiment is closed. View Source def on_train_end ( self ) -> None : \"\"\" Called at the end of training before logger experiment is closed. \"\"\"","title":"on_train_end"},{"location":"reference/sagemaker_defect_detection/detector/#on_train_start","text":"def on_train_start ( self ) -> None Called at the beginning of training before sanity check. View Source def on_train_start ( self ) -> None : \"\"\" Called at the beginning of training before sanity check. \"\"\"","title":"on_train_start"},{"location":"reference/sagemaker_defect_detection/detector/#optimizer_step","text":"def optimizer_step ( self , epoch : int , batch_idx : int , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int , second_order_closure : Union [ Callable , NoneType ] = None , on_tpu : bool = False , using_native_amp : bool = False , using_lbfgs : bool = False ) -> None Override this method to adjust the default way the :class: ~pytorch_lightning.trainer.trainer.Trainer calls each optimizer. By default, Lightning calls step() and zero_grad() as shown in the example once per optimizer. Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers this indexes into that list. second_order_closure: closure for second order methods on_tpu: true if TPU backward is required using_native_amp: True if using native amp using_lbfgs: True if the matching optimizer is lbfgs Examples: .. code-block:: python # DEFAULT def optimizer_step ( self , current_epoch , batch_idx , optimizer , optimizer_idx , second_order_closure , on_tpu , using_native_amp , using_lbfgs ): optimizer . step () # Alternating schedule for optimizer steps ( i . e .: GANs ) def optimizer_step ( self , current_epoch , batch_idx , optimizer , optimizer_idx , second_order_closure , on_tpu , using_native_amp , using_lbfgs ): # update generator opt every 2 steps if optimizer_idx == 0 : if batch_idx % 2 == 0 : optimizer . step () optimizer . zero_grad () # update discriminator opt every 4 steps if optimizer_idx == 1 : if batch_idx % 4 == 0 : optimizer . step () optimizer . zero_grad () # ... # add as many optimizers as you want Here 's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1., float(self.trainer.global_step + 1) / 500.) for pg in optimizer.param_groups: pg[' lr ' ] = lr_scale * self . learning_rate # update params optimizer . step () optimizer . zero_grad () Note: If you also override the :meth: ~pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad model hook don't forget to add the call to it before optimizer.zero_grad() yourself. View Source def optimizer_step ( self , epoch : int , batch_idx : int , optimizer : Optimizer , optimizer_idx : int , second_order_closure : Optional [ Callable ] = None , on_tpu : bool = False , using_native_amp : bool = False , using_lbfgs : bool = False , ) -> None : r \"\"\" Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls each optimizer. By default, Lightning calls ``step()`` and ``zero_grad()`` as shown in the example once per optimizer. Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers this indexes into that list. second_order_closure: closure for second order methods on_tpu: true if TPU backward is required using_native_amp: True if using native amp using_lbfgs: True if the matching optimizer is lbfgs Examples: .. code-block:: python # DEFAULT def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step() # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every 2 steps if optimizer_idx == 0: if batch_idx % 2 == 0 : optimizer.step() optimizer.zero_grad() # update discriminator opt every 4 steps if optimizer_idx == 1: if batch_idx % 4 == 0 : optimizer.step() optimizer.zero_grad() # ... # add as many optimizers as you want Here's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1., float(self.trainer.global_step + 1) / 500.) for pg in optimizer.param_groups: pg['lr'] = lr_scale * self.learning_rate # update params optimizer.step() optimizer.zero_grad() Note: If you also override the :meth:`~pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad` model hook don't forget to add the call to it before ``optimizer.zero_grad()`` yourself. \"\"\" if on_tpu : xm . optimizer_step ( optimizer ) elif using_native_amp : self . trainer . scaler . step ( optimizer ) elif using_lbfgs : optimizer . step ( second_order_closure ) else : optimizer . step ()","title":"optimizer_step"},{"location":"reference/sagemaker_defect_detection/detector/#optimizer_zero_grad","text":"def optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) View Source def optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : Optimizer , optimizer_idx : int ): optimizer . zero_grad ()","title":"optimizer_zero_grad"},{"location":"reference/sagemaker_defect_detection/detector/#parameters","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param","title":"parameters"},{"location":"reference/sagemaker_defect_detection/detector/#prepare_data","text":"def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data ( self ): # good download_data () tokenize () etc () # bad self . split = data_split self . some_state = some_other_state () In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK = 0 of that node Trainer ( prepare_data_per_node = True ) # call on GLOBAL_RANK = 0 ( great for shared file systems ) Trainer ( prepare_data_per_node = False ) This is called before requesting the dataloaders: .. code-block:: python model . prepare_data () if ddp / tpu : init () model . setup ( stage ) model . train_dataloader () model . val_dataloader () model . test_dataloader () View Source def prepare_data ( self ) -> None : \" \"\" Use this to download and prepare data. .. warning:: DO NOT set state to the model (use `setup` instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): 1. Once per node. This is the default and is only called on LOCAL_RANK=0. 2. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() if ddp/tpu: init() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() \"\" \"","title":"prepare_data"},{"location":"reference/sagemaker_defect_detection/detector/#print","text":"def print ( self , * args , ** kwargs ) -> None Prints only from process 0. Use this in any distributed mode to log only once. Args: args: The thing to print. Will be passed to Python's built-in print function. *kwargs: Will be passed to Python's built-in print function. Example: .. code-block :: python def forward ( self , x ): self . print ( x , 'in forward' ) View Source def print ( self , * args , ** kwargs ) -> None : r \"\"\" Prints only from process 0. Use this in any distributed mode to log only once. Args: *args: The thing to print. Will be passed to Python's built-in print function. **kwargs: Will be passed to Python's built-in print function. Example: .. code-block:: python def forward(self, x): self.print(x, 'in forward') \"\"\" if self . trainer . is_global_zero : print ( * args , ** kwargs )","title":"print"},{"location":"reference/sagemaker_defect_detection/detector/#register_backward_hook","text":"def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle","title":"register_backward_hook"},{"location":"reference/sagemaker_defect_detection/detector/#register_buffer","text":"def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name )","title":"register_buffer"},{"location":"reference/sagemaker_defect_detection/detector/#register_forward_hook","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle","title":"register_forward_hook"},{"location":"reference/sagemaker_defect_detection/detector/#register_forward_pre_hook","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle","title":"register_forward_pre_hook"},{"location":"reference/sagemaker_defect_detection/detector/#register_parameter","text":"def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param","title":"register_parameter"},{"location":"reference/sagemaker_defect_detection/detector/#requires_grad_","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self","title":"requires_grad_"},{"location":"reference/sagemaker_defect_detection/detector/#save_hyperparameters","text":"def save_hyperparameters ( self , * args , frame = None ) -> None Save all model arguments. Args: args: single object of dict , NameSpace or OmegaConf or string names or argumenst from class __init__ from collections import OrderedDict class ManuallyArgsModel(LightningModule): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assine arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... model = ManuallyArgsModel(1, 'abc', 3.14) model.hparams \"arg1\": 1 \"arg3\": 3.14 class AutomaticArgsModel(LightningModule): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, args, *kwargs): ... ... model = AutomaticArgsModel(1, 'abc', 3.14) model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 class SingleArgModel(LightningModule): ... def init (self, params): ... super(). init () ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, args, *kwargs): ... ... model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 View Source def save_hyperparameters ( self , * args , frame = None ) -> None : \"\"\"Save all model arguments. Args: args: single object of `dict`, `NameSpace` or `OmegaConf` or string names or argumenst from class `__init__` >>> from collections import OrderedDict >>> class ManuallyArgsModel(LightningModule): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # manually assine arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(LightningModule): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(LightningModule): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 \"\"\" if not frame : frame = inspect . currentframe () . f_back init_args = get_init_args ( frame ) assert init_args , 'failed to inspect the self init' if not args : hp = init_args self . _hparams_name = 'kwargs' if hp else None else : isx_non_str = [ i for i , arg in enumerate ( args ) if not isinstance ( arg , str )] if len ( isx_non_str ) == 1 : hp = args [ isx_non_str [ 0 ]] cand_names = [ k for k , v in init_args . items () if v == hp ] self . _hparams_name = cand_names [ 0 ] if cand_names else None else : hp = { arg : init_args [ arg ] for arg in args if isinstance ( arg , str )} self . _hparams_name = 'kwargs' # `hparams` are expected here if hp : self . _set_hparams ( hp )","title":"save_hyperparameters"},{"location":"reference/sagemaker_defect_detection/detector/#setup","text":"def setup ( self , stage ) -> None View Source def setup ( self , stage ) -> None : if self . train_rpn : # step 2 self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . pretrained_mfn_ckpt , \"model.mfn\" ) self . rpn = RPN () elif self . train_roi : # step 3 self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . pretrained_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . pretrained_rpn_ckpt , prefix = \"rpn\" ) freeze ( self . rpn ) self . roi = RoI ( self . num_classes ) elif self . finetune_rpn : # step 4 or extra finetune rpn if self . finetuned_rpn_ckpt and self . finetuned_roi_ckpt : # extra finetune rpn self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . finetuned_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . finetuned_rpn_ckpt , prefix = \"rpn\" ) self . roi = load_checkpoint ( RoI ( self . num_classes ), self . finetuned_roi_ckpt , prefix = \"roi\" ) freeze ( self . roi ) self . model = Detection ( self . mfn , self . rpn , self . roi ) else : self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . pretrained_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . pretrained_rpn_ckpt , prefix = \"rpn\" ) self . roi = load_checkpoint ( RoI ( self . num_classes ), self . pretrained_roi_ckpt , prefix = \"roi\" ) freeze ( self . roi ) self . model = Detection ( self . mfn , self . rpn , self . roi ) elif self . finetune_roi : # step 5 or extra finetune roi if self . finetuned_rpn_ckpt and self . finetuned_roi_ckpt : # extra finetune roi self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . finetuned_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . finetuned_rpn_ckpt , prefix = \"rpn\" ) freeze ( self . rpn ) self . roi = load_checkpoint ( RoI ( self . num_classes ), self . finetuned_roi_ckpt , prefix = \"roi\" ) self . model = Detection ( self . mfn , self . rpn , self . roi ) else : self . mfn = load_checkpoint ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , self . finetuned_rpn_ckpt , prefix = \"mfn\" ) freeze ( self . mfn ) self . rpn = load_checkpoint ( RPN (), self . finetuned_rpn_ckpt , prefix = \"rpn\" ) freeze ( self . rpn ) self . roi = load_checkpoint ( RoI ( self . num_classes ), self . pretrained_roi_ckpt , prefix = \"roi\" ) self . model = Detection ( self . mfn , self . rpn , self . roi ) else : # step 6: final/joint model load_checkpoint_fn = load_checkpoint if self . finetuned_roi_ckpt is not None : ckpt_path = self . finetuned_rpn_ckpt elif self . resume_sagemaker_from_checkpoint is not None : ckpt_path = self . resume_sagemaker_from_checkpoint else : ckpt_path = None # ignore load_checkpoint load_checkpoint_fn = lambda * args : args [ 0 ] self . mfn = load_checkpoint_fn ( Classification ( self . backbone , self . num_classes - 1 ) . mfn , ckpt_path , \"mfn\" ) self . rpn = load_checkpoint_fn ( RPN (), ckpt_path , \"rpn\" ) self . roi = load_checkpoint_fn ( RoI ( self . num_classes ), ckpt_path , \"roi\" ) self . model = Detection ( self . mfn , self . rpn , self . roi ) return","title":"setup"},{"location":"reference/sagemaker_defect_detection/detector/#share_memory","text":"def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ())","title":"share_memory"},{"location":"reference/sagemaker_defect_detection/detector/#state_dict","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination","title":"state_dict"},{"location":"reference/sagemaker_defect_detection/detector/#summarize","text":"def summarize ( self , mode : str = 'top' ) -> pytorch_lightning . core . memory . ModelSummary View Source def summarize ( self , mode : str = ModelSummary . MODE_DEFAULT ) -> ModelSummary : model_summary = ModelSummary ( self , mode = mode ) log . info ( '\\n' + str ( model_summary )) return model_summary","title":"summarize"},{"location":"reference/sagemaker_defect_detection/detector/#tbptt_split_batch","text":"def tbptt_split_batch ( self , batch : torch . Tensor , split_size : int ) -> list When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Args: batch: Current batch split_size: The size of the split Return: List of batch splits. Each split will be passed to :meth: training_step to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples: .. code-block:: python def tbptt_split_batch ( self , batch , split_size ) : splits = [] for t in range ( 0 , time_dims [ 0 ] , split_size ) : batch_split = [] for i , x in enumerate ( batch ) : if isinstance ( x , torch . Tensor ) : split_x = x [ :, t:t + split_size ] elif isinstance ( x , collections . Sequence ) : split_x = [ None ] * len ( x ) for batch_idx in range ( len ( x )) : split_x [ batch_idx ] = x [ batch_idx ][ t:t + split_size ] batch_split . append ( split_x ) splits . append ( batch_split ) return splits Note: Called in the training loop after :meth: ~pytorch_lightning.callbacks.base.Callback.on_batch_start if :paramref: ~pytorch_lightning.trainer.Trainer.truncated_bptt_steps > 0. Each returned batch split is passed separately to :meth: training_step . View Source def tbptt_split_batch ( self , batch : Tensor , split_size : int ) -> list : r \"\"\" When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Args: batch: Current batch split_size: The size of the split Return: List of batch splits. Each split will be passed to :meth:`training_step` to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples: .. code-block:: python def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits Note: Called in the training loop after :meth:`~pytorch_lightning.callbacks.base.Callback.on_batch_start` if :paramref:`~pytorch_lightning.trainer.Trainer.truncated_bptt_steps` > 0. Each returned batch split is passed separately to :meth:`training_step`. \"\"\" time_dims = [ len(x[0 ] ) for x in batch if isinstance ( x , ( torch . Tensor , collections . Sequence )) ] assert len ( time_dims ) >= 1 , \"Unable to determine batch time dimension\" assert all ( x == time_dims [ 0 ] for x in time_dims ), \"Batch time dimension length is ambiguous\" splits = [] for t in range ( 0 , time_dims [ 0 ] , split_size ) : batch_split = [] for i , x in enumerate ( batch ) : if isinstance ( x , torch . Tensor ) : split_x = x [ :, t:t + split_size ] elif isinstance ( x , collections . Sequence ) : split_x = [ None ] * len ( x ) for batch_idx in range ( len ( x )) : split_x [ batch_idx ] = x [ batch_idx ][ t:t + split_size ] batch_split . append ( split_x ) splits . append ( batch_split ) return splits","title":"tbptt_split_batch"},{"location":"reference/sagemaker_defect_detection/detector/#teardown","text":"def teardown ( self , stage : str ) Called at the end of fit and test. Args: stage: either 'fit' or 'test' View Source def teardown ( self , stage : str ): \"\"\" Called at the end of fit and test. Args: stage: either 'fit' or 'test' \"\"\"","title":"teardown"},{"location":"reference/sagemaker_defect_detection/detector/#test_dataloader","text":"def test_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , List [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . For data processing use the following pattern: - download in : meth : `prepare_data` - process and split in : meth : `setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Return: Single or multiple PyTorch DataLoaders. Example: .. code-block:: python def test_dataloader ( self ): transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.5 ,), ( 1.0 ,))]) dataset = MNIST ( root = '/path/to/mnist/' , train = False , transform = transform , download = True ) loader = torch . utils . data . DataLoader ( dataset = dataset , batch_size = self . batch_size , shuffle = False ) return loader Note: If you don't need a test dataset and a :meth: test_step , you don't need to implement this method. View Source def test_dataloader ( self ) -> Union [ DataLoader , List [ DataLoader ]] : r \" \"\" Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be called every epoch unless you set :paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch` to ``True``. For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data - :meth:`~pytorch_lightning.trainer.Trainer.fit` - ... - :meth:`prepare_data` - :meth:`setup` - :meth:`train_dataloader` - :meth:`val_dataloader` - :meth:`test_dataloader` Note: Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Return: Single or multiple PyTorch DataLoaders. Example: .. code-block:: python def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader Note: If you don't need a test dataset and a :meth:`test_step`, you don't need to implement this method. \"\" \"","title":"test_dataloader"},{"location":"reference/sagemaker_defect_detection/detector/#test_end","text":"def test_end ( self , outputs ) Warnings: Deprecated in v0.7.0. Use :meth: test_epoch_end instead. Will be removed in 1.0.0. View Source def test_end ( self , outputs ) : \" \"\" Warnings: Deprecated in v0.7.0. Use :meth:`test_epoch_end` instead. Will be removed in 1.0.0. \"\" \"","title":"test_end"},{"location":"reference/sagemaker_defect_detection/detector/#test_epoch_end","text":"def test_epoch_end ( self , outputs : Union [ List [ Dict [ str , torch . Tensor ]], List [ List [ Dict [ str , torch . Tensor ]]]] ) -> Dict [ str , Dict [ str , torch . Tensor ]] Called at the end of a test epoch with the output of all test steps. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Args: outputs: List of outputs you defined in :meth: test_step_end , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader Return: Dict or OrderedDict: Dict has the following optional keys: - progress_bar -> Dict for progress bar display. Must have only tensors. - log -> Dict of metrics to add to logger. Must have only tensors (no images, etc). Note: If you didn't define a :meth: test_step , this won't be called. The outputs here are strictly for logging or progress bar. If you don't need to display anything, don't return anything. If you want to manually set current step, specify it with the 'step' key in the 'log' Dict Examples: With a single dataloader: .. code - block :: python def test_epoch_end ( self , outputs ) : test_acc_mean = 0 for output in outputs : test_acc_mean += output [ 'test_acc' ] test_acc_mean /= len ( outputs ) tqdm_dict = { 'test_acc' : test_acc_mean . item () } # show test_loss and test_acc in progress bar but only log test_loss results = { 'progress_bar' : tqdm_dict , 'log' : { 'test_acc' : test_acc_mean . item () } } return results With multiple dataloaders , `outputs` will be a list of lists . The outer list contains one entry per dataloader , while the inner list contains the individual outputs of each test step for that dataloader . .. code - block :: python def test_epoch_end ( self , outputs ) : test_acc_mean = 0 i = 0 for dataloader_outputs in outputs : for output in dataloader_outputs : test_acc_mean += output [ 'test_acc' ] i += 1 test_acc_mean /= i tqdm_dict = { 'test_acc' : test_acc_mean . item () } # show test_loss and test_acc in progress bar but only log test_loss results = { 'progress_bar' : tqdm_dict , 'log' : { 'test_acc' : test_acc_mean . item (), 'step' : self . current_epoch } } return results View Source def test_epoch_end ( self , outputs : Union [ List [ Dict [ str , Tensor ]] , List [ List [ Dict [ str , Tensor ]]]] ) -> Dict [ str , Dict [ str , Tensor ]] : \" \"\" Called at the end of a test epoch with the output of all test steps. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Args: outputs: List of outputs you defined in :meth:`test_step_end`, or if there are multiple dataloaders, a list containing a list of outputs for each dataloader Return: Dict or OrderedDict: Dict has the following optional keys: - progress_bar -> Dict for progress bar display. Must have only tensors. - log -> Dict of metrics to add to logger. Must have only tensors (no images, etc). Note: If you didn't define a :meth:`test_step`, this won't be called. - The outputs here are strictly for logging or progress bar. - If you don't need to display anything, don't return anything. - If you want to manually set current step, specify it with the 'step' key in the 'log' Dict Examples: With a single dataloader: .. code-block:: python def test_epoch_end(self, outputs): test_acc_mean = 0 for output in outputs: test_acc_mean += output['test_acc'] test_acc_mean /= len(outputs) tqdm_dict = {'test_acc': test_acc_mean.item()} # show test_loss and test_acc in progress bar but only log test_loss results = { 'progress_bar': tqdm_dict, 'log': {'test_acc': test_acc_mean.item()} } return results With multiple dataloaders, `outputs` will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each test step for that dataloader. .. code-block:: python def test_epoch_end(self, outputs): test_acc_mean = 0 i = 0 for dataloader_outputs in outputs: for output in dataloader_outputs: test_acc_mean += output['test_acc'] i += 1 test_acc_mean /= i tqdm_dict = {'test_acc': test_acc_mean.item()} # show test_loss and test_acc in progress bar but only log test_loss results = { 'progress_bar': tqdm_dict, 'log': {'test_acc': test_acc_mean.item(), 'step': self.current_epoch} } return results \"\" \"","title":"test_epoch_end"},{"location":"reference/sagemaker_defect_detection/detector/#test_step","text":"def test_step ( self , * args , ** kwargs ) -> Dict [ str , torch . Tensor ] Operates on a single batch of data from the test set. In this step you'd normally generate examples or calculate anything of interest such as accuracy. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch. dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple test datasets used). Return: Dict or OrderedDict - passed to the :meth: test_epoch_end method. If you defined :meth: test_step_end it will go to that first. .. code-block:: python # if you have one test dataloader: def test_step ( self , batch , batch_idx ) # if you have multiple test dataloaders: def test_step ( self , batch , batch_idx , dataloader_idx ) Examples: .. code-block:: python # CASE 1: A single test dataset def test_step ( self , batch , batch_idx ) : x , y = batch # implement your own out = self ( x ) loss = self . loss ( out , y ) # log 6 example images # or generated text... or whatever sample_imgs = x [ : 6 ] grid = torchvision . utils . make_grid ( sample_imgs ) self . logger . experiment . add_image ( 'example_images' , grid , 0 ) # calculate acc labels_hat = torch . argmax ( out , dim = 1 ) val_acc = torch . sum ( y == labels_hat ). item () / ( len ( y ) * 1.0 ) # all optional... # return whatever you need for the collation function test_epoch_end output = OrderedDict ( { 'val_loss' : loss_val , 'val_acc' : torch . tensor ( val_acc ), # everything must be a tensor } ) # return an optional dict return output If you pass in multiple validation datasets , : meth : `test_step` will have an additional argument . .. code - block :: python # CASE 2: multiple test datasets def test_step ( self , batch , batch_idx , dataset_idx ) : # dataset_idx tells you which dataset this is. Note: If you don't need to validate you don't need to implement this method. Note: When the :meth: test_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of the test epoch, the model goes back to training mode and gradients are enabled. View Source def test_step ( self , * args , ** kwargs ) -> Dict [ str , Tensor ] : r \" \"\" Operates on a single batch of data from the test set. In this step you'd normally generate examples or calculate anything of interest such as accuracy. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Args: batch (:class:`~torch.Tensor` | (:class:`~torch.Tensor`, ...) | [:class:`~torch.Tensor`, ...]): The output of your :class:`~torch.utils.data.DataLoader`. A tensor, tuple or list. batch_idx (int): The index of this batch. dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple test datasets used). Return: Dict or OrderedDict - passed to the :meth:`test_epoch_end` method. If you defined :meth:`test_step_end` it will go to that first. .. code-block:: python # if you have one test dataloader: def test_step(self, batch, batch_idx) # if you have multiple test dataloaders: def test_step(self, batch, batch_idx, dataloader_idx) Examples: .. code-block:: python # CASE 1: A single test dataset def test_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # all optional... # return whatever you need for the collation function test_epoch_end output = OrderedDict({ 'val_loss': loss_val, 'val_acc': torch.tensor(val_acc), # everything must be a tensor }) # return an optional dict return output If you pass in multiple validation datasets, :meth:`test_step` will have an additional argument. .. code-block:: python # CASE 2: multiple test datasets def test_step(self, batch, batch_idx, dataset_idx): # dataset_idx tells you which dataset this is. Note: If you don't need to validate you don't need to implement this method. Note: When the :meth:`test_step` is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of the test epoch, the model goes back to training mode and gradients are enabled. \"\" \"","title":"test_step"},{"location":"reference/sagemaker_defect_detection/detector/#test_step_end","text":"def test_step_end ( self , * args , ** kwargs ) -> Dict [ str , torch . Tensor ] Use this when testing with dp or ddp2 because :meth: test_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: test_step for each batch part. Return: Dict or OrderedDict - passed to the :meth: test_epoch_end . Examples: .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2 , this batch is 1 / num_gpus large def test_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) loss = self . softmax ( out ) loss = nce_loss ( loss ) return { 'loss' : loss } # -------------- # with test_step_end to do softmax over the full batch def test_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) return { 'out' : out } def test_step_end ( self , outputs ): # this out is now the full size of the batch out = outputs [ 'out' ] # this softmax now uses the full batch size loss = nce_loss ( loss ) return { 'loss' : loss } See Also: See the :ref: multi-gpu-training guide for more details. View Source def test_step_end ( self , * args , ** kwargs ) -> Dict [ str , Tensor ] : \" \"\" Use this when testing with dp or ddp2 because :meth:`test_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth:`test_step` for each batch part. Return: Dict or OrderedDict - passed to the :meth:`test_epoch_end`. Examples: .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) loss = nce_loss(loss) return {'loss': loss} # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return {'out': out} def test_step_end(self, outputs): # this out is now the full size of the batch out = outputs['out'] # this softmax now uses the full batch size loss = nce_loss(loss) return {'loss': loss} See Also: See the :ref:`multi-gpu-training` guide for more details. \"\" \"","title":"test_step_end"},{"location":"reference/sagemaker_defect_detection/detector/#tng_dataloader","text":"def tng_dataloader ( self ) Warnings: Deprecated in v0.5.0. Use :meth: train_dataloader instead. Will be removed in 1.0.0. View Source def tng_dataloader ( self ) : # todo: remove in v1.0.0 \" \"\" Warnings: Deprecated in v0.5.0. Use :meth:`train_dataloader` instead. Will be removed in 1.0.0. \"\" \" output = self . train_dataloader () rank_zero_warn ( \"`tng_dataloader` has been renamed to `train_dataloader` since v0.5.0.\" \" and this method will be removed in v1.0.0\" , DeprecationWarning ) return output","title":"tng_dataloader"},{"location":"reference/sagemaker_defect_detection/detector/#to","text":"def to ( self , * args , ** kwargs ) -> torch . nn . modules . module . Module Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note: This method modifies the module in-place. Args: device: the desired device of the parameters and buffers in this module dtype: the desired floating point type of the floating point parameters and buffers in this module tensor: Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module Returns: Module: self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def init (self, weight: torch.Tensor): ... super(). init () ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) -> Module : \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note: This method modifies the module in-place. Args: device: the desired device of the parameters and buffers in this module dtype: the desired floating point type of the floating point parameters and buffers in this module tensor: Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module Returns: Module: self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def __init__(self, weight: torch.Tensor): ... super().__init__() ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) \"\" \" # there is diff nb vars in PT 1.5 out = torch . _C . _nn . _parse_to ( * args , ** kwargs ) device = out [ 0 ] dtype = out [ 1 ] if device is not None : self . _device = device if dtype is not None : self . _dtype = dtype return super (). to ( * args , ** kwargs )","title":"to"},{"location":"reference/sagemaker_defect_detection/detector/#train","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self","title":"train"},{"location":"reference/sagemaker_defect_detection/detector/#train_dataloader","text":"def train_dataloader ( self ) View Source def train_dataloader ( self ): train_loader = DataLoader ( dataset = self . train_dataset , batch_size = self . batch_size , collate_fn = self . train_dataset . collate_fn , shuffle = True , num_workers = cpu_count (), ) return train_loader","title":"train_dataloader"},{"location":"reference/sagemaker_defect_detection/detector/#training_end","text":"def training_end ( self , * args , ** kwargs ) Warnings: Deprecated in v0.7.0. Use :meth: training_step_end instead. View Source def training_end ( self , * args , ** kwargs ) : \" \"\" Warnings: Deprecated in v0.7.0. Use :meth:`training_step_end` instead. \"\" \"","title":"training_end"},{"location":"reference/sagemaker_defect_detection/detector/#training_epoch_end","text":"def training_epoch_end ( self , outputs : Union [ List [ Dict [ str , torch . Tensor ]], List [ List [ Dict [ str , torch . Tensor ]]]] ) -> Dict [ str , Dict [ str , torch . Tensor ]] Called at the end of the training epoch with the outputs of all training steps. .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Args: outputs: List of outputs you defined in :meth: training_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: Dict or OrderedDict. May contain the following optional keys: - log (metrics to be added to the logger; only tensors) - progress_bar (dict for progress bar display) - any metric used in a callback (e.g. early stopping). Note: If this method is not overridden, this won't be called. The outputs here are strictly for logging or progress bar. If you don't need to display anything, don't return anything. If you want to manually set current step, you can specify the 'step' key in the 'log' dict. Examples: With a single dataloader: .. code - block :: python def training_epoch_end ( self , outputs ) : train_acc_mean = 0 for output in outputs : train_acc_mean += output [ 'train_acc' ] train_acc_mean /= len ( outputs ) # log training accuracy at the end of an epoch results = { 'log' : { 'train_acc' : train_acc_mean . item () } , 'progress_bar' : { 'train_acc' : train_acc_mean } , } return results With multiple dataloaders , ``outputs`` will be a list of lists . The outer list contains one entry per dataloader , while the inner list contains the individual outputs of each training step for that dataloader . .. code - block :: python def training_epoch_end ( self , outputs ) : train_acc_mean = 0 i = 0 for dataloader_outputs in outputs : for output in dataloader_outputs : train_acc_mean += output [ 'train_acc' ] i += 1 train_acc_mean /= i # log training accuracy at the end of an epoch results = { 'log' : { 'train_acc' : train_acc_mean . item (), 'step' : self . current_epoch } 'progress_bar' : { 'train_acc' : train_acc_mean } , } return results View Source def training_epoch_end ( self , outputs : Union [ List [ Dict [ str , Tensor ]] , List [ List [ Dict [ str , Tensor ]]]] ) -> Dict [ str , Dict [ str , Tensor ]] : \" \"\" Called at the end of the training epoch with the outputs of all training steps. .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Args: outputs: List of outputs you defined in :meth:`training_step`, or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: Dict or OrderedDict. May contain the following optional keys: - log (metrics to be added to the logger; only tensors) - progress_bar (dict for progress bar display) - any metric used in a callback (e.g. early stopping). Note: If this method is not overridden, this won't be called. - The outputs here are strictly for logging or progress bar. - If you don't need to display anything, don't return anything. - If you want to manually set current step, you can specify the 'step' key in the 'log' dict. Examples: With a single dataloader: .. code-block:: python def training_epoch_end(self, outputs): train_acc_mean = 0 for output in outputs: train_acc_mean += output['train_acc'] train_acc_mean /= len(outputs) # log training accuracy at the end of an epoch results = { 'log': {'train_acc': train_acc_mean.item()}, 'progress_bar': {'train_acc': train_acc_mean}, } return results With multiple dataloaders, ``outputs`` will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each training step for that dataloader. .. code-block:: python def training_epoch_end(self, outputs): train_acc_mean = 0 i = 0 for dataloader_outputs in outputs: for output in dataloader_outputs: train_acc_mean += output['train_acc'] i += 1 train_acc_mean /= i # log training accuracy at the end of an epoch results = { 'log': {'train_acc': train_acc_mean.item(), 'step': self.current_epoch} 'progress_bar': {'train_acc': train_acc_mean}, } return results \"\" \"","title":"training_epoch_end"},{"location":"reference/sagemaker_defect_detection/detector/#training_step","text":"def training_step ( self , batch , batch_idx ) View Source def training_step ( self , batch , batch_idx ) : images , targets , _ = batch if self . train_rpn: targets = [{ \"boxes\" : t [ \"boxes\" ]} for t in targets ] _ , loss_dict = self ( images , targets = targets ) loss = sum ( loss for loss in loss_dict . values ()) return OrderedDict ({ \"loss\" : loss , \"progress_bar\" : loss_dict , \"log\" : loss_dict }) elif self . train_roi: _ , loss_dict = self ( images , targets = targets ) loss = sum ( loss for loss in loss_dict . values ()) return OrderedDict ({ \"loss\" : loss , \"progress_bar\" : loss_dict , \"log\" : loss_dict }) else : images = list ( image for image in images ) targets = [{ k: v for k , v in t . items ()} for t in targets ] loss_dict = self ( images , targets = targets ) # loss keys: [' loss_classifier ', ' loss_box_reg ', ' loss_objectness ', ' loss_rpn_box_reg '] loss = sum ( loss for loss in loss_dict . values ()) if not math . isfinite ( loss . item ()) : sys . exit ( 1 ) return OrderedDict ({ \"loss\" : loss , \"progress_bar\" : loss_dict , \"log\" : loss_dict })","title":"training_step"},{"location":"reference/sagemaker_defect_detection/detector/#training_step_end","text":"def training_step_end ( self , * args , ** kwargs ) -> Dict [ str , Union [ torch . Tensor , Dict [ str , torch . Tensor ]]] Use this when training with dp or ddp2 because :meth: training_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in training_step for each batch part. Return: Dict with loss key and optional log or progress bar keys. - loss -> tensor scalar **REQUIRED** - progress_bar -> Dict for progress bar display. Must have only tensors - log -> Dict of metrics to add to logger. Must have only tensors (no images, etc) Examples: .. code-block:: python # WITHOUT training_step_end # if used in DP or DDP2 , this batch is 1 / num_gpus large def training_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) loss = self . softmax ( out ) loss = nce_loss ( loss ) return { 'loss' : loss } # -------------- # with training_step_end to do softmax over the full batch def training_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) return { 'out' : out } def training_step_end ( self , outputs ): # this out is now the full size of the batch out = outputs [ 'out' ] # this softmax now uses the full batch size loss = nce_loss ( loss ) return { 'loss' : loss } See Also: See the :ref: multi-gpu-training guide for more details. View Source def training_step_end ( self , * args , ** kwargs ) -> Dict [ str , Union [ Tensor , Dict [ str , Tensor ]] ] : \" \"\" Use this when training with dp or ddp2 because :meth:`training_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in `training_step` for each batch part. Return: Dict with loss key and optional log or progress bar keys. - loss -> tensor scalar **REQUIRED** - progress_bar -> Dict for progress bar display. Must have only tensors - log -> Dict of metrics to add to logger. Must have only tensors (no images, etc) Examples: .. code-block:: python # WITHOUT training_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) loss = nce_loss(loss) return {'loss': loss} # -------------- # with training_step_end to do softmax over the full batch def training_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return {'out': out} def training_step_end(self, outputs): # this out is now the full size of the batch out = outputs['out'] # this softmax now uses the full batch size loss = nce_loss(loss) return {'loss': loss} See Also: See the :ref:`multi-gpu-training` guide for more details. \"\" \"","title":"training_step_end"},{"location":"reference/sagemaker_defect_detection/detector/#transfer_batch_to_device","text":"def transfer_batch_to_device ( self , batch : Any , device : torch . device ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Example:: def transfer_batch_to_device(self, batch, device) if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) else: batch = super().transfer_batch_to_device(data, device) return batch Args: batch: A batch of data that needs to be transferred to a new device. device: The target device as defined in PyTorch. Returns: A reference to the data on the new device. Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). The :class: ~pytorch_lightning.trainer.trainer.Trainer already takes care of splitting the batch and determines the target devices. See Also: - :func: ~pytorch_lightning.utilities.apply_func.move_data_to_device - :func: ~pytorch_lightning.utilities.apply_func.apply_to_collection View Source def transfer_batch_to_device ( self , batch : Any , device : torch . device ) -> Any : \" \"\" Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: - :class:`torch.Tensor` or anything that implements `.to(...)` - :class:`list` - :class:`dict` - :class:`tuple` - :class:`torchtext.data.batch.Batch` For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Example:: def transfer_batch_to_device(self, batch, device) if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) else: batch = super().transfer_batch_to_device(data, device) return batch Args: batch: A batch of data that needs to be transferred to a new device. device: The target device as defined in PyTorch. Returns: A reference to the data on the new device. Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). The :class:`~pytorch_lightning.trainer.trainer.Trainer` already takes care of splitting the batch and determines the target devices. See Also: - :func:`~pytorch_lightning.utilities.apply_func.move_data_to_device` - :func:`~pytorch_lightning.utilities.apply_func.apply_to_collection` \"\" \" return move_data_to_device ( batch , device )","title":"transfer_batch_to_device"},{"location":"reference/sagemaker_defect_detection/detector/#type","text":"def type ( self , dst_type : Union [ str , torch . dtype ] ) -> torch . nn . modules . module . Module Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self , dst_type : Union [ str , torch . dtype ] ) -> Module : \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" self . _dtype = dst_type return super (). type ( dst_type = dst_type )","title":"type"},{"location":"reference/sagemaker_defect_detection/detector/#unfreeze","text":"def unfreeze ( self ) -> None Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() View Source def unfreeze ( self ) -> None : \"\"\" Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() \"\"\" for param in self . parameters (): param . requires_grad = True self . train ()","title":"unfreeze"},{"location":"reference/sagemaker_defect_detection/detector/#val_dataloader","text":"def val_dataloader ( self ) View Source def val_dataloader ( self ): val_loader = DataLoader ( self . val_dataset , batch_size = self . batch_size , collate_fn = self . val_dataset . collate_fn , shuffle = False , num_workers = cpu_count () // 2 , ) self . coco_evaluator = self . _get_evaluator ( val_loader . dataset ) return val_loader","title":"val_dataloader"},{"location":"reference/sagemaker_defect_detection/detector/#validation_end","text":"def validation_end ( self , outputs ) Warnings: Deprecated in v0.7.0. Use :meth: validation_epoch_end instead. Will be removed in 1.0.0. View Source def validation_end ( self , outputs ) : \" \"\" Warnings: Deprecated in v0.7.0. Use :meth:`validation_epoch_end` instead. Will be removed in 1.0.0. \"\" \"","title":"validation_end"},{"location":"reference/sagemaker_defect_detection/detector/#validation_epoch_end","text":"def validation_epoch_end ( self , outputs ) View Source @auto_move_data def validation_epoch_end ( self , outputs ) : if self . train_rpn : return {} elif self . train_roi : # TODO : above return {} else : self . coco_evaluator . synchronize_between_processes () self . coco_evaluator . accumulate () self . coco_evaluator . summarize () metric = self . coco_evaluator . coco_eval [ \"bbox\" ] . stats [ 0 ] metric = torch . as_tensor ( metric ) tensorboard_logs = { \"main_score\" : metric } self . coco_evaluator = self . _get_evaluator ( self . val_dataset ) # need to update for the new evaluation return { \"val_loss\" : metric , \"log\" : tensorboard_logs , \"progress_bar\" : tensorboard_logs }","title":"validation_epoch_end"},{"location":"reference/sagemaker_defect_detection/detector/#validation_step","text":"def validation_step ( self , batch , batch_idx ) View Source @auto_move_data def validation_step ( self , batch , batch_idx ) : images , targets , _ = batch if self . train_rpn : # rpn doesn ' t compute loss for val return {} elif self . train_roi : # TODO : scores are predictions scores , not a metric ! iou ? + acc ? return {} else : images = list ( image for image in images ) targets = [ {k: v for k, v in t.items()} for t in targets ] outputs = self ( images , targets = targets ) ret = { target [ \"image_id\" ] . item () : output for target , output in zip ( targets , outputs ) } self . coco_evaluator . update ( ret ) return {}","title":"validation_step"},{"location":"reference/sagemaker_defect_detection/detector/#validation_step_end","text":"def validation_step_end ( self , * args , ** kwargs ) -> Dict [ str , torch . Tensor ] Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: validation_step for each batch part. Return: Dict or OrderedDict - passed to the :meth: validation_epoch_end method. Examples: .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2 , this batch is 1 / num_gpus large def validation_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) loss = self . softmax ( out ) loss = nce_loss ( loss ) return { 'loss' : loss } # -------------- # with validation_step_end to do softmax over the full batch def validation_step ( self , batch , batch_idx ): # batch is 1 / num_gpus big x , y = batch out = self ( x ) return { 'out' : out } def validation_epoch_end ( self , outputs ): # this out is now the full size of the batch out = outputs [ 'out' ] # this softmax now uses the full batch size loss = nce_loss ( loss ) return { 'loss' : loss } See Also: See the :ref: multi-gpu-training guide for more details. View Source def validation_step_end ( self , * args , ** kwargs ) -> Dict [ str , Tensor ] : \" \"\" Use this when validating with dp or ddp2 because :meth:`validation_step` will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth:`validation_step` for each batch part. Return: Dict or OrderedDict - passed to the :meth:`validation_epoch_end` method. Examples: .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) loss = nce_loss(loss) return {'loss': loss} # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return {'out': out} def validation_epoch_end(self, outputs): # this out is now the full size of the batch out = outputs['out'] # this softmax now uses the full batch size loss = nce_loss(loss) return {'loss': loss} See Also: See the :ref:`multi-gpu-training` guide for more details. \"\" \"","title":"validation_step_end"},{"location":"reference/sagemaker_defect_detection/detector/#zero_grad","text":"def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ ()","title":"zero_grad"},{"location":"reference/sagemaker_defect_detection/transforms/","text":"Module sagemaker_defect_detection.transforms View Source from typing import Callable import torchvision.transforms as transforms import albumentations as albu import albumentations.pytorch.transforms as albu_transforms PROBABILITY = 0.5 ROTATION_ANGLE = 90 NUM_CHANNELS = 3 # required for resnet IMAGE_RESIZE_HEIGHT = 256 IMAGE_RESIZE_WIDTH = 256 IMAGE_HEIGHT = 224 IMAGE_WIDTH = 224 # standard imagenet1k mean and standard deviation of RGB channels MEAN_RED = 0.485 MEAN_GREEN = 0.456 MEAN_BLUE = 0.406 STD_RED = 0.229 STD_GREEN = 0.224 STD_BLUE = 0.225 def get_transform ( split : str ) -> Callable : \"\"\" Image data transformations such as normalization for train split for classification task Parameters ---------- split : str train or else Returns ------- Callable Image transformation function \"\"\" normalize = transforms . Normalize ( mean = [ MEAN_RED , MEAN_GREEN , MEAN_BLUE ], std = [ STD_RED , STD_GREEN , STD_BLUE ]) if split == \"train\" : return transforms . Compose ( [ transforms . RandomResizedCrop ( IMAGE_HEIGHT ), transforms . RandomRotation ( ROTATION_ANGLE ), transforms . RandomHorizontalFlip (), transforms . ToTensor (), normalize , ] ) else : return transforms . Compose ( [ transforms . Resize ( IMAGE_RESIZE_HEIGHT ), transforms . CenterCrop ( IMAGE_HEIGHT ), transforms . ToTensor (), normalize , ] ) def get_augmentation ( split : str ) -> Callable : \"\"\" Obtains proper image augmentation in train split for detection task. We have splitted transformations done for detection task into augmentation and preprocessing for clarity Parameters ---------- split : str train or else Returns ------- Callable Image augmentation function \"\"\" if split == \"train\" : return albu . Compose ( [ albu . Resize ( IMAGE_RESIZE_HEIGHT , IMAGE_RESIZE_WIDTH , always_apply = True ), albu . RandomCrop ( IMAGE_HEIGHT , IMAGE_WIDTH , always_apply = True ), albu . RandomRotate90 ( p = PROBABILITY ), albu . HorizontalFlip ( p = PROBABILITY ), albu . RandomBrightness ( p = PROBABILITY ), ], bbox_params = albu . BboxParams ( format = \"pascal_voc\" , label_fields = [ \"labels\" ], min_visibility = 0.2 , ), ) else : return albu . Compose ( [ albu . Resize ( IMAGE_HEIGHT , IMAGE_WIDTH )], bbox_params = albu . BboxParams ( format = \"pascal_voc\" , label_fields = [ \"labels\" ]), ) def get_preprocess () -> Callable : \"\"\" Image normalization using albumentation for detection task that aligns well with image augmentation Returns ------- Callable Image normalization function \"\"\" return albu . Compose ( [ albu . Normalize ( mean = [ MEAN_RED , MEAN_GREEN , MEAN_BLUE ], std = [ STD_RED , STD_GREEN , STD_BLUE ]), albu_transforms . ToTensorV2 (), ] ) Variables IMAGE_HEIGHT IMAGE_RESIZE_HEIGHT IMAGE_RESIZE_WIDTH IMAGE_WIDTH MEAN_BLUE MEAN_GREEN MEAN_RED NUM_CHANNELS PROBABILITY ROTATION_ANGLE STD_BLUE STD_GREEN STD_RED Functions get_augmentation def get_augmentation ( split : str ) -> Callable Obtains proper image augmentation in train split for detection task. We have splitted transformations done for detection task into augmentation and preprocessing for clarity Parameters split : str train or else Returns Callable Image augmentation function View Source def get_augmentation ( split : str ) -> Callable : \"\"\" Obtains proper image augmentation in train split for detection task. We have splitted transformations done for detection task into augmentation and preprocessing for clarity Parameters ---------- split : str train or else Returns ------- Callable Image augmentation function \"\"\" if split == \"train\" : return albu . Compose ( [ albu . Resize ( IMAGE_RESIZE_HEIGHT , IMAGE_RESIZE_WIDTH , always_apply = True ), albu . RandomCrop ( IMAGE_HEIGHT , IMAGE_WIDTH , always_apply = True ), albu . RandomRotate90 ( p = PROBABILITY ), albu . HorizontalFlip ( p = PROBABILITY ), albu . RandomBrightness ( p = PROBABILITY ), ], bbox_params = albu . BboxParams ( format = \"pascal_voc\" , label_fields = [ \"labels\" ], min_visibility = 0 . 2 , ), ) else : return albu . Compose ( [ albu . Resize ( IMAGE_HEIGHT , IMAGE_WIDTH )], bbox_params = albu . BboxParams ( format = \"pascal_voc\" , label_fields = [ \"labels\" ]), ) get_preprocess def get_preprocess ( ) -> Callable Image normalization using albumentation for detection task that aligns well with image augmentation Returns Callable Image normalization function View Source def get_preprocess () -> Callable : \"\"\" Image normalization using albumentation for detection task that aligns well with image augmentation Returns ------- Callable Image normalization function \"\"\" return albu . Compose ( [ albu . Normalize ( mean = [ MEAN_RED , MEAN_GREEN , MEAN_BLUE ], std = [ STD_RED , STD_GREEN , STD_BLUE ]), albu_transforms . ToTensorV2 (), ] ) get_transform def get_transform ( split : str ) -> Callable Image data transformations such as normalization for train split for classification task Parameters split : str train or else Returns Callable Image transformation function View Source def get_transform ( split : str ) -> Callable : \"\"\" Image data transformations such as normalization for train split for classification task Parameters ---------- split : str train or else Returns ------- Callable Image transformation function \"\"\" normalize = transforms . Normalize ( mean = [ MEAN_RED , MEAN_GREEN , MEAN_BLUE ], std = [ STD_RED , STD_GREEN , STD_BLUE ]) if split == \"train\" : return transforms . Compose ( [ transforms . RandomResizedCrop ( IMAGE_HEIGHT ), transforms . RandomRotation ( ROTATION_ANGLE ), transforms . RandomHorizontalFlip (), transforms . ToTensor (), normalize , ] ) else : return transforms . Compose ( [ transforms . Resize ( IMAGE_RESIZE_HEIGHT ), transforms . CenterCrop ( IMAGE_HEIGHT ), transforms . ToTensor (), normalize , ] )","title":"Transforms"},{"location":"reference/sagemaker_defect_detection/transforms/#module-sagemaker_defect_detectiontransforms","text":"View Source from typing import Callable import torchvision.transforms as transforms import albumentations as albu import albumentations.pytorch.transforms as albu_transforms PROBABILITY = 0.5 ROTATION_ANGLE = 90 NUM_CHANNELS = 3 # required for resnet IMAGE_RESIZE_HEIGHT = 256 IMAGE_RESIZE_WIDTH = 256 IMAGE_HEIGHT = 224 IMAGE_WIDTH = 224 # standard imagenet1k mean and standard deviation of RGB channels MEAN_RED = 0.485 MEAN_GREEN = 0.456 MEAN_BLUE = 0.406 STD_RED = 0.229 STD_GREEN = 0.224 STD_BLUE = 0.225 def get_transform ( split : str ) -> Callable : \"\"\" Image data transformations such as normalization for train split for classification task Parameters ---------- split : str train or else Returns ------- Callable Image transformation function \"\"\" normalize = transforms . Normalize ( mean = [ MEAN_RED , MEAN_GREEN , MEAN_BLUE ], std = [ STD_RED , STD_GREEN , STD_BLUE ]) if split == \"train\" : return transforms . Compose ( [ transforms . RandomResizedCrop ( IMAGE_HEIGHT ), transforms . RandomRotation ( ROTATION_ANGLE ), transforms . RandomHorizontalFlip (), transforms . ToTensor (), normalize , ] ) else : return transforms . Compose ( [ transforms . Resize ( IMAGE_RESIZE_HEIGHT ), transforms . CenterCrop ( IMAGE_HEIGHT ), transforms . ToTensor (), normalize , ] ) def get_augmentation ( split : str ) -> Callable : \"\"\" Obtains proper image augmentation in train split for detection task. We have splitted transformations done for detection task into augmentation and preprocessing for clarity Parameters ---------- split : str train or else Returns ------- Callable Image augmentation function \"\"\" if split == \"train\" : return albu . Compose ( [ albu . Resize ( IMAGE_RESIZE_HEIGHT , IMAGE_RESIZE_WIDTH , always_apply = True ), albu . RandomCrop ( IMAGE_HEIGHT , IMAGE_WIDTH , always_apply = True ), albu . RandomRotate90 ( p = PROBABILITY ), albu . HorizontalFlip ( p = PROBABILITY ), albu . RandomBrightness ( p = PROBABILITY ), ], bbox_params = albu . BboxParams ( format = \"pascal_voc\" , label_fields = [ \"labels\" ], min_visibility = 0.2 , ), ) else : return albu . Compose ( [ albu . Resize ( IMAGE_HEIGHT , IMAGE_WIDTH )], bbox_params = albu . BboxParams ( format = \"pascal_voc\" , label_fields = [ \"labels\" ]), ) def get_preprocess () -> Callable : \"\"\" Image normalization using albumentation for detection task that aligns well with image augmentation Returns ------- Callable Image normalization function \"\"\" return albu . Compose ( [ albu . Normalize ( mean = [ MEAN_RED , MEAN_GREEN , MEAN_BLUE ], std = [ STD_RED , STD_GREEN , STD_BLUE ]), albu_transforms . ToTensorV2 (), ] )","title":"Module sagemaker_defect_detection.transforms"},{"location":"reference/sagemaker_defect_detection/transforms/#variables","text":"IMAGE_HEIGHT IMAGE_RESIZE_HEIGHT IMAGE_RESIZE_WIDTH IMAGE_WIDTH MEAN_BLUE MEAN_GREEN MEAN_RED NUM_CHANNELS PROBABILITY ROTATION_ANGLE STD_BLUE STD_GREEN STD_RED","title":"Variables"},{"location":"reference/sagemaker_defect_detection/transforms/#functions","text":"","title":"Functions"},{"location":"reference/sagemaker_defect_detection/transforms/#get_augmentation","text":"def get_augmentation ( split : str ) -> Callable Obtains proper image augmentation in train split for detection task. We have splitted transformations done for detection task into augmentation and preprocessing for clarity","title":"get_augmentation"},{"location":"reference/sagemaker_defect_detection/transforms/#parameters","text":"split : str train or else","title":"Parameters"},{"location":"reference/sagemaker_defect_detection/transforms/#returns","text":"Callable Image augmentation function View Source def get_augmentation ( split : str ) -> Callable : \"\"\" Obtains proper image augmentation in train split for detection task. We have splitted transformations done for detection task into augmentation and preprocessing for clarity Parameters ---------- split : str train or else Returns ------- Callable Image augmentation function \"\"\" if split == \"train\" : return albu . Compose ( [ albu . Resize ( IMAGE_RESIZE_HEIGHT , IMAGE_RESIZE_WIDTH , always_apply = True ), albu . RandomCrop ( IMAGE_HEIGHT , IMAGE_WIDTH , always_apply = True ), albu . RandomRotate90 ( p = PROBABILITY ), albu . HorizontalFlip ( p = PROBABILITY ), albu . RandomBrightness ( p = PROBABILITY ), ], bbox_params = albu . BboxParams ( format = \"pascal_voc\" , label_fields = [ \"labels\" ], min_visibility = 0 . 2 , ), ) else : return albu . Compose ( [ albu . Resize ( IMAGE_HEIGHT , IMAGE_WIDTH )], bbox_params = albu . BboxParams ( format = \"pascal_voc\" , label_fields = [ \"labels\" ]), )","title":"Returns"},{"location":"reference/sagemaker_defect_detection/transforms/#get_preprocess","text":"def get_preprocess ( ) -> Callable Image normalization using albumentation for detection task that aligns well with image augmentation","title":"get_preprocess"},{"location":"reference/sagemaker_defect_detection/transforms/#returns_1","text":"Callable Image normalization function View Source def get_preprocess () -> Callable : \"\"\" Image normalization using albumentation for detection task that aligns well with image augmentation Returns ------- Callable Image normalization function \"\"\" return albu . Compose ( [ albu . Normalize ( mean = [ MEAN_RED , MEAN_GREEN , MEAN_BLUE ], std = [ STD_RED , STD_GREEN , STD_BLUE ]), albu_transforms . ToTensorV2 (), ] )","title":"Returns"},{"location":"reference/sagemaker_defect_detection/transforms/#get_transform","text":"def get_transform ( split : str ) -> Callable Image data transformations such as normalization for train split for classification task","title":"get_transform"},{"location":"reference/sagemaker_defect_detection/transforms/#parameters_1","text":"split : str train or else","title":"Parameters"},{"location":"reference/sagemaker_defect_detection/transforms/#returns_2","text":"Callable Image transformation function View Source def get_transform ( split : str ) -> Callable : \"\"\" Image data transformations such as normalization for train split for classification task Parameters ---------- split : str train or else Returns ------- Callable Image transformation function \"\"\" normalize = transforms . Normalize ( mean = [ MEAN_RED , MEAN_GREEN , MEAN_BLUE ], std = [ STD_RED , STD_GREEN , STD_BLUE ]) if split == \"train\" : return transforms . Compose ( [ transforms . RandomResizedCrop ( IMAGE_HEIGHT ), transforms . RandomRotation ( ROTATION_ANGLE ), transforms . RandomHorizontalFlip (), transforms . ToTensor (), normalize , ] ) else : return transforms . Compose ( [ transforms . Resize ( IMAGE_RESIZE_HEIGHT ), transforms . CenterCrop ( IMAGE_HEIGHT ), transforms . ToTensor (), normalize , ] )","title":"Returns"},{"location":"reference/sagemaker_defect_detection/dataset/","text":"Module sagemaker_defect_detection.dataset Sub-modules sagemaker_defect_detection.dataset.neu","title":"Index"},{"location":"reference/sagemaker_defect_detection/dataset/#module-sagemaker_defect_detectiondataset","text":"","title":"Module sagemaker_defect_detection.dataset"},{"location":"reference/sagemaker_defect_detection/dataset/#sub-modules","text":"sagemaker_defect_detection.dataset.neu","title":"Sub-modules"},{"location":"reference/sagemaker_defect_detection/dataset/neu/","text":"Module sagemaker_defect_detection.dataset.neu View Source from typing import List , Tuple , Optional , Callable import os from pathlib import Path from collections import namedtuple from xml.etree.ElementTree import ElementTree import numpy as np import cv2 import torch from torch.utils.data.dataset import Dataset from torchvision.datasets import ImageFolder class NEUCLS ( ImageFolder ): \"\"\" NEU-CLS dataset processing and loading \"\"\" def __init__ ( self , root : str , split : str , augmentation : Optional [ Callable ] = None , preprocessing : Optional [ Callable ] = None , seed : int = 123 , ** kwargs , ) -> None : \"\"\" NEU-CLS dataset Parameters ---------- root : str Dataset root path split : str Data split from train, val and test augmentation : Optional[Callable], optional Image augmentation function, by default None preprocess : Optional[Callable], optional Image preprocessing function, by default None seed : int, optional Random number generator seed, by default 123 Raises ------ ValueError If unsupported split is used \"\"\" super () . __init__ ( root , ** kwargs ) self . samples : List [ Tuple [ str , int ]] self . split = split self . augmentation = augmentation self . preprocessing = preprocessing n_items = len ( self . samples ) np . random . seed ( seed ) perm = np . random . permutation ( list ( range ( n_items ))) # TODO: add split ratios as parameters train_end = int ( 0.6 * n_items ) val_end = int ( 0.2 * n_items ) + train_end if split == \"train\" : self . samples = [ self . samples [ i ] for i in perm [: train_end ]] elif split == \"val\" : self . samples = [ self . samples [ i ] for i in perm [ train_end : val_end ]] elif split == \"test\" : self . samples = [ self . samples [ i ] for i in perm [ val_end :]] else : raise ValueError ( f \"Unknown split mode. Choose from `train`, `val` or `test`. Given { split } \" ) DetectionSample = namedtuple ( \"DetectionSample\" , [ \"image_path\" , \"class_idx\" , \"annotations\" ]) class NEUDET ( Dataset ): \"\"\" NEU-DET dataset processing and loading \"\"\" def __init__ ( self , root : str , split : str , augmentation : Optional [ Callable ] = None , preprocess : Optional [ Callable ] = None , seed : int = 123 , ): \"\"\" NEU-DET dataset Parameters ---------- root : str Dataset root path split : str Data split from train, val and test augmentation : Optional[Callable], optional Image augmentation function, by default None preprocess : Optional[Callable], optional Image preprocessing function, by default None seed : int, optional Random number generator seed, by default 123 Raises ------ ValueError If unsupported split is used \"\"\" super () . __init__ () self . root = Path ( root ) self . split = split self . classes , self . class_to_idx = self . _find_classes () self . samples : List [ DetectionSample ] = self . _make_dataset () self . augmentation = augmentation self . preprocess = preprocess n_items = len ( self . samples ) np . random . seed ( seed ) perm = np . random . permutation ( list ( range ( n_items ))) train_end = int ( 0.6 * n_items ) val_end = int ( 0.2 * n_items ) + train_end if split == \"train\" : self . samples = [ self . samples [ i ] for i in perm [: train_end ]] elif split == \"val\" : self . samples = [ self . samples [ i ] for i in perm [ train_end : val_end ]] elif split == \"test\" : self . samples = [ self . samples [ i ] for i in perm [ val_end :]] else : raise ValueError ( f \"Unknown split mode. Choose from `train`, `val` or `test`. Given { split } \" ) def _make_dataset ( self ) -> List [ DetectionSample ]: instances = [] base_dir = self . root . expanduser () for target_cls in sorted ( self . class_to_idx . keys ()): cls_idx = self . class_to_idx [ target_cls ] target_dir = base_dir / target_cls if not target_dir . is_dir (): continue images = sorted ( list (( target_dir / \"images\" ) . glob ( \"*.jpg\" ))) annotations = sorted ( list (( target_dir / \"annotations\" ) . glob ( \"*.xml\" ))) assert len ( images ) == len ( annotations ), f \"something is wrong. Mismatched number of images and annotations\" for path , ann in zip ( images , annotations ): instances . append ( DetectionSample ( str ( path ), int ( cls_idx ), str ( ann ))) return instances def _find_classes ( self ): classes = sorted ([ d . name for d in os . scandir ( str ( self . root )) if d . is_dir ()]) class_to_idx = { cls_name : i for i , cls_name in enumerate ( classes , 1 )} # no bg label in NEU return classes , class_to_idx @staticmethod def _get_bboxes ( ann : str ) -> List [ List [ int ]]: tree = ElementTree () . parse ( ann ) bboxes = [] for bndbox in tree . iterfind ( \"object/bndbox\" ): # should subtract 1 like coco? bbox = [ int ( bndbox . findtext ( t )) - 1 for t in ( \"xmin\" , \"ymin\" , \"xmax\" , \"ymax\" )] # type: ignore assert bbox [ 2 ] > bbox [ 0 ] and bbox [ 3 ] > bbox [ 1 ], f \"box size error, given { bbox } \" bboxes . append ( bbox ) return bboxes def __len__ ( self ): return len ( self . samples ) def __getitem__ ( self , idx : int ): # Note: images are grayscaled BUT resnet needs 3 channels image = cv2 . imread ( self . samples [ idx ] . image_path ) # BGR channel last image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) boxes = self . _get_bboxes ( self . samples [ idx ] . annotations ) num_objs = len ( boxes ) boxes = torch . as_tensor ( boxes , dtype = torch . float32 ) labels = torch . tensor ([ self . samples [ idx ] . class_idx ] * num_objs , dtype = torch . int64 ) image_id = torch . tensor ([ idx ], dtype = torch . int64 ) iscrowd = torch . zeros (( len ( boxes ),), dtype = torch . int64 ) target = {} target [ \"boxes\" ] = boxes target [ \"labels\" ] = labels target [ \"image_id\" ] = image_id target [ \"iscrowd\" ] = iscrowd if self . augmentation is not None : sample = self . augmentation ( ** { \"image\" : image , \"bboxes\" : boxes , \"labels\" : labels }) image = sample [ \"image\" ] target [ \"boxes\" ] = torch . as_tensor ( sample [ \"bboxes\" ], dtype = torch . float32 ) # guards against crops that don't pass the min_visibility augmentation threshold if not target [ \"boxes\" ] . numel (): return None target [ \"labels\" ] = torch . as_tensor ( sample [ \"labels\" ], dtype = torch . int64 ) if self . preprocess is not None : image = self . preprocess ( image = image )[ \"image\" ] boxes = target [ \"boxes\" ] target [ \"area\" ] = ( boxes [:, 3 ] - boxes [:, 1 ]) * ( boxes [:, 2 ] - boxes [:, 0 ]) return image , target , image_id def collate_fn ( self , batch ): batch = filter ( lambda x : x is not None , batch ) return tuple ( zip ( * batch )) Classes DetectionSample class DetectionSample ( / , * args , ** kwargs ) DetectionSample(image_path, class_idx, annotations) Ancestors (in MRO) builtins.tuple Instance variables annotations Alias for field number 2 class_idx Alias for field number 1 image_path Alias for field number 0 Methods count def count ( ... ) T.count(value) -> integer -- return number of occurrences of value index def index ( ... ) T.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. NEUCLS class NEUCLS ( root : str , split : str , augmentation : Union [ Callable , NoneType ] = None , preprocessing : Union [ Callable , NoneType ] = None , seed : int = 123 , ** kwargs ) NEU-CLS dataset processing and loading Ancestors (in MRO) torchvision.datasets.folder.ImageFolder torchvision.datasets.folder.DatasetFolder torchvision.datasets.vision.VisionDataset torch.utils.data.dataset.Dataset Methods extra_repr def extra_repr ( self ) View Source def extra_repr ( self ): return \"\" NEUDET class NEUDET ( root : str , split : str , augmentation : Union [ Callable , NoneType ] = None , preprocess : Union [ Callable , NoneType ] = None , seed : int = 123 ) NEU-DET dataset processing and loading Ancestors (in MRO) torch.utils.data.dataset.Dataset Methods collate_fn def collate_fn ( self , batch ) View Source def collate_fn ( self , batch ): batch = filter ( lambda x : x is not None , batch ) return tuple ( zip ( * batch ))","title":"Neu"},{"location":"reference/sagemaker_defect_detection/dataset/neu/#module-sagemaker_defect_detectiondatasetneu","text":"View Source from typing import List , Tuple , Optional , Callable import os from pathlib import Path from collections import namedtuple from xml.etree.ElementTree import ElementTree import numpy as np import cv2 import torch from torch.utils.data.dataset import Dataset from torchvision.datasets import ImageFolder class NEUCLS ( ImageFolder ): \"\"\" NEU-CLS dataset processing and loading \"\"\" def __init__ ( self , root : str , split : str , augmentation : Optional [ Callable ] = None , preprocessing : Optional [ Callable ] = None , seed : int = 123 , ** kwargs , ) -> None : \"\"\" NEU-CLS dataset Parameters ---------- root : str Dataset root path split : str Data split from train, val and test augmentation : Optional[Callable], optional Image augmentation function, by default None preprocess : Optional[Callable], optional Image preprocessing function, by default None seed : int, optional Random number generator seed, by default 123 Raises ------ ValueError If unsupported split is used \"\"\" super () . __init__ ( root , ** kwargs ) self . samples : List [ Tuple [ str , int ]] self . split = split self . augmentation = augmentation self . preprocessing = preprocessing n_items = len ( self . samples ) np . random . seed ( seed ) perm = np . random . permutation ( list ( range ( n_items ))) # TODO: add split ratios as parameters train_end = int ( 0.6 * n_items ) val_end = int ( 0.2 * n_items ) + train_end if split == \"train\" : self . samples = [ self . samples [ i ] for i in perm [: train_end ]] elif split == \"val\" : self . samples = [ self . samples [ i ] for i in perm [ train_end : val_end ]] elif split == \"test\" : self . samples = [ self . samples [ i ] for i in perm [ val_end :]] else : raise ValueError ( f \"Unknown split mode. Choose from `train`, `val` or `test`. Given { split } \" ) DetectionSample = namedtuple ( \"DetectionSample\" , [ \"image_path\" , \"class_idx\" , \"annotations\" ]) class NEUDET ( Dataset ): \"\"\" NEU-DET dataset processing and loading \"\"\" def __init__ ( self , root : str , split : str , augmentation : Optional [ Callable ] = None , preprocess : Optional [ Callable ] = None , seed : int = 123 , ): \"\"\" NEU-DET dataset Parameters ---------- root : str Dataset root path split : str Data split from train, val and test augmentation : Optional[Callable], optional Image augmentation function, by default None preprocess : Optional[Callable], optional Image preprocessing function, by default None seed : int, optional Random number generator seed, by default 123 Raises ------ ValueError If unsupported split is used \"\"\" super () . __init__ () self . root = Path ( root ) self . split = split self . classes , self . class_to_idx = self . _find_classes () self . samples : List [ DetectionSample ] = self . _make_dataset () self . augmentation = augmentation self . preprocess = preprocess n_items = len ( self . samples ) np . random . seed ( seed ) perm = np . random . permutation ( list ( range ( n_items ))) train_end = int ( 0.6 * n_items ) val_end = int ( 0.2 * n_items ) + train_end if split == \"train\" : self . samples = [ self . samples [ i ] for i in perm [: train_end ]] elif split == \"val\" : self . samples = [ self . samples [ i ] for i in perm [ train_end : val_end ]] elif split == \"test\" : self . samples = [ self . samples [ i ] for i in perm [ val_end :]] else : raise ValueError ( f \"Unknown split mode. Choose from `train`, `val` or `test`. Given { split } \" ) def _make_dataset ( self ) -> List [ DetectionSample ]: instances = [] base_dir = self . root . expanduser () for target_cls in sorted ( self . class_to_idx . keys ()): cls_idx = self . class_to_idx [ target_cls ] target_dir = base_dir / target_cls if not target_dir . is_dir (): continue images = sorted ( list (( target_dir / \"images\" ) . glob ( \"*.jpg\" ))) annotations = sorted ( list (( target_dir / \"annotations\" ) . glob ( \"*.xml\" ))) assert len ( images ) == len ( annotations ), f \"something is wrong. Mismatched number of images and annotations\" for path , ann in zip ( images , annotations ): instances . append ( DetectionSample ( str ( path ), int ( cls_idx ), str ( ann ))) return instances def _find_classes ( self ): classes = sorted ([ d . name for d in os . scandir ( str ( self . root )) if d . is_dir ()]) class_to_idx = { cls_name : i for i , cls_name in enumerate ( classes , 1 )} # no bg label in NEU return classes , class_to_idx @staticmethod def _get_bboxes ( ann : str ) -> List [ List [ int ]]: tree = ElementTree () . parse ( ann ) bboxes = [] for bndbox in tree . iterfind ( \"object/bndbox\" ): # should subtract 1 like coco? bbox = [ int ( bndbox . findtext ( t )) - 1 for t in ( \"xmin\" , \"ymin\" , \"xmax\" , \"ymax\" )] # type: ignore assert bbox [ 2 ] > bbox [ 0 ] and bbox [ 3 ] > bbox [ 1 ], f \"box size error, given { bbox } \" bboxes . append ( bbox ) return bboxes def __len__ ( self ): return len ( self . samples ) def __getitem__ ( self , idx : int ): # Note: images are grayscaled BUT resnet needs 3 channels image = cv2 . imread ( self . samples [ idx ] . image_path ) # BGR channel last image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) boxes = self . _get_bboxes ( self . samples [ idx ] . annotations ) num_objs = len ( boxes ) boxes = torch . as_tensor ( boxes , dtype = torch . float32 ) labels = torch . tensor ([ self . samples [ idx ] . class_idx ] * num_objs , dtype = torch . int64 ) image_id = torch . tensor ([ idx ], dtype = torch . int64 ) iscrowd = torch . zeros (( len ( boxes ),), dtype = torch . int64 ) target = {} target [ \"boxes\" ] = boxes target [ \"labels\" ] = labels target [ \"image_id\" ] = image_id target [ \"iscrowd\" ] = iscrowd if self . augmentation is not None : sample = self . augmentation ( ** { \"image\" : image , \"bboxes\" : boxes , \"labels\" : labels }) image = sample [ \"image\" ] target [ \"boxes\" ] = torch . as_tensor ( sample [ \"bboxes\" ], dtype = torch . float32 ) # guards against crops that don't pass the min_visibility augmentation threshold if not target [ \"boxes\" ] . numel (): return None target [ \"labels\" ] = torch . as_tensor ( sample [ \"labels\" ], dtype = torch . int64 ) if self . preprocess is not None : image = self . preprocess ( image = image )[ \"image\" ] boxes = target [ \"boxes\" ] target [ \"area\" ] = ( boxes [:, 3 ] - boxes [:, 1 ]) * ( boxes [:, 2 ] - boxes [:, 0 ]) return image , target , image_id def collate_fn ( self , batch ): batch = filter ( lambda x : x is not None , batch ) return tuple ( zip ( * batch ))","title":"Module sagemaker_defect_detection.dataset.neu"},{"location":"reference/sagemaker_defect_detection/dataset/neu/#classes","text":"","title":"Classes"},{"location":"reference/sagemaker_defect_detection/dataset/neu/#detectionsample","text":"class DetectionSample ( / , * args , ** kwargs ) DetectionSample(image_path, class_idx, annotations)","title":"DetectionSample"},{"location":"reference/sagemaker_defect_detection/dataset/neu/#ancestors-in-mro","text":"builtins.tuple","title":"Ancestors (in MRO)"},{"location":"reference/sagemaker_defect_detection/dataset/neu/#instance-variables","text":"annotations Alias for field number 2 class_idx Alias for field number 1 image_path Alias for field number 0","title":"Instance variables"},{"location":"reference/sagemaker_defect_detection/dataset/neu/#methods","text":"","title":"Methods"},{"location":"reference/sagemaker_defect_detection/dataset/neu/#count","text":"def count ( ... ) T.count(value) -> integer -- return number of occurrences of value","title":"count"},{"location":"reference/sagemaker_defect_detection/dataset/neu/#index","text":"def index ( ... ) T.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/sagemaker_defect_detection/dataset/neu/#neucls","text":"class NEUCLS ( root : str , split : str , augmentation : Union [ Callable , NoneType ] = None , preprocessing : Union [ Callable , NoneType ] = None , seed : int = 123 , ** kwargs ) NEU-CLS dataset processing and loading","title":"NEUCLS"},{"location":"reference/sagemaker_defect_detection/dataset/neu/#ancestors-in-mro_1","text":"torchvision.datasets.folder.ImageFolder torchvision.datasets.folder.DatasetFolder torchvision.datasets.vision.VisionDataset torch.utils.data.dataset.Dataset","title":"Ancestors (in MRO)"},{"location":"reference/sagemaker_defect_detection/dataset/neu/#methods_1","text":"","title":"Methods"},{"location":"reference/sagemaker_defect_detection/dataset/neu/#extra_repr","text":"def extra_repr ( self ) View Source def extra_repr ( self ): return \"\"","title":"extra_repr"},{"location":"reference/sagemaker_defect_detection/dataset/neu/#neudet","text":"class NEUDET ( root : str , split : str , augmentation : Union [ Callable , NoneType ] = None , preprocess : Union [ Callable , NoneType ] = None , seed : int = 123 ) NEU-DET dataset processing and loading","title":"NEUDET"},{"location":"reference/sagemaker_defect_detection/dataset/neu/#ancestors-in-mro_2","text":"torch.utils.data.dataset.Dataset","title":"Ancestors (in MRO)"},{"location":"reference/sagemaker_defect_detection/dataset/neu/#methods_2","text":"","title":"Methods"},{"location":"reference/sagemaker_defect_detection/dataset/neu/#collate_fn","text":"def collate_fn ( self , batch ) View Source def collate_fn ( self , batch ): batch = filter ( lambda x : x is not None , batch ) return tuple ( zip ( * batch ))","title":"collate_fn"},{"location":"reference/sagemaker_defect_detection/models/","text":"Module sagemaker_defect_detection.models Sub-modules sagemaker_defect_detection.models.ddn","title":"Index"},{"location":"reference/sagemaker_defect_detection/models/#module-sagemaker_defect_detectionmodels","text":"","title":"Module sagemaker_defect_detection.models"},{"location":"reference/sagemaker_defect_detection/models/#sub-modules","text":"sagemaker_defect_detection.models.ddn","title":"Sub-modules"},{"location":"reference/sagemaker_defect_detection/models/ddn/","text":"Module sagemaker_defect_detection.models.ddn View Source import torch import torch.nn as nn import torch.nn.functional as F import torchvision from torchvision.models.detection.faster_rcnn import FastRCNNPredictor from torchvision.models.detection.transform import GeneralizedRCNNTransform from torchvision.models.detection.generalized_rcnn import GeneralizedRCNN from torchvision.models.detection.roi_heads import RoIHeads from torchvision.models.detection.rpn import AnchorGenerator , RPNHead , RegionProposalNetwork from torchvision.ops import MultiScaleRoIAlign def get_backbone ( name : str ) -> nn . Module : \"\"\" Get official pretrained ResNet34 and ResNet50 as backbones Parameters ---------- name : str Either `resnet34` or `resnet50` Returns ------- nn.Module resnet34 or resnet50 pytorch modules Raises ------ ValueError If unsupported name is used \"\"\" if name == \"resnet34\" : return torchvision . models . resnet34 ( pretrained = True ) elif name == \"resnet50\" : return torchvision . models . resnet50 ( pretrained = True ) else : raise ValueError ( \"Unsupported backbone\" ) def init_weights ( m ) -> None : \"\"\" Weight initialization Parameters ---------- m : [type] Module used in recursive call \"\"\" if isinstance ( m , nn . Conv2d ): nn . init . xavier_normal_ ( m . weight ) elif isinstance ( m , nn . Linear ): m . weight . data . normal_ ( 0.0 , 0.02 ) m . bias . data . fill_ ( 0.0 ) return class MFN ( nn . Module ): def __init__ ( self , backbone : str ): \"\"\" Implementation of MFN model as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. Parameters ---------- backbone : str Either `resnet34` or `resnet50` \"\"\" super () . __init__ () self . backbone = get_backbone ( backbone ) # input 224x224 -> conv1 output size 112x112 self . start_layer = nn . Sequential ( self . backbone . conv1 , # type: ignore self . backbone . bn1 , # type: ignore self . backbone . relu , # type: ignore self . backbone . maxpool , # type: ignore ) self . r2 = self . backbone . layer1 # 64/256x56x56 <- (resnet34/resnet50) self . r3 = self . backbone . layer2 # 128/512x28x28 self . r4 = self . backbone . layer3 # 256/1024x14x14 self . r5 = self . backbone . layer4 # 512/2048x7x7 in_channel = 64 if backbone == \"resnet34\" else 256 self . b2 = nn . Sequential ( nn . Conv2d ( in_channel , in_channel , kernel_size = 3 , padding = 1 , stride = 2 ), # 56 -> 28 without Relu or batchnorm not in the paper ??? nn . BatchNorm2d ( in_channel ), nn . ReLU ( inplace = True ), nn . Conv2d ( in_channel , in_channel , kernel_size = 3 , padding = 1 , stride = 2 ), # 28 -> 14 nn . BatchNorm2d ( in_channel ), nn . ReLU ( inplace = True ), nn . Conv2d ( in_channel , in_channel * 2 , kernel_size = 1 , padding = 0 ), nn . BatchNorm2d ( in_channel * 2 ), nn . ReLU ( inplace = True ), ) . apply ( init_weights ) # after r2: 128/512x14x14 <- self . b3 = nn . MaxPool2d ( 2 ) # after r3: 128/512x14x14 <- in_channel *= 2 # 128/512 self . b4 = nn . Sequential ( nn . Conv2d ( in_channel * 2 , in_channel , kernel_size = 1 , padding = 0 ), nn . BatchNorm2d ( in_channel ), nn . ReLU ( inplace = True ), ) . apply ( init_weights ) # after r4: 128/512x14x14 in_channel *= 4 # 512 / 2048 self . b5 = nn . Sequential ( nn . ConvTranspose2d ( in_channel , in_channel , kernel_size = 3 , stride = 2 , padding = 1 , output_padding = 1 ), # <- after r5 which is 512x7x7 -> 512x14x14 nn . BatchNorm2d ( in_channel ), nn . ReLU ( inplace = True ), nn . Conv2d ( in_channel , in_channel // 4 , kernel_size = 1 , padding = 0 ), nn . BatchNorm2d ( in_channel // 4 ), nn . ReLU ( inplace = True ), ) . apply ( init_weights ) self . out_channels = 512 if backbone == \"resnet34\" else 2048 # required for FasterRCNN def forward ( self , x ): x = self . start_layer ( x ) x = self . r2 ( x ) b2_out = self . b2 ( x ) x = self . r3 ( x ) b3_out = self . b3 ( x ) x = self . r4 ( x ) b4_out = self . b4 ( x ) x = self . r5 ( x ) b5_out = self . b5 ( x ) # BatchNorm works better than L2 normalize # out = torch.cat([F.normalize(o, p=2, dim=1) for o in (b2_out, b3_out, b4_out, b5_out)], dim=1) out = torch . cat (( b2_out , b3_out , b4_out , b5_out ), dim = 1 ) return out class Classification ( nn . Module ): \"\"\" Classification network Parameters ---------- backbone : str Either `resnet34` or `resnet50` num_classes : int Number of classes \"\"\" def __init__ ( self , backbone : str , num_classes : int ) -> None : super () . __init__ () self . mfn = MFN ( backbone ) self . flatten = nn . Flatten () self . fc = nn . Linear ( self . mfn . out_channels * 14 ** 2 , num_classes ) def forward ( self , x ): return self . fc ( self . flatten ( self . mfn ( x ))) class RPN ( nn . Module ): \"\"\" RPN Module as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. \"\"\" def __init__ ( self , out_channels : int = 512 , rpn_pre_nms_top_n_train : int = 1000 , # torchvision default 2000, rpn_pre_nms_top_n_test : int = 500 , # torchvision default 1000, rpn_post_nms_top_n_train : int = 1000 , # torchvision default 2000, rpn_post_nms_top_n_test : int = 500 , # torchvision default 1000, rpn_nms_thresh : float = 0.7 , rpn_fg_iou_thresh : float = 0.7 , rpn_bg_iou_thresh : float = 0.3 , rpn_batch_size_per_image : int = 256 , rpn_positive_fraction : float = 0.5 , ) -> None : super () . __init__ () rpn_anchor_generator = AnchorGenerator ( sizes = (( 64 , 128 , 256 , 512 ),), aspect_ratios = (( 0.5 , 1.0 , 2.0 ),)) rpn_head = RPNHead ( out_channels , rpn_anchor_generator . num_anchors_per_location ()[ 0 ]) rpn_pre_nms_top_n = dict ( training = rpn_pre_nms_top_n_train , testing = rpn_pre_nms_top_n_test ) rpn_post_nms_top_n = dict ( training = rpn_post_nms_top_n_train , testing = rpn_post_nms_top_n_test ) self . rpn = RegionProposalNetwork ( rpn_anchor_generator , rpn_head , rpn_fg_iou_thresh , rpn_bg_iou_thresh , rpn_batch_size_per_image , rpn_positive_fraction , rpn_pre_nms_top_n , rpn_post_nms_top_n , rpn_nms_thresh , ) def forward ( self , * args , ** kwargs ): return self . rpn ( * args , ** kwargs ) class CustomTwoMLPHead ( nn . Module ): def __init__ ( self , in_channels : int , representation_size : int ): super () . __init__ () self . avgpool = nn . AdaptiveAvgPool2d ( 7 ) self . mlp = nn . Sequential ( nn . Linear ( in_channels , representation_size ), nn . ReLU ( inplace = True ), nn . Linear ( representation_size , representation_size ), nn . ReLU ( inplace = True ), ) def forward ( self , x ): x = self . avgpool ( x ) x = x . flatten ( start_dim = 1 ) x = self . mlp ( x ) return x class RoI ( nn . Module ): \"\"\" ROI Module as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. \"\"\" def __init__ ( self , num_classes : int , box_fg_iou_thresh = 0.5 , box_bg_iou_thresh = 0.5 , box_batch_size_per_image = 512 , box_positive_fraction = 0.25 , bbox_reg_weights = None , box_score_thresh = 0.05 , box_nms_thresh = 0.5 , box_detections_per_img = 100 , ) -> None : super () . __init__ () roi_pooler = MultiScaleRoIAlign ( featmap_names = [ \"0\" ], output_size = 7 , sampling_ratio = 2 ) box_head = CustomTwoMLPHead ( 512 * 7 ** 2 , 1024 ) box_predictor = FastRCNNPredictor ( 1024 , num_classes = num_classes ) self . roi_head = RoIHeads ( roi_pooler , box_head , box_predictor , box_fg_iou_thresh , box_bg_iou_thresh , box_batch_size_per_image , box_positive_fraction , bbox_reg_weights , box_score_thresh , box_nms_thresh , box_detections_per_img , ) def forward ( self , * args , ** kwargs ): return self . roi_head ( * args , ** kwargs ) class Detection ( GeneralizedRCNN ): \"\"\" Detection network as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. \"\"\" def __init__ ( self , mfn , rpn , roi ): dummy_transform = GeneralizedRCNNTransform ( 800 , 1333 , [ 00.0 , 0.0 , 0.0 ], [ 1.0 , 1.0 , 1.0 ]) super () . __init__ ( mfn , rpn , roi , dummy_transform ) Functions get_backbone def get_backbone ( name : str ) -> torch . nn . modules . module . Module Get official pretrained ResNet34 and ResNet50 as backbones Parameters name : str Either resnet34 or resnet50 Returns nn.Module resnet34 or resnet50 pytorch modules Raises ValueError If unsupported name is used View Source def get_backbone ( name : str ) -> nn . Module : \" \"\" Get official pretrained ResNet34 and ResNet50 as backbones Parameters ---------- name : str Either `resnet34` or `resnet50` Returns ------- nn.Module resnet34 or resnet50 pytorch modules Raises ------ ValueError If unsupported name is used \"\" \" if name == \"resnet34\" : return torchvision . models . resnet34 ( pretrained = True ) elif name == \"resnet50\" : return torchvision . models . resnet50 ( pretrained = True ) else : raise ValueError ( \"Unsupported backbone\" ) init_weights def init_weights ( m ) -> None Weight initialization Parameters m : [type] Module used in recursive call View Source def init_weights ( m ) -> None : \"\"\" Weight initialization Parameters ---------- m : [type] Module used in recursive call \"\"\" if isinstance ( m , nn . Conv2d ) : nn . init . xavier_normal_ ( m . weight ) elif isinstance ( m , nn . Linear ) : m . weight . data . normal_ ( 0.0 , 0.02 ) m . bias . data . fill_ ( 0.0 ) return Classes Classification class Classification ( backbone : str , num_classes : int ) Classification network Parameters backbone : str Either resnet34 or resnet50 num_classes : int Number of classes Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module apply def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf children def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ()) cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device )) double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t ) eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False ) extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t ) forward def forward ( self , x ) View Source def forward ( self , x ): return self . fc ( self . flatten ( self . mfn ( x ))) half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t ) load_state_dict def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys ) modules def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param register_backward_hook def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_parameter def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self share_memory def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ()) state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert ) train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type )) zero_grad def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ () CustomTwoMLPHead class CustomTwoMLPHead ( in_channels : int , representation_size : int ) Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model ( nn . Module ): def __init__ ( self ): super ( Model , self ) . __init__ () self . conv1 = nn . Conv2d ( 1 , 20 , 5 ) self . conv2 = nn . Conv2d ( 20 , 20 , 5 ) def forward ( self , x ): x = F . relu ( self . conv1 ( x )) return F . relu ( self . conv2 ( x )) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module apply def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf children def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ()) cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device )) double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t ) eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False ) extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t ) forward def forward ( self , x ) View Source def forward ( self , x ): x = self . avgpool ( x ) x = x . flatten ( start_dim = 1 ) x = self . mlp ( x ) return x half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t ) load_state_dict def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys ) modules def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param register_backward_hook def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_parameter def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self share_memory def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ()) state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert ) train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type )) zero_grad def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ () Detection class Detection ( mfn , rpn , roi ) Detection network as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. Ancestors (in MRO) torchvision.models.detection.generalized_rcnn.GeneralizedRCNN torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module apply def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf children def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ()) cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device )) double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t ) eager_outputs def eager_outputs ( self , losses , detections ) View Source @torch . jit . unused def eager_outputs ( self , losses , detections ) : # type : ( Dict [ str, Tensor ] , List [ Dict[str, Tensor ] ] ) -> Tuple [ Dict[str, Tensor ] , List [ Dict[str, Tensor ] ]] if self . training : return losses return detections eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False ) extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t ) forward def forward ( self , images , targets = None ) Arguments: images (list[Tensor]): images to be processed targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional) Returns: result (list[BoxList] or dict[Tensor]): the output from the model. During training, it returns a dict[Tensor] which contains the losses. During testing, it returns list[BoxList] contains additional fields like scores , labels and mask (for Mask R-CNN models). View Source def forward ( self , images , targets = None ) : # type : ( List [ Tensor ] , Optional [ List[Dict[str, Tensor ] ]] ) -> Tuple [ Dict[str, Tensor ] , List [ Dict[str, Tensor ] ]] \"\"\" Arguments: images (list[Tensor]): images to be processed targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional) Returns: result (list[BoxList] or dict[Tensor]): the output from the model. During training, it returns a dict[Tensor] which contains the losses. During testing, it returns list[BoxList] contains additional fields like `scores`, `labels` and `mask` (for Mask R-CNN models). \"\"\" if self . training and targets is None : raise ValueError ( \"In training mode, targets should be passed\" ) if self . training : assert targets is not None for target in targets : boxes = target [ \"boxes\" ] if isinstance ( boxes , torch . Tensor ) : if len ( boxes . shape ) != 2 or boxes . shape [ -1 ] != 4 : raise ValueError ( \"Expected target boxes to be a tensor\" \"of shape [N, 4], got {:}.\" . format ( boxes . shape )) else : raise ValueError ( \"Expected target boxes to be of type \" \"Tensor, got {:}.\" . format ( type ( boxes ))) original_image_sizes = torch . jit . annotate ( List [ Tuple[int, int ] ] , [] ) for img in images : val = img . shape [ -2: ] assert len ( val ) == 2 original_image_sizes . append (( val [ 0 ] , val [ 1 ] )) images , targets = self . transform ( images , targets ) # Check for degenerate boxes # TODO : Move this to a function if targets is not None : for target_idx , target in enumerate ( targets ) : boxes = target [ \"boxes\" ] degenerate_boxes = boxes [ :, 2: ] <= boxes [ :, :2 ] if degenerate_boxes . any () : # print the first degenrate box bb_idx = degenerate_boxes . any ( dim = 1 ). nonzero (). view ( - 1 ) [ 0 ] degen_bb : List [ float ] = boxes [ bb_idx ] . tolist () raise ValueError ( \"All bounding boxes should have positive height and width.\" \" Found invaid box {} for target at index {}.\" . format ( degen_bb , target_idx )) features = self . backbone ( images . tensors ) if isinstance ( features , torch . Tensor ) : features = OrderedDict ( [ ('0', features) ] ) proposals , proposal_losses = self . rpn ( images , features , targets ) detections , detector_losses = self . roi_heads ( features , proposals , images . image_sizes , targets ) detections = self . transform . postprocess ( detections , images . image_sizes , original_image_sizes ) losses = {} losses . update ( detector_losses ) losses . update ( proposal_losses ) if torch . jit . is_scripting () : if not self . _has_warned : warnings . warn ( \"RCNN always returns a (Losses, Detections) tuple in scripting\" ) self . _has_warned = True return ( losses , detections ) else : return self . eager_outputs ( losses , detections ) half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t ) load_state_dict def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys ) modules def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param register_backward_hook def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_parameter def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self share_memory def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ()) state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert ) train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type )) zero_grad def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ () MFN class MFN ( backbone : str ) Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model ( nn . Module ): def __init__ ( self ): super ( Model , self ) . __init__ () self . conv1 = nn . Conv2d ( 1 , 20 , 5 ) self . conv2 = nn . Conv2d ( 20 , 20 , 5 ) def forward ( self , x ): x = F . relu ( self . conv1 ( x )) return F . relu ( self . conv2 ( x )) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module apply def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf children def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ()) cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device )) double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t ) eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False ) extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t ) forward def forward ( self , x ) View Source def forward ( self , x ): x = self . start_layer ( x ) x = self . r2 ( x ) b2_out = self . b2 ( x ) x = self . r3 ( x ) b3_out = self . b3 ( x ) x = self . r4 ( x ) b4_out = self . b4 ( x ) x = self . r5 ( x ) b5_out = self . b5 ( x ) # BatchNorm works better than L2 normalize # out = torch . cat ([ F . normalize ( o , p = 2 , dim = 1 ) for o in ( b2_out , b3_out , b4_out , b5_out )], dim = 1 ) out = torch . cat (( b2_out , b3_out , b4_out , b5_out ), dim = 1 ) return out half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t ) load_state_dict def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys ) modules def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param register_backward_hook def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_parameter def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self share_memory def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ()) state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert ) train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type )) zero_grad def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ () RPN class RPN ( out_channels : int = 512 , rpn_pre_nms_top_n_train : int = 1000 , rpn_pre_nms_top_n_test : int = 500 , rpn_post_nms_top_n_train : int = 1000 , rpn_post_nms_top_n_test : int = 500 , rpn_nms_thresh : float = 0.7 , rpn_fg_iou_thresh : float = 0.7 , rpn_bg_iou_thresh : float = 0.3 , rpn_batch_size_per_image : int = 256 , rpn_positive_fraction : float = 0.5 ) RPN Module as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module apply def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf children def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ()) cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device )) double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t ) eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False ) extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t ) forward def forward ( self , * args , ** kwargs ) View Source def forward ( self , * args , ** kwargs ): return self . rpn ( * args , ** kwargs ) half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t ) load_state_dict def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys ) modules def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param register_backward_hook def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_parameter def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self share_memory def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ()) state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert ) train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type )) zero_grad def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ () RoI class RoI ( num_classes : int , box_fg_iou_thresh = 0.5 , box_bg_iou_thresh = 0.5 , box_batch_size_per_image = 512 , box_positive_fraction = 0.25 , bbox_reg_weights = None , box_score_thresh = 0.05 , box_nms_thresh = 0.5 , box_detections_per_img = 100 ) ROI Module as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module apply def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t ) buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf children def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ()) cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device )) double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t ) eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False ) extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return '' float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t ) forward def forward ( self , * args , ** kwargs ) View Source def forward ( self , * args , ** kwargs ): return self . roi_head ( * args , ** kwargs ) half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t ) load_state_dict def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys ) modules def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem named_children def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module named_modules def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param register_backward_hook def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle register_buffer def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name ) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle register_parameter def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self share_memory def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ()) state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert ) train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type )) zero_grad def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ ()","title":"Ddn"},{"location":"reference/sagemaker_defect_detection/models/ddn/#module-sagemaker_defect_detectionmodelsddn","text":"View Source import torch import torch.nn as nn import torch.nn.functional as F import torchvision from torchvision.models.detection.faster_rcnn import FastRCNNPredictor from torchvision.models.detection.transform import GeneralizedRCNNTransform from torchvision.models.detection.generalized_rcnn import GeneralizedRCNN from torchvision.models.detection.roi_heads import RoIHeads from torchvision.models.detection.rpn import AnchorGenerator , RPNHead , RegionProposalNetwork from torchvision.ops import MultiScaleRoIAlign def get_backbone ( name : str ) -> nn . Module : \"\"\" Get official pretrained ResNet34 and ResNet50 as backbones Parameters ---------- name : str Either `resnet34` or `resnet50` Returns ------- nn.Module resnet34 or resnet50 pytorch modules Raises ------ ValueError If unsupported name is used \"\"\" if name == \"resnet34\" : return torchvision . models . resnet34 ( pretrained = True ) elif name == \"resnet50\" : return torchvision . models . resnet50 ( pretrained = True ) else : raise ValueError ( \"Unsupported backbone\" ) def init_weights ( m ) -> None : \"\"\" Weight initialization Parameters ---------- m : [type] Module used in recursive call \"\"\" if isinstance ( m , nn . Conv2d ): nn . init . xavier_normal_ ( m . weight ) elif isinstance ( m , nn . Linear ): m . weight . data . normal_ ( 0.0 , 0.02 ) m . bias . data . fill_ ( 0.0 ) return class MFN ( nn . Module ): def __init__ ( self , backbone : str ): \"\"\" Implementation of MFN model as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. Parameters ---------- backbone : str Either `resnet34` or `resnet50` \"\"\" super () . __init__ () self . backbone = get_backbone ( backbone ) # input 224x224 -> conv1 output size 112x112 self . start_layer = nn . Sequential ( self . backbone . conv1 , # type: ignore self . backbone . bn1 , # type: ignore self . backbone . relu , # type: ignore self . backbone . maxpool , # type: ignore ) self . r2 = self . backbone . layer1 # 64/256x56x56 <- (resnet34/resnet50) self . r3 = self . backbone . layer2 # 128/512x28x28 self . r4 = self . backbone . layer3 # 256/1024x14x14 self . r5 = self . backbone . layer4 # 512/2048x7x7 in_channel = 64 if backbone == \"resnet34\" else 256 self . b2 = nn . Sequential ( nn . Conv2d ( in_channel , in_channel , kernel_size = 3 , padding = 1 , stride = 2 ), # 56 -> 28 without Relu or batchnorm not in the paper ??? nn . BatchNorm2d ( in_channel ), nn . ReLU ( inplace = True ), nn . Conv2d ( in_channel , in_channel , kernel_size = 3 , padding = 1 , stride = 2 ), # 28 -> 14 nn . BatchNorm2d ( in_channel ), nn . ReLU ( inplace = True ), nn . Conv2d ( in_channel , in_channel * 2 , kernel_size = 1 , padding = 0 ), nn . BatchNorm2d ( in_channel * 2 ), nn . ReLU ( inplace = True ), ) . apply ( init_weights ) # after r2: 128/512x14x14 <- self . b3 = nn . MaxPool2d ( 2 ) # after r3: 128/512x14x14 <- in_channel *= 2 # 128/512 self . b4 = nn . Sequential ( nn . Conv2d ( in_channel * 2 , in_channel , kernel_size = 1 , padding = 0 ), nn . BatchNorm2d ( in_channel ), nn . ReLU ( inplace = True ), ) . apply ( init_weights ) # after r4: 128/512x14x14 in_channel *= 4 # 512 / 2048 self . b5 = nn . Sequential ( nn . ConvTranspose2d ( in_channel , in_channel , kernel_size = 3 , stride = 2 , padding = 1 , output_padding = 1 ), # <- after r5 which is 512x7x7 -> 512x14x14 nn . BatchNorm2d ( in_channel ), nn . ReLU ( inplace = True ), nn . Conv2d ( in_channel , in_channel // 4 , kernel_size = 1 , padding = 0 ), nn . BatchNorm2d ( in_channel // 4 ), nn . ReLU ( inplace = True ), ) . apply ( init_weights ) self . out_channels = 512 if backbone == \"resnet34\" else 2048 # required for FasterRCNN def forward ( self , x ): x = self . start_layer ( x ) x = self . r2 ( x ) b2_out = self . b2 ( x ) x = self . r3 ( x ) b3_out = self . b3 ( x ) x = self . r4 ( x ) b4_out = self . b4 ( x ) x = self . r5 ( x ) b5_out = self . b5 ( x ) # BatchNorm works better than L2 normalize # out = torch.cat([F.normalize(o, p=2, dim=1) for o in (b2_out, b3_out, b4_out, b5_out)], dim=1) out = torch . cat (( b2_out , b3_out , b4_out , b5_out ), dim = 1 ) return out class Classification ( nn . Module ): \"\"\" Classification network Parameters ---------- backbone : str Either `resnet34` or `resnet50` num_classes : int Number of classes \"\"\" def __init__ ( self , backbone : str , num_classes : int ) -> None : super () . __init__ () self . mfn = MFN ( backbone ) self . flatten = nn . Flatten () self . fc = nn . Linear ( self . mfn . out_channels * 14 ** 2 , num_classes ) def forward ( self , x ): return self . fc ( self . flatten ( self . mfn ( x ))) class RPN ( nn . Module ): \"\"\" RPN Module as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. \"\"\" def __init__ ( self , out_channels : int = 512 , rpn_pre_nms_top_n_train : int = 1000 , # torchvision default 2000, rpn_pre_nms_top_n_test : int = 500 , # torchvision default 1000, rpn_post_nms_top_n_train : int = 1000 , # torchvision default 2000, rpn_post_nms_top_n_test : int = 500 , # torchvision default 1000, rpn_nms_thresh : float = 0.7 , rpn_fg_iou_thresh : float = 0.7 , rpn_bg_iou_thresh : float = 0.3 , rpn_batch_size_per_image : int = 256 , rpn_positive_fraction : float = 0.5 , ) -> None : super () . __init__ () rpn_anchor_generator = AnchorGenerator ( sizes = (( 64 , 128 , 256 , 512 ),), aspect_ratios = (( 0.5 , 1.0 , 2.0 ),)) rpn_head = RPNHead ( out_channels , rpn_anchor_generator . num_anchors_per_location ()[ 0 ]) rpn_pre_nms_top_n = dict ( training = rpn_pre_nms_top_n_train , testing = rpn_pre_nms_top_n_test ) rpn_post_nms_top_n = dict ( training = rpn_post_nms_top_n_train , testing = rpn_post_nms_top_n_test ) self . rpn = RegionProposalNetwork ( rpn_anchor_generator , rpn_head , rpn_fg_iou_thresh , rpn_bg_iou_thresh , rpn_batch_size_per_image , rpn_positive_fraction , rpn_pre_nms_top_n , rpn_post_nms_top_n , rpn_nms_thresh , ) def forward ( self , * args , ** kwargs ): return self . rpn ( * args , ** kwargs ) class CustomTwoMLPHead ( nn . Module ): def __init__ ( self , in_channels : int , representation_size : int ): super () . __init__ () self . avgpool = nn . AdaptiveAvgPool2d ( 7 ) self . mlp = nn . Sequential ( nn . Linear ( in_channels , representation_size ), nn . ReLU ( inplace = True ), nn . Linear ( representation_size , representation_size ), nn . ReLU ( inplace = True ), ) def forward ( self , x ): x = self . avgpool ( x ) x = x . flatten ( start_dim = 1 ) x = self . mlp ( x ) return x class RoI ( nn . Module ): \"\"\" ROI Module as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. \"\"\" def __init__ ( self , num_classes : int , box_fg_iou_thresh = 0.5 , box_bg_iou_thresh = 0.5 , box_batch_size_per_image = 512 , box_positive_fraction = 0.25 , bbox_reg_weights = None , box_score_thresh = 0.05 , box_nms_thresh = 0.5 , box_detections_per_img = 100 , ) -> None : super () . __init__ () roi_pooler = MultiScaleRoIAlign ( featmap_names = [ \"0\" ], output_size = 7 , sampling_ratio = 2 ) box_head = CustomTwoMLPHead ( 512 * 7 ** 2 , 1024 ) box_predictor = FastRCNNPredictor ( 1024 , num_classes = num_classes ) self . roi_head = RoIHeads ( roi_pooler , box_head , box_predictor , box_fg_iou_thresh , box_bg_iou_thresh , box_batch_size_per_image , box_positive_fraction , bbox_reg_weights , box_score_thresh , box_nms_thresh , box_detections_per_img , ) def forward ( self , * args , ** kwargs ): return self . roi_head ( * args , ** kwargs ) class Detection ( GeneralizedRCNN ): \"\"\" Detection network as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504. \"\"\" def __init__ ( self , mfn , rpn , roi ): dummy_transform = GeneralizedRCNNTransform ( 800 , 1333 , [ 00.0 , 0.0 , 0.0 ], [ 1.0 , 1.0 , 1.0 ]) super () . __init__ ( mfn , rpn , roi , dummy_transform )","title":"Module sagemaker_defect_detection.models.ddn"},{"location":"reference/sagemaker_defect_detection/models/ddn/#functions","text":"","title":"Functions"},{"location":"reference/sagemaker_defect_detection/models/ddn/#get_backbone","text":"def get_backbone ( name : str ) -> torch . nn . modules . module . Module Get official pretrained ResNet34 and ResNet50 as backbones","title":"get_backbone"},{"location":"reference/sagemaker_defect_detection/models/ddn/#parameters","text":"name : str Either resnet34 or resnet50","title":"Parameters"},{"location":"reference/sagemaker_defect_detection/models/ddn/#returns","text":"nn.Module resnet34 or resnet50 pytorch modules","title":"Returns"},{"location":"reference/sagemaker_defect_detection/models/ddn/#raises","text":"ValueError If unsupported name is used View Source def get_backbone ( name : str ) -> nn . Module : \" \"\" Get official pretrained ResNet34 and ResNet50 as backbones Parameters ---------- name : str Either `resnet34` or `resnet50` Returns ------- nn.Module resnet34 or resnet50 pytorch modules Raises ------ ValueError If unsupported name is used \"\" \" if name == \"resnet34\" : return torchvision . models . resnet34 ( pretrained = True ) elif name == \"resnet50\" : return torchvision . models . resnet50 ( pretrained = True ) else : raise ValueError ( \"Unsupported backbone\" )","title":"Raises"},{"location":"reference/sagemaker_defect_detection/models/ddn/#init_weights","text":"def init_weights ( m ) -> None Weight initialization","title":"init_weights"},{"location":"reference/sagemaker_defect_detection/models/ddn/#parameters_1","text":"m : [type] Module used in recursive call View Source def init_weights ( m ) -> None : \"\"\" Weight initialization Parameters ---------- m : [type] Module used in recursive call \"\"\" if isinstance ( m , nn . Conv2d ) : nn . init . xavier_normal_ ( m . weight ) elif isinstance ( m , nn . Linear ) : m . weight . data . normal_ ( 0.0 , 0.02 ) m . bias . data . fill_ ( 0.0 ) return","title":"Parameters"},{"location":"reference/sagemaker_defect_detection/models/ddn/#classes","text":"","title":"Classes"},{"location":"reference/sagemaker_defect_detection/models/ddn/#classification","text":"class Classification ( backbone : str , num_classes : int ) Classification network","title":"Classification"},{"location":"reference/sagemaker_defect_detection/models/ddn/#parameters_2","text":"backbone : str Either resnet34 or resnet50 num_classes : int Number of classes","title":"Parameters"},{"location":"reference/sagemaker_defect_detection/models/ddn/#ancestors-in-mro","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/sagemaker_defect_detection/models/ddn/#class-variables","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/sagemaker_defect_detection/models/ddn/#methods","text":"","title":"Methods"},{"location":"reference/sagemaker_defect_detection/models/ddn/#add_module","text":"def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module","title":"add_module"},{"location":"reference/sagemaker_defect_detection/models/ddn/#apply","text":"def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self","title":"apply"},{"location":"reference/sagemaker_defect_detection/models/ddn/#bfloat16","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t )","title":"bfloat16"},{"location":"reference/sagemaker_defect_detection/models/ddn/#buffers","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf","title":"buffers"},{"location":"reference/sagemaker_defect_detection/models/ddn/#children","text":"def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module","title":"children"},{"location":"reference/sagemaker_defect_detection/models/ddn/#cpu","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ())","title":"cpu"},{"location":"reference/sagemaker_defect_detection/models/ddn/#cuda","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device ))","title":"cuda"},{"location":"reference/sagemaker_defect_detection/models/ddn/#double","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t )","title":"double"},{"location":"reference/sagemaker_defect_detection/models/ddn/#eval","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False )","title":"eval"},{"location":"reference/sagemaker_defect_detection/models/ddn/#extra_repr","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return ''","title":"extra_repr"},{"location":"reference/sagemaker_defect_detection/models/ddn/#float","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t )","title":"float"},{"location":"reference/sagemaker_defect_detection/models/ddn/#forward","text":"def forward ( self , x ) View Source def forward ( self , x ): return self . fc ( self . flatten ( self . mfn ( x )))","title":"forward"},{"location":"reference/sagemaker_defect_detection/models/ddn/#half","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t )","title":"half"},{"location":"reference/sagemaker_defect_detection/models/ddn/#load_state_dict","text":"def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys )","title":"load_state_dict"},{"location":"reference/sagemaker_defect_detection/models/ddn/#modules","text":"def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module","title":"modules"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_buffers","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_buffers"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_children","text":"def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module","title":"named_children"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_modules","text":"def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m","title":"named_modules"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_parameters","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_parameters"},{"location":"reference/sagemaker_defect_detection/models/ddn/#parameters_3","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param","title":"parameters"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_backward_hook","text":"def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle","title":"register_backward_hook"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_buffer","text":"def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name )","title":"register_buffer"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_forward_hook","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle","title":"register_forward_hook"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_forward_pre_hook","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle","title":"register_forward_pre_hook"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_parameter","text":"def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param","title":"register_parameter"},{"location":"reference/sagemaker_defect_detection/models/ddn/#requires_grad_","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self","title":"requires_grad_"},{"location":"reference/sagemaker_defect_detection/models/ddn/#share_memory","text":"def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ())","title":"share_memory"},{"location":"reference/sagemaker_defect_detection/models/ddn/#state_dict","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination","title":"state_dict"},{"location":"reference/sagemaker_defect_detection/models/ddn/#to","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert )","title":"to"},{"location":"reference/sagemaker_defect_detection/models/ddn/#train","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self","title":"train"},{"location":"reference/sagemaker_defect_detection/models/ddn/#type","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type ))","title":"type"},{"location":"reference/sagemaker_defect_detection/models/ddn/#zero_grad","text":"def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ ()","title":"zero_grad"},{"location":"reference/sagemaker_defect_detection/models/ddn/#customtwomlphead","text":"class CustomTwoMLPHead ( in_channels : int , representation_size : int ) Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model ( nn . Module ): def __init__ ( self ): super ( Model , self ) . __init__ () self . conv1 = nn . Conv2d ( 1 , 20 , 5 ) self . conv2 = nn . Conv2d ( 20 , 20 , 5 ) def forward ( self , x ): x = F . relu ( self . conv1 ( x )) return F . relu ( self . conv2 ( x )) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc.","title":"CustomTwoMLPHead"},{"location":"reference/sagemaker_defect_detection/models/ddn/#ancestors-in-mro_1","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/sagemaker_defect_detection/models/ddn/#class-variables_1","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/sagemaker_defect_detection/models/ddn/#methods_1","text":"","title":"Methods"},{"location":"reference/sagemaker_defect_detection/models/ddn/#add_module_1","text":"def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module","title":"add_module"},{"location":"reference/sagemaker_defect_detection/models/ddn/#apply_1","text":"def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self","title":"apply"},{"location":"reference/sagemaker_defect_detection/models/ddn/#bfloat16_1","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t )","title":"bfloat16"},{"location":"reference/sagemaker_defect_detection/models/ddn/#buffers_1","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf","title":"buffers"},{"location":"reference/sagemaker_defect_detection/models/ddn/#children_1","text":"def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module","title":"children"},{"location":"reference/sagemaker_defect_detection/models/ddn/#cpu_1","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ())","title":"cpu"},{"location":"reference/sagemaker_defect_detection/models/ddn/#cuda_1","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device ))","title":"cuda"},{"location":"reference/sagemaker_defect_detection/models/ddn/#double_1","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t )","title":"double"},{"location":"reference/sagemaker_defect_detection/models/ddn/#eval_1","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False )","title":"eval"},{"location":"reference/sagemaker_defect_detection/models/ddn/#extra_repr_1","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return ''","title":"extra_repr"},{"location":"reference/sagemaker_defect_detection/models/ddn/#float_1","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t )","title":"float"},{"location":"reference/sagemaker_defect_detection/models/ddn/#forward_1","text":"def forward ( self , x ) View Source def forward ( self , x ): x = self . avgpool ( x ) x = x . flatten ( start_dim = 1 ) x = self . mlp ( x ) return x","title":"forward"},{"location":"reference/sagemaker_defect_detection/models/ddn/#half_1","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t )","title":"half"},{"location":"reference/sagemaker_defect_detection/models/ddn/#load_state_dict_1","text":"def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys )","title":"load_state_dict"},{"location":"reference/sagemaker_defect_detection/models/ddn/#modules_1","text":"def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module","title":"modules"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_buffers_1","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_buffers"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_children_1","text":"def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module","title":"named_children"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_modules_1","text":"def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m","title":"named_modules"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_parameters_1","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_parameters"},{"location":"reference/sagemaker_defect_detection/models/ddn/#parameters_4","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param","title":"parameters"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_backward_hook_1","text":"def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle","title":"register_backward_hook"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_buffer_1","text":"def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name )","title":"register_buffer"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_forward_hook_1","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle","title":"register_forward_hook"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_forward_pre_hook_1","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle","title":"register_forward_pre_hook"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_parameter_1","text":"def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param","title":"register_parameter"},{"location":"reference/sagemaker_defect_detection/models/ddn/#requires_grad__1","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self","title":"requires_grad_"},{"location":"reference/sagemaker_defect_detection/models/ddn/#share_memory_1","text":"def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ())","title":"share_memory"},{"location":"reference/sagemaker_defect_detection/models/ddn/#state_dict_1","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination","title":"state_dict"},{"location":"reference/sagemaker_defect_detection/models/ddn/#to_1","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert )","title":"to"},{"location":"reference/sagemaker_defect_detection/models/ddn/#train_1","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self","title":"train"},{"location":"reference/sagemaker_defect_detection/models/ddn/#type_1","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type ))","title":"type"},{"location":"reference/sagemaker_defect_detection/models/ddn/#zero_grad_1","text":"def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ ()","title":"zero_grad"},{"location":"reference/sagemaker_defect_detection/models/ddn/#detection","text":"class Detection ( mfn , rpn , roi ) Detection network as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504.","title":"Detection"},{"location":"reference/sagemaker_defect_detection/models/ddn/#ancestors-in-mro_2","text":"torchvision.models.detection.generalized_rcnn.GeneralizedRCNN torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/sagemaker_defect_detection/models/ddn/#class-variables_2","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/sagemaker_defect_detection/models/ddn/#methods_2","text":"","title":"Methods"},{"location":"reference/sagemaker_defect_detection/models/ddn/#add_module_2","text":"def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module","title":"add_module"},{"location":"reference/sagemaker_defect_detection/models/ddn/#apply_2","text":"def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self","title":"apply"},{"location":"reference/sagemaker_defect_detection/models/ddn/#bfloat16_2","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t )","title":"bfloat16"},{"location":"reference/sagemaker_defect_detection/models/ddn/#buffers_2","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf","title":"buffers"},{"location":"reference/sagemaker_defect_detection/models/ddn/#children_2","text":"def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module","title":"children"},{"location":"reference/sagemaker_defect_detection/models/ddn/#cpu_2","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ())","title":"cpu"},{"location":"reference/sagemaker_defect_detection/models/ddn/#cuda_2","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device ))","title":"cuda"},{"location":"reference/sagemaker_defect_detection/models/ddn/#double_2","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t )","title":"double"},{"location":"reference/sagemaker_defect_detection/models/ddn/#eager_outputs","text":"def eager_outputs ( self , losses , detections ) View Source @torch . jit . unused def eager_outputs ( self , losses , detections ) : # type : ( Dict [ str, Tensor ] , List [ Dict[str, Tensor ] ] ) -> Tuple [ Dict[str, Tensor ] , List [ Dict[str, Tensor ] ]] if self . training : return losses return detections","title":"eager_outputs"},{"location":"reference/sagemaker_defect_detection/models/ddn/#eval_2","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False )","title":"eval"},{"location":"reference/sagemaker_defect_detection/models/ddn/#extra_repr_2","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return ''","title":"extra_repr"},{"location":"reference/sagemaker_defect_detection/models/ddn/#float_2","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t )","title":"float"},{"location":"reference/sagemaker_defect_detection/models/ddn/#forward_2","text":"def forward ( self , images , targets = None ) Arguments: images (list[Tensor]): images to be processed targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional) Returns: result (list[BoxList] or dict[Tensor]): the output from the model. During training, it returns a dict[Tensor] which contains the losses. During testing, it returns list[BoxList] contains additional fields like scores , labels and mask (for Mask R-CNN models). View Source def forward ( self , images , targets = None ) : # type : ( List [ Tensor ] , Optional [ List[Dict[str, Tensor ] ]] ) -> Tuple [ Dict[str, Tensor ] , List [ Dict[str, Tensor ] ]] \"\"\" Arguments: images (list[Tensor]): images to be processed targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional) Returns: result (list[BoxList] or dict[Tensor]): the output from the model. During training, it returns a dict[Tensor] which contains the losses. During testing, it returns list[BoxList] contains additional fields like `scores`, `labels` and `mask` (for Mask R-CNN models). \"\"\" if self . training and targets is None : raise ValueError ( \"In training mode, targets should be passed\" ) if self . training : assert targets is not None for target in targets : boxes = target [ \"boxes\" ] if isinstance ( boxes , torch . Tensor ) : if len ( boxes . shape ) != 2 or boxes . shape [ -1 ] != 4 : raise ValueError ( \"Expected target boxes to be a tensor\" \"of shape [N, 4], got {:}.\" . format ( boxes . shape )) else : raise ValueError ( \"Expected target boxes to be of type \" \"Tensor, got {:}.\" . format ( type ( boxes ))) original_image_sizes = torch . jit . annotate ( List [ Tuple[int, int ] ] , [] ) for img in images : val = img . shape [ -2: ] assert len ( val ) == 2 original_image_sizes . append (( val [ 0 ] , val [ 1 ] )) images , targets = self . transform ( images , targets ) # Check for degenerate boxes # TODO : Move this to a function if targets is not None : for target_idx , target in enumerate ( targets ) : boxes = target [ \"boxes\" ] degenerate_boxes = boxes [ :, 2: ] <= boxes [ :, :2 ] if degenerate_boxes . any () : # print the first degenrate box bb_idx = degenerate_boxes . any ( dim = 1 ). nonzero (). view ( - 1 ) [ 0 ] degen_bb : List [ float ] = boxes [ bb_idx ] . tolist () raise ValueError ( \"All bounding boxes should have positive height and width.\" \" Found invaid box {} for target at index {}.\" . format ( degen_bb , target_idx )) features = self . backbone ( images . tensors ) if isinstance ( features , torch . Tensor ) : features = OrderedDict ( [ ('0', features) ] ) proposals , proposal_losses = self . rpn ( images , features , targets ) detections , detector_losses = self . roi_heads ( features , proposals , images . image_sizes , targets ) detections = self . transform . postprocess ( detections , images . image_sizes , original_image_sizes ) losses = {} losses . update ( detector_losses ) losses . update ( proposal_losses ) if torch . jit . is_scripting () : if not self . _has_warned : warnings . warn ( \"RCNN always returns a (Losses, Detections) tuple in scripting\" ) self . _has_warned = True return ( losses , detections ) else : return self . eager_outputs ( losses , detections )","title":"forward"},{"location":"reference/sagemaker_defect_detection/models/ddn/#half_2","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t )","title":"half"},{"location":"reference/sagemaker_defect_detection/models/ddn/#load_state_dict_2","text":"def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys )","title":"load_state_dict"},{"location":"reference/sagemaker_defect_detection/models/ddn/#modules_2","text":"def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module","title":"modules"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_buffers_2","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_buffers"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_children_2","text":"def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module","title":"named_children"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_modules_2","text":"def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m","title":"named_modules"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_parameters_2","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_parameters"},{"location":"reference/sagemaker_defect_detection/models/ddn/#parameters_5","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param","title":"parameters"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_backward_hook_2","text":"def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle","title":"register_backward_hook"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_buffer_2","text":"def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name )","title":"register_buffer"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_forward_hook_2","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle","title":"register_forward_hook"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_forward_pre_hook_2","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle","title":"register_forward_pre_hook"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_parameter_2","text":"def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param","title":"register_parameter"},{"location":"reference/sagemaker_defect_detection/models/ddn/#requires_grad__2","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self","title":"requires_grad_"},{"location":"reference/sagemaker_defect_detection/models/ddn/#share_memory_2","text":"def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ())","title":"share_memory"},{"location":"reference/sagemaker_defect_detection/models/ddn/#state_dict_2","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination","title":"state_dict"},{"location":"reference/sagemaker_defect_detection/models/ddn/#to_2","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert )","title":"to"},{"location":"reference/sagemaker_defect_detection/models/ddn/#train_2","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self","title":"train"},{"location":"reference/sagemaker_defect_detection/models/ddn/#type_2","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type ))","title":"type"},{"location":"reference/sagemaker_defect_detection/models/ddn/#zero_grad_2","text":"def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ ()","title":"zero_grad"},{"location":"reference/sagemaker_defect_detection/models/ddn/#mfn","text":"class MFN ( backbone : str ) Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model ( nn . Module ): def __init__ ( self ): super ( Model , self ) . __init__ () self . conv1 = nn . Conv2d ( 1 , 20 , 5 ) self . conv2 = nn . Conv2d ( 20 , 20 , 5 ) def forward ( self , x ): x = F . relu ( self . conv1 ( x )) return F . relu ( self . conv2 ( x )) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc.","title":"MFN"},{"location":"reference/sagemaker_defect_detection/models/ddn/#ancestors-in-mro_3","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/sagemaker_defect_detection/models/ddn/#class-variables_3","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/sagemaker_defect_detection/models/ddn/#methods_3","text":"","title":"Methods"},{"location":"reference/sagemaker_defect_detection/models/ddn/#add_module_3","text":"def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module","title":"add_module"},{"location":"reference/sagemaker_defect_detection/models/ddn/#apply_3","text":"def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self","title":"apply"},{"location":"reference/sagemaker_defect_detection/models/ddn/#bfloat16_3","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t )","title":"bfloat16"},{"location":"reference/sagemaker_defect_detection/models/ddn/#buffers_3","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf","title":"buffers"},{"location":"reference/sagemaker_defect_detection/models/ddn/#children_3","text":"def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module","title":"children"},{"location":"reference/sagemaker_defect_detection/models/ddn/#cpu_3","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ())","title":"cpu"},{"location":"reference/sagemaker_defect_detection/models/ddn/#cuda_3","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device ))","title":"cuda"},{"location":"reference/sagemaker_defect_detection/models/ddn/#double_3","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t )","title":"double"},{"location":"reference/sagemaker_defect_detection/models/ddn/#eval_3","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False )","title":"eval"},{"location":"reference/sagemaker_defect_detection/models/ddn/#extra_repr_3","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return ''","title":"extra_repr"},{"location":"reference/sagemaker_defect_detection/models/ddn/#float_3","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t )","title":"float"},{"location":"reference/sagemaker_defect_detection/models/ddn/#forward_3","text":"def forward ( self , x ) View Source def forward ( self , x ): x = self . start_layer ( x ) x = self . r2 ( x ) b2_out = self . b2 ( x ) x = self . r3 ( x ) b3_out = self . b3 ( x ) x = self . r4 ( x ) b4_out = self . b4 ( x ) x = self . r5 ( x ) b5_out = self . b5 ( x ) # BatchNorm works better than L2 normalize # out = torch . cat ([ F . normalize ( o , p = 2 , dim = 1 ) for o in ( b2_out , b3_out , b4_out , b5_out )], dim = 1 ) out = torch . cat (( b2_out , b3_out , b4_out , b5_out ), dim = 1 ) return out","title":"forward"},{"location":"reference/sagemaker_defect_detection/models/ddn/#half_3","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t )","title":"half"},{"location":"reference/sagemaker_defect_detection/models/ddn/#load_state_dict_3","text":"def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys )","title":"load_state_dict"},{"location":"reference/sagemaker_defect_detection/models/ddn/#modules_3","text":"def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module","title":"modules"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_buffers_3","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_buffers"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_children_3","text":"def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module","title":"named_children"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_modules_3","text":"def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m","title":"named_modules"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_parameters_3","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_parameters"},{"location":"reference/sagemaker_defect_detection/models/ddn/#parameters_6","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param","title":"parameters"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_backward_hook_3","text":"def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle","title":"register_backward_hook"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_buffer_3","text":"def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name )","title":"register_buffer"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_forward_hook_3","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle","title":"register_forward_hook"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_forward_pre_hook_3","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle","title":"register_forward_pre_hook"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_parameter_3","text":"def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param","title":"register_parameter"},{"location":"reference/sagemaker_defect_detection/models/ddn/#requires_grad__3","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self","title":"requires_grad_"},{"location":"reference/sagemaker_defect_detection/models/ddn/#share_memory_3","text":"def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ())","title":"share_memory"},{"location":"reference/sagemaker_defect_detection/models/ddn/#state_dict_3","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination","title":"state_dict"},{"location":"reference/sagemaker_defect_detection/models/ddn/#to_3","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert )","title":"to"},{"location":"reference/sagemaker_defect_detection/models/ddn/#train_3","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self","title":"train"},{"location":"reference/sagemaker_defect_detection/models/ddn/#type_3","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type ))","title":"type"},{"location":"reference/sagemaker_defect_detection/models/ddn/#zero_grad_3","text":"def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ ()","title":"zero_grad"},{"location":"reference/sagemaker_defect_detection/models/ddn/#rpn","text":"class RPN ( out_channels : int = 512 , rpn_pre_nms_top_n_train : int = 1000 , rpn_pre_nms_top_n_test : int = 500 , rpn_post_nms_top_n_train : int = 1000 , rpn_post_nms_top_n_test : int = 500 , rpn_nms_thresh : float = 0.7 , rpn_fg_iou_thresh : float = 0.7 , rpn_bg_iou_thresh : float = 0.3 , rpn_batch_size_per_image : int = 256 , rpn_positive_fraction : float = 0.5 ) RPN Module as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504.","title":"RPN"},{"location":"reference/sagemaker_defect_detection/models/ddn/#ancestors-in-mro_4","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/sagemaker_defect_detection/models/ddn/#class-variables_4","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/sagemaker_defect_detection/models/ddn/#methods_4","text":"","title":"Methods"},{"location":"reference/sagemaker_defect_detection/models/ddn/#add_module_4","text":"def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module","title":"add_module"},{"location":"reference/sagemaker_defect_detection/models/ddn/#apply_4","text":"def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self","title":"apply"},{"location":"reference/sagemaker_defect_detection/models/ddn/#bfloat16_4","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t )","title":"bfloat16"},{"location":"reference/sagemaker_defect_detection/models/ddn/#buffers_4","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf","title":"buffers"},{"location":"reference/sagemaker_defect_detection/models/ddn/#children_4","text":"def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module","title":"children"},{"location":"reference/sagemaker_defect_detection/models/ddn/#cpu_4","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ())","title":"cpu"},{"location":"reference/sagemaker_defect_detection/models/ddn/#cuda_4","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device ))","title":"cuda"},{"location":"reference/sagemaker_defect_detection/models/ddn/#double_4","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t )","title":"double"},{"location":"reference/sagemaker_defect_detection/models/ddn/#eval_4","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False )","title":"eval"},{"location":"reference/sagemaker_defect_detection/models/ddn/#extra_repr_4","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return ''","title":"extra_repr"},{"location":"reference/sagemaker_defect_detection/models/ddn/#float_4","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t )","title":"float"},{"location":"reference/sagemaker_defect_detection/models/ddn/#forward_4","text":"def forward ( self , * args , ** kwargs ) View Source def forward ( self , * args , ** kwargs ): return self . rpn ( * args , ** kwargs )","title":"forward"},{"location":"reference/sagemaker_defect_detection/models/ddn/#half_4","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t )","title":"half"},{"location":"reference/sagemaker_defect_detection/models/ddn/#load_state_dict_4","text":"def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys )","title":"load_state_dict"},{"location":"reference/sagemaker_defect_detection/models/ddn/#modules_4","text":"def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module","title":"modules"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_buffers_4","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_buffers"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_children_4","text":"def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module","title":"named_children"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_modules_4","text":"def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m","title":"named_modules"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_parameters_4","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_parameters"},{"location":"reference/sagemaker_defect_detection/models/ddn/#parameters_7","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param","title":"parameters"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_backward_hook_4","text":"def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle","title":"register_backward_hook"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_buffer_4","text":"def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name )","title":"register_buffer"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_forward_hook_4","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle","title":"register_forward_hook"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_forward_pre_hook_4","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle","title":"register_forward_pre_hook"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_parameter_4","text":"def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param","title":"register_parameter"},{"location":"reference/sagemaker_defect_detection/models/ddn/#requires_grad__4","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self","title":"requires_grad_"},{"location":"reference/sagemaker_defect_detection/models/ddn/#share_memory_4","text":"def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ())","title":"share_memory"},{"location":"reference/sagemaker_defect_detection/models/ddn/#state_dict_4","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination","title":"state_dict"},{"location":"reference/sagemaker_defect_detection/models/ddn/#to_4","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert )","title":"to"},{"location":"reference/sagemaker_defect_detection/models/ddn/#train_4","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self","title":"train"},{"location":"reference/sagemaker_defect_detection/models/ddn/#type_4","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type ))","title":"type"},{"location":"reference/sagemaker_defect_detection/models/ddn/#zero_grad_4","text":"def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ ()","title":"zero_grad"},{"location":"reference/sagemaker_defect_detection/models/ddn/#roi","text":"class RoI ( num_classes : int , box_fg_iou_thresh = 0.5 , box_bg_iou_thresh = 0.5 , box_batch_size_per_image = 512 , box_positive_fraction = 0.25 , bbox_reg_weights = None , box_score_thresh = 0.05 , box_nms_thresh = 0.5 , box_detections_per_img = 100 ) ROI Module as described in Yu He, Kechen Song, Qinggang Meng, Yunhui Yan, \u201cAn End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,\u201d IEEE Transactions on Instrumentation and Measuremente, 2020,69(4),1493-1504.","title":"RoI"},{"location":"reference/sagemaker_defect_detection/models/ddn/#ancestors-in-mro_5","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/sagemaker_defect_detection/models/ddn/#class-variables_5","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/sagemaker_defect_detection/models/ddn/#methods_5","text":"","title":"Methods"},{"location":"reference/sagemaker_defect_detection/models/ddn/#add_module_5","text":"def add_module ( self , name : str , module : 'Module' ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. View Source def add_module ( self , name : str , module : 'Module' ) -> None : r \"\"\"Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. \"\"\" if not isinstance ( module , Module ) and module is not None : raise TypeError ( \"{} is not a Module subclass\" . format ( torch . typename ( module ))) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"module name should be a string. Got {}\" . format ( torch . typename ( name ))) elif hasattr ( self , name ) and name not in self . _modules : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif '.' in name : raise KeyError ( \"module name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"module name can't be empty string \\\" \\ \"\" ) self . _modules [ name ] = module","title":"add_module"},{"location":"reference/sagemaker_defect_detection/models/ddn/#apply_5","text":"def apply ( self : ~ T , fn : Callable [[ _ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) View Source def apply ( self : T , fn : Callable [[ 'Module' ] , None ] ) -> T : r \" \"\" Applies ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) \"\" \" for module in self . children () : module . apply ( fn ) fn ( self ) return self","title":"apply"},{"location":"reference/sagemaker_defect_detection/models/ddn/#bfloat16_5","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self View Source def bfloat16 ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``bfloat16`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . bfloat16 () if t . is_floating_point () else t )","title":"bfloat16"},{"location":"reference/sagemaker_defect_detection/models/ddn/#buffers_5","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def buffers ( self , recurse : bool = True ) -> Iterator [ Tensor ] : r \"\"\"Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , buf in self . named_buffers ( recurse = recurse ) : yield buf","title":"buffers"},{"location":"reference/sagemaker_defect_detection/models/ddn/#children_5","text":"def children ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Module: a child module View Source def children ( self ) -> Iterator [ 'Module']: r \"\"\"Returns an iterator over immediate children modules. Yields: Module: a child module \"\"\" for name , module in self . named_children (): yield module","title":"children"},{"location":"reference/sagemaker_defect_detection/models/ddn/#cpu_5","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. Returns: Module: self View Source def cpu ( self : T ) -> T : r \"\"\"Moves all model parameters and buffers to the CPU. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cpu ())","title":"cpu"},{"location":"reference/sagemaker_defect_detection/models/ddn/#cuda_5","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self View Source def cuda ( self : T , device : Optional [ Union [ int , device ]] = None ) -> T : r \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self \"\"\" return self . _apply ( lambda t : t . cuda ( device ))","title":"cuda"},{"location":"reference/sagemaker_defect_detection/models/ddn/#double_5","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. Returns: Module: self View Source def double ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``double`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . double () if t . is_floating_point () else t )","title":"double"},{"location":"reference/sagemaker_defect_detection/models/ddn/#eval_5","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self View Source def eval ( self : T ) -> T : r \" \"\" Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. Returns: Module: self \"\" \" return self . train ( False )","title":"eval"},{"location":"reference/sagemaker_defect_detection/models/ddn/#extra_repr_5","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. View Source def extra_repr ( self ) -> str : r \"\"\"Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. \"\"\" return ''","title":"extra_repr"},{"location":"reference/sagemaker_defect_detection/models/ddn/#float_5","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. Returns: Module: self View Source def float ( self : T ) -> T : r \"\"\"Casts all floating point parameters and buffers to float datatype. Returns: Module: self \"\"\" return self . _apply ( lambda t : t . float () if t . is_floating_point () else t )","title":"float"},{"location":"reference/sagemaker_defect_detection/models/ddn/#forward_5","text":"def forward ( self , * args , ** kwargs ) View Source def forward ( self , * args , ** kwargs ): return self . roi_head ( * args , ** kwargs )","title":"forward"},{"location":"reference/sagemaker_defect_detection/models/ddn/#half_5","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. Returns: Module: self View Source def half ( self : T ) -> T : r \" \"\" Casts all floating point parameters and buffers to ``half`` datatype. Returns: Module: self \"\" \" return self . _apply ( lambda t : t . half () if t . is_floating_point () else t )","title":"half"},{"location":"reference/sagemaker_defect_detection/models/ddn/#load_state_dict_5","text":"def load_state_dict ( self , state_dict : Dict [ str , torch . Tensor ], strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys View Source def load_state_dict ( self , state_dict: Union [ Dict [ str , Tensor ], Dict [ str , Tensor ]], strict : bool = True ) : r \"\"\"Copies parameters and buffers from :attr:`state_dict` into this module and its descendants. If :attr:`strict` is ``True``, then the keys of :attr:`state_dict` must exactly match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Arguments: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: * **missing_keys** is a list of str containing the missing keys * **unexpected_keys** is a list of str containing the unexpected keys \"\"\" missing_keys = [] unexpected_keys = [] error_msgs = [] # copy state_dict so _ load_from_state_dict can modify it metadata = getattr ( state_dict , '_metadata' , None ) state_dict = state_dict . copy () if metadata is not None : state_dict . _ metadata = metadata def load ( module , prefix='' ) : local_metadata = {} if metadata is None else metadata . get ( prefix [:- 1 ], {}) module . _ load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) for name , child in module . _ modules . items () : if child is not None : load ( child , prefix + name + '.' ) load ( self ) load = None # break load->load reference cycle if strict : if len ( unexpected_keys ) > 0 : error_msgs . insert ( 0 , 'Unexpected key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in unexpected_keys ))) if len ( missing_keys ) > 0 : error_msgs . insert ( 0 , 'Missing key(s) in state_dict: {}. ' . format ( ', ' . join ( '\"{}\"' . format ( k ) for k in missing_keys ))) if len ( error_msgs ) > 0 : raise RuntimeError ( 'Error(s) in loading state_dict for {}:\\n\\t{}' . format ( self . __ class__ . __ name__ , \"\\n\\t\" . join ( error_msgs ))) return _ IncompatibleKeys ( missing_keys , unexpected_keys )","title":"load_state_dict"},{"location":"reference/sagemaker_defect_detection/models/ddn/#modules_5","text":"def modules ( self ) -> Iterator [ _ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . modules ()): print ( idx , '->' , m ) 0 -> Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ) 1 -> Linear ( in_features = 2 , out_features = 2 , bias = True ) View Source def modules ( self ) -> Iterator [ 'Module' ] : r \" \"\" Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) \"\" \" for name , module in self . named_modules () : yield module","title":"modules"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_buffers_5","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name , buf in self . named_buffers (): >>> if name in [ 'running_var' ]: >>> print ( buf . size ()) View Source def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) \"\"\" gen = self . _named_members ( lambda module : module . _buffers . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_buffers"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_children_5","text":"def named_children ( self ) -> Iterator [ Tuple [ str , _ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) View Source def named_children ( self ) -> Iterator [ Tuple [ str , 'Module' ]]: r \"\"\"Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) \"\"\" memo = set () for name , module in self . _modules . items (): if module is not None and module not in memo : memo . add ( module ) yield name , module","title":"named_children"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_modules_5","text":"def named_modules ( self , memo : Union [ Set [ _ForwardRef ( 'Module' )], NoneType ] = None , prefix : str = '' ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn . Linear ( 2 , 2 ) >>> net = nn . Sequential ( l , l ) >>> for idx , m in enumerate ( net . named_modules ()): print ( idx , '->' , m ) 0 -> ( '' , Sequential ( ( 0 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) ( 1 ): Linear ( in_features = 2 , out_features = 2 , bias = True ) )) 1 -> ( '0' , Linear ( in_features = 2 , out_features = 2 , bias = True )) View Source def named_modules ( self , memo : Optional [ Set [ 'Module' ]] = None , prefix : str = '' ) : r \" \"\" Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) \"\" \" if memo is None : memo = set () if self not in memo : memo . add ( self ) yield prefix , self for name , module in self . _modules . items () : if module is None : continue submodule_prefix = prefix + ( '.' if prefix else '' ) + name for m in module . named_modules ( memo , submodule_prefix ) : yield m","title":"named_modules"},{"location":"reference/sagemaker_defect_detection/models/ddn/#named_parameters_5","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) View Source def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , Tensor ]]: r \"\"\"Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) \"\"\" gen = self . _named_members ( lambda module : module . _parameters . items (), prefix = prefix , recurse = recurse ) for elem in gen : yield elem","title":"named_parameters"},{"location":"reference/sagemaker_defect_detection/models/ddn/#parameters_8","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) View Source def parameters ( self , recurse : bool = True ) -> Iterator [ Parameter ] : r \"\"\"Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) \"\"\" for name , param in self . named_parameters ( recurse = recurse ) : yield param","title":"parameters"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_backward_hook_5","text":"def register_backward_hook ( self , hook : Callable [[ _ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Union [ NoneType , torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex : class : `Module` that perform many operations . In some failure cases , : attr : `grad_input` and : attr : `grad_output` will only contain the gradients for a subset of the inputs and outputs . For such : class : `Module` , you should use : func : `torch.Tensor.register_hook` directly on a specific input or output to get the required gradients . The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr: grad_input and :attr: grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_backward_hook ( self , hook : Callable [[ 'Module' , _grad_t , _grad_t ] , Union [ None , Tensor ]] ) -> RemovableHandle : r \" \"\" Registers a backward hook on the module. .. warning :: The current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> Tensor or None The :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _backward_hooks ) self . _backward_hooks [ handle . id ] = hook return handle","title":"register_backward_hook"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_buffer_5","text":"def register_buffer ( self , name : str , tensor : torch . Tensor , persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) View Source def register_buffer ( self , name : str , tensor : Tensor , persistent : bool = True ) -> None : r \" \"\" Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) \"\" \" if persistent is False and isinstance ( self , torch . jit . ScriptModule ) : raise RuntimeError ( \"ScriptModule does not support non-persistent buffers\" ) if '_buffers' not in self . __dict__ : raise AttributeError ( \"cannot assign buffer before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"buffer name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"buffer name can't contain \\\" . \\\" \" ) elif name == '' : raise KeyError ( \"buffer name can't be empty string \\\"\\\" \" ) elif hasattr ( self , name ) and name not in self . _buffers : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) elif tensor is not None and not isinstance ( tensor , torch . Tensor ) : raise TypeError ( \"cannot assign '{}' object to buffer '{}' \" \"(torch Tensor or None required)\" . format ( torch . typename ( tensor ), name )) else : self . _buffers [ name ] = tensor if persistent : self . _non_persistent_buffers_set . discard ( name ) else : self . _non_persistent_buffers_set . add ( name )","title":"register_buffer"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_forward_hook_5","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_hooks ) self . _forward_hooks [ handle . id ] = hook return handle","title":"register_forward_hook"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_forward_pre_hook_5","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() View Source def register_forward_pre_hook ( self , hook : Callable [ ..., None ] ) -> RemovableHandle : r \" \"\" Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` \"\" \" handle = hooks . RemovableHandle ( self . _forward_pre_hooks ) self . _forward_pre_hooks [ handle . id ] = hook return handle","title":"register_forward_pre_hook"},{"location":"reference/sagemaker_defect_detection/models/ddn/#register_parameter_5","text":"def register_parameter ( self , name : str , param : torch . nn . parameter . Parameter ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. View Source def register_parameter ( self , name : str , param : Parameter ) -> None : r \"\"\"Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. \"\"\" if '_parameters' not in self . __dict__ : raise AttributeError ( \"cannot assign parameter before Module.__init__() call\" ) elif not isinstance ( name , torch . _six . string_classes ) : raise TypeError ( \"parameter name should be a string. \" \"Got {}\" . format ( torch . typename ( name ))) elif '.' in name : raise KeyError ( \"parameter name can't contain \\\" . \\ \"\" ) elif name == '' : raise KeyError ( \"parameter name can't be empty string \\\" \\ \"\" ) elif hasattr ( self , name ) and name not in self . _parameters : raise KeyError ( \"attribute '{}' already exists\" . format ( name )) if param is None : self . _parameters [ name ] = None elif not isinstance ( param , Parameter ) : raise TypeError ( \"cannot assign '{}' object to parameter '{}' \" \"(torch.nn.Parameter or None required)\" . format ( torch . typename ( param ), name )) elif param . grad_fn : raise ValueError ( \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \" \"parameters must be created explicitly. To express '{0}' \" \"as a function of another Tensor, compute the value in \" \"the forward() method.\" . format ( name )) else : self . _parameters [ name ] = param","title":"register_parameter"},{"location":"reference/sagemaker_defect_detection/models/ddn/#requires_grad__5","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self View Source def requires_grad_ ( self : T , requires_grad : bool = True ) -> T : r \" \"\" Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self \"\" \" for p in self . parameters () : p . requires_grad_ ( requires_grad ) return self","title":"requires_grad_"},{"location":"reference/sagemaker_defect_detection/models/ddn/#share_memory_5","text":"def share_memory ( self : ~ T ) -> ~ T View Source def share_memory ( self : T ) -> T : return self . _apply ( lambda t : t . share_memory_ ())","title":"share_memory"},{"location":"reference/sagemaker_defect_detection/models/ddn/#state_dict_5","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] View Source def state_dict ( self , destination = None , prefix='' , keep_vars = False ) : r \"\"\"Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] \"\"\" if destination is None : destination = OrderedDict () destination . _ metadata = OrderedDict () destination . _ metadata [ prefix [:- 1 ]] = local_metadata = dict ( version = self . _ version ) self . _ save_to_state_dict ( destination , prefix , keep_vars ) for name , module in self . _ modules . items () : if module is not None : module . state_dict ( destination , prefix + name + '.' , keep_vars = keep_vars ) for hook in self . _ state_dict_hooks . values () : hook_result = hook ( self , destination , prefix , local_metadata ) if hook_result is not None : destination = hook_result return destination","title":"state_dict"},{"location":"reference/sagemaker_defect_detection/models/ddn/#to_5","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) View Source def to ( self , * args , ** kwargs ) : r \" \"\" Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point desired :attr:`dtype` s. In addition, this method will only cast the floating point parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Example:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\" cuda : 1 \") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\" cpu \") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) \"\" \" device , dtype , non_blocking , convert_to_format = torch . _C . _nn . _parse_to ( * args , ** kwargs ) if dtype is not None : if not dtype . is_floating_point : raise TypeError ( 'nn.Module.to only accepts floating point ' 'dtypes, but got desired dtype={}' . format ( dtype )) def convert ( t ) : if convert_to_format is not None and t . dim () == 4 : return t . to ( device , dtype if t . is_floating_point () else None , non_blocking , memory_format = convert_to_format ) return t . to ( device , dtype if t . is_floating_point () else None , non_blocking ) return self . _apply ( convert )","title":"to"},{"location":"reference/sagemaker_defect_detection/models/ddn/#train_5","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self View Source def train ( self : T , mode : bool = True ) -> T : r \" \"\" Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self \"\" \" self . training = mode for module in self . children () : module . train ( mode ) return self","title":"train"},{"location":"reference/sagemaker_defect_detection/models/ddn/#type_5","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . Arguments: dst_type (type or string): the desired type Returns: Module: self View Source def type ( self : T , dst_type : Union [ dtype , str ] ) -> T : r \" \"\" Casts all parameters and buffers to :attr:`dst_type`. Arguments: dst_type (type or string): the desired type Returns: Module: self \"\" \" return self . _apply ( lambda t : t . type ( dst_type ))","title":"type"},{"location":"reference/sagemaker_defect_detection/models/ddn/#zero_grad_5","text":"def zero_grad ( self ) -> None Sets gradients of all model parameters to zero. View Source def zero_grad ( self ) -> None : r \"\"\"Sets gradients of all model parameters to zero.\"\"\" if getattr ( self , '_is_replica' , False ): warnings . warn ( \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \" \"The parameters are copied (in a differentiable manner) from the original module. \" \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \" \"If you need gradients in your forward method, consider using autograd.grad instead.\" ) for p in self . parameters (): if p . grad is not None : p . grad . detach_ () p . grad . zero_ ()","title":"zero_grad"},{"location":"reference/sagemaker_defect_detection/utils/","text":"Module sagemaker_defect_detection.utils View Source from typing import Optional , Union from pathlib import Path import tarfile import logging from logging.config import fileConfig import torch import torch.nn as nn logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) def get_logger ( config_path : str ) -> logging . Logger : fileConfig ( config_path , disable_existing_loggers = False ) logger = logging . getLogger () return logger def str2bool ( flag : Union [ str , bool ]) -> bool : if not isinstance ( flag , bool ): if flag . lower () == \"false\" : flag = False elif flag . lower () == \"true\" : flag = True else : raise ValueError ( \"Wrong boolean argument!\" ) return flag def freeze ( m : nn . Module ) -> None : assert isinstance ( m , nn . Module ), \"freeze only is applied to modules\" for param in m . parameters (): param . requires_grad = False return def load_checkpoint ( model : nn . Module , path : str , prefix : Optional [ str ]) -> nn . Module : path = Path ( path ) logger . info ( f \"path: {path}\" ) if path . is_dir (): path_str = str ( list ( path . rglob ( \"*.ckpt\" ))[ 0 ]) else : path_str = str ( path ) device = \"cuda\" if torch . cuda . is_available () else \"cpu\" state_dict = torch . load ( path_str , map_location = torch . device ( device ))[ \"state_dict\" ] if prefix is not None : if prefix [ - 1 ] != \".\" : prefix += \".\" state_dict = { k [ len ( prefix ) :]: v for k , v in state_dict . items () if k . startswith ( prefix )} model . load_state_dict ( state_dict , strict = True ) return model Sub-modules sagemaker_defect_detection.utils.coco_eval sagemaker_defect_detection.utils.coco_utils sagemaker_defect_detection.utils.visualize Variables logger Functions freeze def freeze ( m : torch . nn . modules . module . Module ) -> None View Source def freeze ( m : nn . Module ) -> None : assert isinstance ( m , nn . Module ), \"freeze only is applied to modules\" for param in m . parameters (): param . requires_grad = False return get_logger def get_logger ( config_path : str ) -> logging . Logger View Source def get_logger ( config_path : str ) -> logging . Logger : fileConfig ( config_path , disable_existing_loggers = False ) logger = logging . getLogger () return logger load_checkpoint def load_checkpoint ( model : torch . nn . modules . module . Module , path : str , prefix : Union [ str , NoneType ] ) -> torch . nn . modules . module . Module View Source def load_checkpoint ( model : nn . Module , path : str , prefix : Optional [ str ] ) -> nn . Module : path = Path ( path ) logger . info ( f \"path: {path}\" ) if path . is_dir () : path_str = str ( list ( path . rglob ( \"*.ckpt\" )) [ 0 ] ) else : path_str = str ( path ) device = \"cuda\" if torch . cuda . is_available () else \"cpu\" state_dict = torch . load ( path_str , map_location = torch . device ( device )) [ \"state_dict\" ] if prefix is not None : if prefix [ -1 ] != \".\" : prefix += \".\" state_dict = { k [ len(prefix) : ] : v for k , v in state_dict . items () if k . startswith ( prefix ) } model . load_state_dict ( state_dict , strict = True ) return model str2bool def str2bool ( flag : Union [ str , bool ] ) -> bool View Source def str2bool ( flag : Union [ str , bool ]) -> bool : if not isinstance ( flag , bool ): if flag . lower () == \"false\" : flag = False elif flag . lower () == \"true\" : flag = True else : raise ValueError ( \"Wrong boolean argument!\" ) return flag","title":"Index"},{"location":"reference/sagemaker_defect_detection/utils/#module-sagemaker_defect_detectionutils","text":"View Source from typing import Optional , Union from pathlib import Path import tarfile import logging from logging.config import fileConfig import torch import torch.nn as nn logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) def get_logger ( config_path : str ) -> logging . Logger : fileConfig ( config_path , disable_existing_loggers = False ) logger = logging . getLogger () return logger def str2bool ( flag : Union [ str , bool ]) -> bool : if not isinstance ( flag , bool ): if flag . lower () == \"false\" : flag = False elif flag . lower () == \"true\" : flag = True else : raise ValueError ( \"Wrong boolean argument!\" ) return flag def freeze ( m : nn . Module ) -> None : assert isinstance ( m , nn . Module ), \"freeze only is applied to modules\" for param in m . parameters (): param . requires_grad = False return def load_checkpoint ( model : nn . Module , path : str , prefix : Optional [ str ]) -> nn . Module : path = Path ( path ) logger . info ( f \"path: {path}\" ) if path . is_dir (): path_str = str ( list ( path . rglob ( \"*.ckpt\" ))[ 0 ]) else : path_str = str ( path ) device = \"cuda\" if torch . cuda . is_available () else \"cpu\" state_dict = torch . load ( path_str , map_location = torch . device ( device ))[ \"state_dict\" ] if prefix is not None : if prefix [ - 1 ] != \".\" : prefix += \".\" state_dict = { k [ len ( prefix ) :]: v for k , v in state_dict . items () if k . startswith ( prefix )} model . load_state_dict ( state_dict , strict = True ) return model","title":"Module sagemaker_defect_detection.utils"},{"location":"reference/sagemaker_defect_detection/utils/#sub-modules","text":"sagemaker_defect_detection.utils.coco_eval sagemaker_defect_detection.utils.coco_utils sagemaker_defect_detection.utils.visualize","title":"Sub-modules"},{"location":"reference/sagemaker_defect_detection/utils/#variables","text":"logger","title":"Variables"},{"location":"reference/sagemaker_defect_detection/utils/#functions","text":"","title":"Functions"},{"location":"reference/sagemaker_defect_detection/utils/#freeze","text":"def freeze ( m : torch . nn . modules . module . Module ) -> None View Source def freeze ( m : nn . Module ) -> None : assert isinstance ( m , nn . Module ), \"freeze only is applied to modules\" for param in m . parameters (): param . requires_grad = False return","title":"freeze"},{"location":"reference/sagemaker_defect_detection/utils/#get_logger","text":"def get_logger ( config_path : str ) -> logging . Logger View Source def get_logger ( config_path : str ) -> logging . Logger : fileConfig ( config_path , disable_existing_loggers = False ) logger = logging . getLogger () return logger","title":"get_logger"},{"location":"reference/sagemaker_defect_detection/utils/#load_checkpoint","text":"def load_checkpoint ( model : torch . nn . modules . module . Module , path : str , prefix : Union [ str , NoneType ] ) -> torch . nn . modules . module . Module View Source def load_checkpoint ( model : nn . Module , path : str , prefix : Optional [ str ] ) -> nn . Module : path = Path ( path ) logger . info ( f \"path: {path}\" ) if path . is_dir () : path_str = str ( list ( path . rglob ( \"*.ckpt\" )) [ 0 ] ) else : path_str = str ( path ) device = \"cuda\" if torch . cuda . is_available () else \"cpu\" state_dict = torch . load ( path_str , map_location = torch . device ( device )) [ \"state_dict\" ] if prefix is not None : if prefix [ -1 ] != \".\" : prefix += \".\" state_dict = { k [ len(prefix) : ] : v for k , v in state_dict . items () if k . startswith ( prefix ) } model . load_state_dict ( state_dict , strict = True ) return model","title":"load_checkpoint"},{"location":"reference/sagemaker_defect_detection/utils/#str2bool","text":"def str2bool ( flag : Union [ str , bool ] ) -> bool View Source def str2bool ( flag : Union [ str , bool ]) -> bool : if not isinstance ( flag , bool ): if flag . lower () == \"false\" : flag = False elif flag . lower () == \"true\" : flag = True else : raise ValueError ( \"Wrong boolean argument!\" ) return flag","title":"str2bool"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/","text":"Module sagemaker_defect_detection.utils.coco_eval BSD 3-Clause License Copyright (c) Soumith Chintala 2016, All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. This module is a modified version of https://github.com/pytorch/vision/tree/03b1d38ba3c67703e648fb067570eeb1a1e61265/references/detection View Source \"\"\" BSD 3-Clause License Copyright (c) Soumith Chintala 2016, All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. This module is a modified version of https://github.com/pytorch/vision/tree/03b1d38ba3c67703e648fb067570eeb1a1e61265/references/detection \"\"\" import json import numpy as np import copy import torch import pickle import torch . distributed as dist from pycocotools . cocoeval import COCOeval from pycocotools . coco import COCO import pycocotools . mask as mask_util from collections import defaultdict def is_dist_avail_and_initialized (): if not dist . is_available (): return False if not dist . is_initialized (): return False return True def get_world_size (): if not is_dist_avail_and_initialized (): return 1 return dist . get_world_size () def all_gather ( data ): \"\"\" Run all_gather on arbitrary picklable data (not necessarily tensors) Args: data: any picklable object Returns: list[data]: list of data gathered from each rank \"\"\" world_size = get_world_size () if world_size == 1 : return [ data ] # serialized to a Tensor buffer = pickle . dumps ( data ) storage = torch . ByteStorage . from_buffer ( buffer ) tensor = torch . ByteTensor ( storage ) . to ( \"cuda\" ) # obtain Tensor size of each rank local_size = torch . tensor ([ tensor . numel ()], device = \"cuda\" ) size_list = [ torch . tensor ([ 0 ], device = \"cuda\" ) for _ in range ( world_size )] dist . all_gather ( size_list , local_size ) size_list = [ int ( size . item ()) for size in size_list ] max_size = max ( size_list ) # receiving Tensor from all ranks # we pad the tensor because torch all_gather does not support # gathering tensors of different shapes tensor_list = [] for _ in size_list : tensor_list . append ( torch . empty (( max_size ,), dtype = torch . uint8 , device = \"cuda\" )) if local_size != max_size : padding = torch . empty ( size = ( max_size - local_size ,), dtype = torch . uint8 , device = \"cuda\" ) tensor = torch . cat (( tensor , padding ), dim = 0 ) dist . all_gather ( tensor_list , tensor ) data_list = [] for size , tensor in zip ( size_list , tensor_list ): buffer = tensor . cpu () . numpy () . tobytes ()[: size ] data_list . append ( pickle . loads ( buffer )) return data_list class CocoEvaluator ( object ): def __init__ ( self , coco_gt , iou_types ): assert isinstance ( iou_types , ( list , tuple )) coco_gt = copy . deepcopy ( coco_gt ) self . coco_gt = coco_gt self . iou_types = iou_types self . coco_eval = {} for iou_type in iou_types : self . coco_eval [ iou_type ] = COCOeval ( coco_gt , iouType = iou_type ) self . img_ids = [] self . eval_imgs = { k : [] for k in iou_types } def update ( self , predictions ): img_ids = list ( np . unique ( list ( predictions . keys ()))) self . img_ids . extend ( img_ids ) for iou_type in self . iou_types : results = self . prepare ( predictions , iou_type ) coco_dt = loadRes ( self . coco_gt , results ) if results else COCO () coco_eval = self . coco_eval [ iou_type ] coco_eval . cocoDt = coco_dt coco_eval . params . imgIds = list ( img_ids ) img_ids , eval_imgs = evaluate ( coco_eval ) if isinstance ( self . eval_imgs [ iou_type ], np . ndarray ): self . eval_imgs [ iou_type ] = self . eval_imgs [ iou_type ] . tolist () self . eval_imgs [ iou_type ] . append ( eval_imgs ) def synchronize_between_processes ( self ): for iou_type in self . iou_types : self . eval_imgs [ iou_type ] = np . concatenate ( self . eval_imgs [ iou_type ], 2 ) create_common_coco_eval ( self . coco_eval [ iou_type ], self . img_ids , self . eval_imgs [ iou_type ]) def accumulate ( self ): for coco_eval in self . coco_eval . values (): coco_eval . accumulate () def summarize ( self ): for iou_type , coco_eval in self . coco_eval . items (): print ( \"IoU metric: {}\" . format ( iou_type )) coco_eval . summarize () def prepare ( self , predictions , iou_type ): return self . prepare_for_coco_detection ( predictions ) def prepare_for_coco_detection ( self , predictions ): coco_results = [] for original_id , prediction in predictions . items (): if len ( prediction ) == 0 : continue boxes = prediction [ \"boxes\" ] boxes = convert_to_xywh ( boxes ) . tolist () scores = prediction [ \"scores\" ] . tolist () labels = prediction [ \"labels\" ] . tolist () coco_results . extend ( [ { \"image_id\" : original_id , \"category_id\" : labels [ k ], \"bbox\" : box , \"score\" : scores [ k ], } for k , box in enumerate ( boxes ) ] ) return coco_results def convert_to_xywh ( boxes ): xmin , ymin , xmax , ymax = boxes . unbind ( 1 ) return torch . stack (( xmin , ymin , xmax - xmin , ymax - ymin ), dim = 1 ) def merge ( img_ids , eval_imgs ): all_img_ids = all_gather ( img_ids ) all_eval_imgs = all_gather ( eval_imgs ) merged_img_ids = [] for p in all_img_ids : merged_img_ids . extend ( p ) merged_eval_imgs = [] for p in all_eval_imgs : merged_eval_imgs . append ( p ) merged_img_ids = np . array ( merged_img_ids ) merged_eval_imgs = np . concatenate ( merged_eval_imgs , 2 ) # keep only unique (and in sorted order) images merged_img_ids , idx = np . unique ( merged_img_ids , return_index = True ) merged_eval_imgs = merged_eval_imgs [ ... , idx ] return merged_img_ids , merged_eval_imgs def create_common_coco_eval ( coco_eval , img_ids , eval_imgs ): img_ids , eval_imgs = merge ( img_ids , eval_imgs ) img_ids = list ( img_ids ) eval_imgs = list ( eval_imgs . flatten ()) coco_eval . evalImgs = eval_imgs coco_eval . params . imgIds = img_ids coco_eval . _paramsEval = copy . deepcopy ( coco_eval . params ) ################################################################# # From pycocotools, just removed the prints and fixed # a Python3 bug about unicode not defined ################################################################# # Ideally, pycocotools wouldn't have hard-coded prints # so that we could avoid copy-pasting those two functions def createIndex ( self ): # create index # print('creating index...') anns , cats , imgs = {}, {}, {} imgToAnns , catToImgs = defaultdict ( list ), defaultdict ( list ) if \"annotations\" in self . dataset : for ann in self . dataset [ \"annotations\" ]: imgToAnns [ ann [ \"image_id\" ]] . append ( ann ) anns [ ann [ \"id\" ]] = ann if \"images\" in self . dataset : for img in self . dataset [ \"images\" ]: imgs [ img [ \"id\" ]] = img if \"categories\" in self . dataset : for cat in self . dataset [ \"categories\" ]: cats [ cat [ \"id\" ]] = cat if \"annotations\" in self . dataset and \"categories\" in self . dataset : for ann in self . dataset [ \"annotations\" ]: catToImgs [ ann [ \"category_id\" ]] . append ( ann [ \"image_id\" ]) # print('index created!') # create class members self . anns = anns self . imgToAnns = imgToAnns self . catToImgs = catToImgs self . imgs = imgs self . cats = cats maskUtils = mask_util def loadRes ( self , resFile ): \"\"\" Load result file and return a result api object. :param resFile (str) : file name of result file :return: res (obj) : result api object \"\"\" res = COCO () res . dataset [ \"images\" ] = [ img for img in self . dataset [ \"images\" ]] # print('Loading and preparing results...') # tic = time.time() if isinstance ( resFile , torch . _six . string_classes ): anns = json . load ( open ( resFile )) elif type ( resFile ) == np . ndarray : anns = self . loadNumpyAnnotations ( resFile ) else : anns = resFile assert type ( anns ) == list , \"results in not an array of objects\" annsImgIds = [ ann [ \"image_id\" ] for ann in anns ] assert set ( annsImgIds ) == ( set ( annsImgIds ) & set ( self . getImgIds ()) ), \"Results do not correspond to current coco set\" if \"caption\" in anns [ 0 ]: imgIds = set ([ img [ \"id\" ] for img in res . dataset [ \"images\" ]]) & set ([ ann [ \"image_id\" ] for ann in anns ]) res . dataset [ \"images\" ] = [ img for img in res . dataset [ \"images\" ] if img [ \"id\" ] in imgIds ] for id , ann in enumerate ( anns ): ann [ \"id\" ] = id + 1 elif \"bbox\" in anns [ 0 ] and not anns [ 0 ][ \"bbox\" ] == []: res . dataset [ \"categories\" ] = copy . deepcopy ( self . dataset [ \"categories\" ]) for id , ann in enumerate ( anns ): bb = ann [ \"bbox\" ] x1 , x2 , y1 , y2 = [ bb [ 0 ], bb [ 0 ] + bb [ 2 ], bb [ 1 ], bb [ 1 ] + bb [ 3 ]] if \"segmentation\" not in ann : ann [ \"segmentation\" ] = [[ x1 , y1 , x1 , y2 , x2 , y2 , x2 , y1 ]] ann [ \"area\" ] = bb [ 2 ] * bb [ 3 ] ann [ \"id\" ] = id + 1 ann [ \"iscrowd\" ] = 0 res . dataset [ \"annotations\" ] = anns createIndex ( res ) return res def evaluate ( self ): \"\"\" Run per image evaluation on given images and store results (a list of dict) in self.evalImgs :return: None \"\"\" # tic = time.time() # print('Running per image evaluation...') p = self . params # add backward compatibility if useSegm is specified in params if p . useSegm is not None : p . iouType = \"segm\" if p . useSegm == 1 else \"bbox\" print ( \"useSegm (deprecated) is not None. Running {} evaluation\" . format ( p . iouType )) # print('Evaluate annotation type *{}*'.format(p.iouType)) p . imgIds = list ( np . unique ( p . imgIds )) if p . useCats : p . catIds = list ( np . unique ( p . catIds )) p . maxDets = sorted ( p . maxDets ) self . params = p self . _prepare () # loop through images, area range, max detection number catIds = p . catIds if p . useCats else [ - 1 ] if p . iouType == \"segm\" or p . iouType == \"bbox\" : computeIoU = self . computeIoU elif p . iouType == \"keypoints\" : computeIoU = self . computeOks self . ious = {( imgId , catId ): computeIoU ( imgId , catId ) for imgId in p . imgIds for catId in catIds } evaluateImg = self . evaluateImg maxDet = p . maxDets [ - 1 ] evalImgs = [ evaluateImg ( imgId , catId , areaRng , maxDet ) for catId in catIds for areaRng in p . areaRng for imgId in p . imgIds ] # this is NOT in the pycocotools code, but could be done outside evalImgs = np . asarray ( evalImgs ) . reshape ( len ( catIds ), len ( p . areaRng ), len ( p . imgIds )) self . _paramsEval = copy . deepcopy ( self . params ) # toc = time.time() # print('DONE (t={:0.2f}s).'.format(toc-tic)) return p . imgIds , evalImgs ################################################################# # end of straight copy from pycocotools, just removing the prints ################################################################# Variables maskUtils Functions all_gather def all_gather ( data ) Run all_gather on arbitrary picklable data (not necessarily tensors) Args: data: any picklable object Returns: list[data]: list of data gathered from each rank View Source def all_gather ( data ) : \"\"\" Run all_gather on arbitrary picklable data (not necessarily tensors) Args: data: any picklable object Returns: list[data]: list of data gathered from each rank \"\"\" world_size = get_world_size () if world_size == 1 : return [ data ] # serialized to a Tensor buffer = pickle . dumps ( data ) storage = torch . ByteStorage . from_buffer ( buffer ) tensor = torch . ByteTensor ( storage ). to ( \"cuda\" ) # obtain Tensor size of each rank local_size = torch . tensor ( [ tensor.numel() ] , device = \"cuda\" ) size_list = [ torch.tensor([0 ] , device = \"cuda\" ) for _ in range ( world_size ) ] dist . all_gather ( size_list , local_size ) size_list = [ int(size.item()) for size in size_list ] max_size = max ( size_list ) # receiving Tensor from all ranks # we pad the tensor because torch all_gather does not support # gathering tensors of different shapes tensor_list = [] for _ in size_list : tensor_list . append ( torch . empty (( max_size ,), dtype = torch . uint8 , device = \"cuda\" )) if local_size != max_size : padding = torch . empty ( size = ( max_size - local_size ,), dtype = torch . uint8 , device = \"cuda\" ) tensor = torch . cat (( tensor , padding ), dim = 0 ) dist . all_gather ( tensor_list , tensor ) data_list = [] for size , tensor in zip ( size_list , tensor_list ) : buffer = tensor . cpu (). numpy (). tobytes () [ :size ] data_list . append ( pickle . loads ( buffer )) return data_list convert_to_xywh def convert_to_xywh ( boxes ) View Source def convert_to_xywh ( boxes ): xmin , ymin , xmax , ymax = boxes . unbind ( 1 ) return torch . stack (( xmin , ymin , xmax - xmin , ymax - ymin ), dim = 1 ) createIndex def createIndex ( self ) View Source def createIndex ( self ): # create index # print ( 'creating index...' ) anns , cats , imgs = {} , {} , {} imgToAnns , catToImgs = defaultdict ( list ), defaultdict ( list ) if \"annotations\" in self . dataset : for ann in self . dataset [ \"annotations\" ]: imgToAnns [ ann [ \"image_id\" ]]. append ( ann ) anns [ ann [ \"id\" ]] = ann if \"images\" in self . dataset : for img in self . dataset [ \"images\" ]: imgs [ img [ \"id\" ]] = img if \"categories\" in self . dataset : for cat in self . dataset [ \"categories\" ]: cats [ cat [ \"id\" ]] = cat if \"annotations\" in self . dataset and \"categories\" in self . dataset : for ann in self . dataset [ \"annotations\" ]: catToImgs [ ann [ \"category_id\" ]]. append ( ann [ \"image_id\" ]) # print ( 'index created!' ) # create class members self . anns = anns self . imgToAnns = imgToAnns self . catToImgs = catToImgs self . imgs = imgs self . cats = cats create_common_coco_eval def create_common_coco_eval ( coco_eval , img_ids , eval_imgs ) View Source def create_common_coco_eval ( coco_eval , img_ids , eval_imgs ): img_ids , eval_imgs = merge ( img_ids , eval_imgs ) img_ids = list ( img_ids ) eval_imgs = list ( eval_imgs . flatten ()) coco_eval . evalImgs = eval_imgs coco_eval . params . imgIds = img_ids coco_eval . _paramsEval = copy . deepcopy ( coco_eval . params ) evaluate def evaluate ( self ) Run per image evaluation on given images and store results (a list of dict) in self.evalImgs :return: None View Source def evaluate ( self ): \"\"\" Run per image evaluation on given images and store results (a list of dict) in self.evalImgs :return: None \"\"\" # tic = time.time() # print('Running per image evaluation...') p = self . params # add backward compatibility if useSegm is specified in params if p . useSegm is not None : p . iouType = \"segm\" if p . useSegm == 1 else \"bbox\" print ( \"useSegm (deprecated) is not None. Running {} evaluation\" . format ( p . iouType )) # print('Evaluate annotation type *{}*'.format(p.iouType)) p . imgIds = list ( np . unique ( p . imgIds )) if p . useCats : p . catIds = list ( np . unique ( p . catIds )) p . maxDets = sorted ( p . maxDets ) self . params = p self . _prepare () # loop through images, area range, max detection number catIds = p . catIds if p . useCats else [ - 1 ] if p . iouType == \"segm\" or p . iouType == \"bbox\" : computeIoU = self . computeIoU elif p . iouType == \"keypoints\" : computeIoU = self . computeOks self . ious = {( imgId , catId ): computeIoU ( imgId , catId ) for imgId in p . imgIds for catId in catIds } evaluateImg = self . evaluateImg maxDet = p . maxDets [ - 1 ] evalImgs = [ evaluateImg ( imgId , catId , areaRng , maxDet ) for catId in catIds for areaRng in p . areaRng for imgId in p . imgIds ] # this is NOT in the pycocotools code, but could be done outside evalImgs = np . asarray ( evalImgs ) . reshape ( len ( catIds ), len ( p . areaRng ), len ( p . imgIds )) self . _paramsEval = copy . deepcopy ( self . params ) # toc = time.time() # print('DONE (t={:0.2f}s).'.format(toc-tic)) return p . imgIds , evalImgs get_world_size def get_world_size ( ) View Source def get_world_size (): if not is_dist_avail_and_initialized (): return 1 return dist . get_world_size () is_dist_avail_and_initialized def is_dist_avail_and_initialized ( ) View Source def is_dist_avail_and_initialized (): if not dist . is_available (): return False if not dist . is_initialized (): return False return True loadRes def loadRes ( self , resFile ) Load result file and return a result api object. :param resFile (str) : file name of result file :return: res (obj) : result api object View Source def loadRes ( self , resFile ): \"\"\" Load result file and return a result api object. :param resFile (str) : file name of result file :return: res (obj) : result api object \"\"\" res = COCO () res . dataset [ \"images\" ] = [ img for img in self . dataset [ \"images\" ]] # print('Loading and preparing results...') # tic = time.time() if isinstance ( resFile , torch . _six . string_classes ): anns = json . load ( open ( resFile )) elif type ( resFile ) == np . ndarray : anns = self . loadNumpyAnnotations ( resFile ) else : anns = resFile assert type ( anns ) == list , \"results in not an array of objects\" annsImgIds = [ ann [ \"image_id\" ] for ann in anns ] assert set ( annsImgIds ) == ( set ( annsImgIds ) & set ( self . getImgIds ()) ), \"Results do not correspond to current coco set\" if \"caption\" in anns [ 0 ]: imgIds = set ([ img [ \"id\" ] for img in res . dataset [ \"images\" ]]) & set ([ ann [ \"image_id\" ] for ann in anns ]) res . dataset [ \"images\" ] = [ img for img in res . dataset [ \"images\" ] if img [ \"id\" ] in imgIds ] for id , ann in enumerate ( anns ): ann [ \"id\" ] = id + 1 elif \"bbox\" in anns [ 0 ] and not anns [ 0 ][ \"bbox\" ] == []: res . dataset [ \"categories\" ] = copy . deepcopy ( self . dataset [ \"categories\" ]) for id , ann in enumerate ( anns ): bb = ann [ \"bbox\" ] x1 , x2 , y1 , y2 = [ bb [ 0 ], bb [ 0 ] + bb [ 2 ], bb [ 1 ], bb [ 1 ] + bb [ 3 ]] if \"segmentation\" not in ann : ann [ \"segmentation\" ] = [[ x1 , y1 , x1 , y2 , x2 , y2 , x2 , y1 ]] ann [ \"area\" ] = bb [ 2 ] * bb [ 3 ] ann [ \"id\" ] = id + 1 ann [ \"iscrowd\" ] = 0 res . dataset [ \"annotations\" ] = anns createIndex ( res ) return res merge def merge ( img_ids , eval_imgs ) View Source def merge ( img_ids , eval_imgs ): all_img_ids = all_gather ( img_ids ) all_eval_imgs = all_gather ( eval_imgs ) merged_img_ids = [] for p in all_img_ids : merged_img_ids . extend ( p ) merged_eval_imgs = [] for p in all_eval_imgs : merged_eval_imgs . append ( p ) merged_img_ids = np . array ( merged_img_ids ) merged_eval_imgs = np . concatenate ( merged_eval_imgs , 2 ) # keep only unique ( and in sorted order ) images merged_img_ids , idx = np . unique ( merged_img_ids , return_index = True ) merged_eval_imgs = merged_eval_imgs [..., idx ] return merged_img_ids , merged_eval_imgs Classes CocoEvaluator class CocoEvaluator ( coco_gt , iou_types ) Methods accumulate def accumulate ( self ) View Source def accumulate ( self ): for coco_eval in self . coco_eval . values (): coco_eval . accumulate () prepare def prepare ( self , predictions , iou_type ) View Source def prepare ( self , predictions , iou_type ): return self . prepare_for_coco_detection ( predictions ) prepare_for_coco_detection def prepare_for_coco_detection ( self , predictions ) View Source def prepare_for_coco_detection ( self , predictions ) : coco_results = [] for original_id , prediction in predictions . items () : if len ( prediction ) == 0 : continue boxes = prediction [ \"boxes\" ] boxes = convert_to_xywh ( boxes ). tolist () scores = prediction [ \"scores\" ] . tolist () labels = prediction [ \"labels\" ] . tolist () coco_results . extend ( [ { \"image_id\": original_id, \"category_id\": labels[k ] , \"bbox\" : box , \"score\" : scores [ k ] , } for k , box in enumerate ( boxes ) ] ) return coco_results summarize def summarize ( self ) View Source def summarize ( self ): for iou_type , coco_eval in self . coco_eval . items (): print ( \"IoU metric: {}\" . format ( iou_type )) coco_eval . summarize () synchronize_between_processes def synchronize_between_processes ( self ) View Source def synchronize_between_processes ( self ) : for iou_type in self . iou_types : self . eval_imgs [ iou_type ] = np . concatenate ( self . eval_imgs [ iou_type ] , 2 ) create_common_coco_eval ( self . coco_eval [ iou_type ] , self . img_ids , self . eval_imgs [ iou_type ] ) update def update ( self , predictions ) View Source def update ( self , predictions ) : img_ids = list ( np . unique ( list ( predictions . keys ()))) self . img_ids . extend ( img_ids ) for iou_type in self . iou_types : results = self . prepare ( predictions , iou_type ) coco_dt = loadRes ( self . coco_gt , results ) if results else COCO () coco_eval = self . coco_eval [ iou_type ] coco_eval . cocoDt = coco_dt coco_eval . params . imgIds = list ( img_ids ) img_ids , eval_imgs = evaluate ( coco_eval ) if isinstance ( self . eval_imgs [ iou_type ] , np . ndarray ) : self . eval_imgs [ iou_type ] = self . eval_imgs [ iou_type ] . tolist () self . eval_imgs [ iou_type ] . append ( eval_imgs )","title":"Coco Eval"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#module-sagemaker_defect_detectionutilscoco_eval","text":"BSD 3-Clause License Copyright (c) Soumith Chintala 2016, All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. This module is a modified version of https://github.com/pytorch/vision/tree/03b1d38ba3c67703e648fb067570eeb1a1e61265/references/detection View Source \"\"\" BSD 3-Clause License Copyright (c) Soumith Chintala 2016, All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. This module is a modified version of https://github.com/pytorch/vision/tree/03b1d38ba3c67703e648fb067570eeb1a1e61265/references/detection \"\"\" import json import numpy as np import copy import torch import pickle import torch . distributed as dist from pycocotools . cocoeval import COCOeval from pycocotools . coco import COCO import pycocotools . mask as mask_util from collections import defaultdict def is_dist_avail_and_initialized (): if not dist . is_available (): return False if not dist . is_initialized (): return False return True def get_world_size (): if not is_dist_avail_and_initialized (): return 1 return dist . get_world_size () def all_gather ( data ): \"\"\" Run all_gather on arbitrary picklable data (not necessarily tensors) Args: data: any picklable object Returns: list[data]: list of data gathered from each rank \"\"\" world_size = get_world_size () if world_size == 1 : return [ data ] # serialized to a Tensor buffer = pickle . dumps ( data ) storage = torch . ByteStorage . from_buffer ( buffer ) tensor = torch . ByteTensor ( storage ) . to ( \"cuda\" ) # obtain Tensor size of each rank local_size = torch . tensor ([ tensor . numel ()], device = \"cuda\" ) size_list = [ torch . tensor ([ 0 ], device = \"cuda\" ) for _ in range ( world_size )] dist . all_gather ( size_list , local_size ) size_list = [ int ( size . item ()) for size in size_list ] max_size = max ( size_list ) # receiving Tensor from all ranks # we pad the tensor because torch all_gather does not support # gathering tensors of different shapes tensor_list = [] for _ in size_list : tensor_list . append ( torch . empty (( max_size ,), dtype = torch . uint8 , device = \"cuda\" )) if local_size != max_size : padding = torch . empty ( size = ( max_size - local_size ,), dtype = torch . uint8 , device = \"cuda\" ) tensor = torch . cat (( tensor , padding ), dim = 0 ) dist . all_gather ( tensor_list , tensor ) data_list = [] for size , tensor in zip ( size_list , tensor_list ): buffer = tensor . cpu () . numpy () . tobytes ()[: size ] data_list . append ( pickle . loads ( buffer )) return data_list class CocoEvaluator ( object ): def __init__ ( self , coco_gt , iou_types ): assert isinstance ( iou_types , ( list , tuple )) coco_gt = copy . deepcopy ( coco_gt ) self . coco_gt = coco_gt self . iou_types = iou_types self . coco_eval = {} for iou_type in iou_types : self . coco_eval [ iou_type ] = COCOeval ( coco_gt , iouType = iou_type ) self . img_ids = [] self . eval_imgs = { k : [] for k in iou_types } def update ( self , predictions ): img_ids = list ( np . unique ( list ( predictions . keys ()))) self . img_ids . extend ( img_ids ) for iou_type in self . iou_types : results = self . prepare ( predictions , iou_type ) coco_dt = loadRes ( self . coco_gt , results ) if results else COCO () coco_eval = self . coco_eval [ iou_type ] coco_eval . cocoDt = coco_dt coco_eval . params . imgIds = list ( img_ids ) img_ids , eval_imgs = evaluate ( coco_eval ) if isinstance ( self . eval_imgs [ iou_type ], np . ndarray ): self . eval_imgs [ iou_type ] = self . eval_imgs [ iou_type ] . tolist () self . eval_imgs [ iou_type ] . append ( eval_imgs ) def synchronize_between_processes ( self ): for iou_type in self . iou_types : self . eval_imgs [ iou_type ] = np . concatenate ( self . eval_imgs [ iou_type ], 2 ) create_common_coco_eval ( self . coco_eval [ iou_type ], self . img_ids , self . eval_imgs [ iou_type ]) def accumulate ( self ): for coco_eval in self . coco_eval . values (): coco_eval . accumulate () def summarize ( self ): for iou_type , coco_eval in self . coco_eval . items (): print ( \"IoU metric: {}\" . format ( iou_type )) coco_eval . summarize () def prepare ( self , predictions , iou_type ): return self . prepare_for_coco_detection ( predictions ) def prepare_for_coco_detection ( self , predictions ): coco_results = [] for original_id , prediction in predictions . items (): if len ( prediction ) == 0 : continue boxes = prediction [ \"boxes\" ] boxes = convert_to_xywh ( boxes ) . tolist () scores = prediction [ \"scores\" ] . tolist () labels = prediction [ \"labels\" ] . tolist () coco_results . extend ( [ { \"image_id\" : original_id , \"category_id\" : labels [ k ], \"bbox\" : box , \"score\" : scores [ k ], } for k , box in enumerate ( boxes ) ] ) return coco_results def convert_to_xywh ( boxes ): xmin , ymin , xmax , ymax = boxes . unbind ( 1 ) return torch . stack (( xmin , ymin , xmax - xmin , ymax - ymin ), dim = 1 ) def merge ( img_ids , eval_imgs ): all_img_ids = all_gather ( img_ids ) all_eval_imgs = all_gather ( eval_imgs ) merged_img_ids = [] for p in all_img_ids : merged_img_ids . extend ( p ) merged_eval_imgs = [] for p in all_eval_imgs : merged_eval_imgs . append ( p ) merged_img_ids = np . array ( merged_img_ids ) merged_eval_imgs = np . concatenate ( merged_eval_imgs , 2 ) # keep only unique (and in sorted order) images merged_img_ids , idx = np . unique ( merged_img_ids , return_index = True ) merged_eval_imgs = merged_eval_imgs [ ... , idx ] return merged_img_ids , merged_eval_imgs def create_common_coco_eval ( coco_eval , img_ids , eval_imgs ): img_ids , eval_imgs = merge ( img_ids , eval_imgs ) img_ids = list ( img_ids ) eval_imgs = list ( eval_imgs . flatten ()) coco_eval . evalImgs = eval_imgs coco_eval . params . imgIds = img_ids coco_eval . _paramsEval = copy . deepcopy ( coco_eval . params ) ################################################################# # From pycocotools, just removed the prints and fixed # a Python3 bug about unicode not defined ################################################################# # Ideally, pycocotools wouldn't have hard-coded prints # so that we could avoid copy-pasting those two functions def createIndex ( self ): # create index # print('creating index...') anns , cats , imgs = {}, {}, {} imgToAnns , catToImgs = defaultdict ( list ), defaultdict ( list ) if \"annotations\" in self . dataset : for ann in self . dataset [ \"annotations\" ]: imgToAnns [ ann [ \"image_id\" ]] . append ( ann ) anns [ ann [ \"id\" ]] = ann if \"images\" in self . dataset : for img in self . dataset [ \"images\" ]: imgs [ img [ \"id\" ]] = img if \"categories\" in self . dataset : for cat in self . dataset [ \"categories\" ]: cats [ cat [ \"id\" ]] = cat if \"annotations\" in self . dataset and \"categories\" in self . dataset : for ann in self . dataset [ \"annotations\" ]: catToImgs [ ann [ \"category_id\" ]] . append ( ann [ \"image_id\" ]) # print('index created!') # create class members self . anns = anns self . imgToAnns = imgToAnns self . catToImgs = catToImgs self . imgs = imgs self . cats = cats maskUtils = mask_util def loadRes ( self , resFile ): \"\"\" Load result file and return a result api object. :param resFile (str) : file name of result file :return: res (obj) : result api object \"\"\" res = COCO () res . dataset [ \"images\" ] = [ img for img in self . dataset [ \"images\" ]] # print('Loading and preparing results...') # tic = time.time() if isinstance ( resFile , torch . _six . string_classes ): anns = json . load ( open ( resFile )) elif type ( resFile ) == np . ndarray : anns = self . loadNumpyAnnotations ( resFile ) else : anns = resFile assert type ( anns ) == list , \"results in not an array of objects\" annsImgIds = [ ann [ \"image_id\" ] for ann in anns ] assert set ( annsImgIds ) == ( set ( annsImgIds ) & set ( self . getImgIds ()) ), \"Results do not correspond to current coco set\" if \"caption\" in anns [ 0 ]: imgIds = set ([ img [ \"id\" ] for img in res . dataset [ \"images\" ]]) & set ([ ann [ \"image_id\" ] for ann in anns ]) res . dataset [ \"images\" ] = [ img for img in res . dataset [ \"images\" ] if img [ \"id\" ] in imgIds ] for id , ann in enumerate ( anns ): ann [ \"id\" ] = id + 1 elif \"bbox\" in anns [ 0 ] and not anns [ 0 ][ \"bbox\" ] == []: res . dataset [ \"categories\" ] = copy . deepcopy ( self . dataset [ \"categories\" ]) for id , ann in enumerate ( anns ): bb = ann [ \"bbox\" ] x1 , x2 , y1 , y2 = [ bb [ 0 ], bb [ 0 ] + bb [ 2 ], bb [ 1 ], bb [ 1 ] + bb [ 3 ]] if \"segmentation\" not in ann : ann [ \"segmentation\" ] = [[ x1 , y1 , x1 , y2 , x2 , y2 , x2 , y1 ]] ann [ \"area\" ] = bb [ 2 ] * bb [ 3 ] ann [ \"id\" ] = id + 1 ann [ \"iscrowd\" ] = 0 res . dataset [ \"annotations\" ] = anns createIndex ( res ) return res def evaluate ( self ): \"\"\" Run per image evaluation on given images and store results (a list of dict) in self.evalImgs :return: None \"\"\" # tic = time.time() # print('Running per image evaluation...') p = self . params # add backward compatibility if useSegm is specified in params if p . useSegm is not None : p . iouType = \"segm\" if p . useSegm == 1 else \"bbox\" print ( \"useSegm (deprecated) is not None. Running {} evaluation\" . format ( p . iouType )) # print('Evaluate annotation type *{}*'.format(p.iouType)) p . imgIds = list ( np . unique ( p . imgIds )) if p . useCats : p . catIds = list ( np . unique ( p . catIds )) p . maxDets = sorted ( p . maxDets ) self . params = p self . _prepare () # loop through images, area range, max detection number catIds = p . catIds if p . useCats else [ - 1 ] if p . iouType == \"segm\" or p . iouType == \"bbox\" : computeIoU = self . computeIoU elif p . iouType == \"keypoints\" : computeIoU = self . computeOks self . ious = {( imgId , catId ): computeIoU ( imgId , catId ) for imgId in p . imgIds for catId in catIds } evaluateImg = self . evaluateImg maxDet = p . maxDets [ - 1 ] evalImgs = [ evaluateImg ( imgId , catId , areaRng , maxDet ) for catId in catIds for areaRng in p . areaRng for imgId in p . imgIds ] # this is NOT in the pycocotools code, but could be done outside evalImgs = np . asarray ( evalImgs ) . reshape ( len ( catIds ), len ( p . areaRng ), len ( p . imgIds )) self . _paramsEval = copy . deepcopy ( self . params ) # toc = time.time() # print('DONE (t={:0.2f}s).'.format(toc-tic)) return p . imgIds , evalImgs ################################################################# # end of straight copy from pycocotools, just removing the prints #################################################################","title":"Module sagemaker_defect_detection.utils.coco_eval"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#variables","text":"maskUtils","title":"Variables"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#functions","text":"","title":"Functions"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#all_gather","text":"def all_gather ( data ) Run all_gather on arbitrary picklable data (not necessarily tensors) Args: data: any picklable object Returns: list[data]: list of data gathered from each rank View Source def all_gather ( data ) : \"\"\" Run all_gather on arbitrary picklable data (not necessarily tensors) Args: data: any picklable object Returns: list[data]: list of data gathered from each rank \"\"\" world_size = get_world_size () if world_size == 1 : return [ data ] # serialized to a Tensor buffer = pickle . dumps ( data ) storage = torch . ByteStorage . from_buffer ( buffer ) tensor = torch . ByteTensor ( storage ). to ( \"cuda\" ) # obtain Tensor size of each rank local_size = torch . tensor ( [ tensor.numel() ] , device = \"cuda\" ) size_list = [ torch.tensor([0 ] , device = \"cuda\" ) for _ in range ( world_size ) ] dist . all_gather ( size_list , local_size ) size_list = [ int(size.item()) for size in size_list ] max_size = max ( size_list ) # receiving Tensor from all ranks # we pad the tensor because torch all_gather does not support # gathering tensors of different shapes tensor_list = [] for _ in size_list : tensor_list . append ( torch . empty (( max_size ,), dtype = torch . uint8 , device = \"cuda\" )) if local_size != max_size : padding = torch . empty ( size = ( max_size - local_size ,), dtype = torch . uint8 , device = \"cuda\" ) tensor = torch . cat (( tensor , padding ), dim = 0 ) dist . all_gather ( tensor_list , tensor ) data_list = [] for size , tensor in zip ( size_list , tensor_list ) : buffer = tensor . cpu (). numpy (). tobytes () [ :size ] data_list . append ( pickle . loads ( buffer )) return data_list","title":"all_gather"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#convert_to_xywh","text":"def convert_to_xywh ( boxes ) View Source def convert_to_xywh ( boxes ): xmin , ymin , xmax , ymax = boxes . unbind ( 1 ) return torch . stack (( xmin , ymin , xmax - xmin , ymax - ymin ), dim = 1 )","title":"convert_to_xywh"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#createindex","text":"def createIndex ( self ) View Source def createIndex ( self ): # create index # print ( 'creating index...' ) anns , cats , imgs = {} , {} , {} imgToAnns , catToImgs = defaultdict ( list ), defaultdict ( list ) if \"annotations\" in self . dataset : for ann in self . dataset [ \"annotations\" ]: imgToAnns [ ann [ \"image_id\" ]]. append ( ann ) anns [ ann [ \"id\" ]] = ann if \"images\" in self . dataset : for img in self . dataset [ \"images\" ]: imgs [ img [ \"id\" ]] = img if \"categories\" in self . dataset : for cat in self . dataset [ \"categories\" ]: cats [ cat [ \"id\" ]] = cat if \"annotations\" in self . dataset and \"categories\" in self . dataset : for ann in self . dataset [ \"annotations\" ]: catToImgs [ ann [ \"category_id\" ]]. append ( ann [ \"image_id\" ]) # print ( 'index created!' ) # create class members self . anns = anns self . imgToAnns = imgToAnns self . catToImgs = catToImgs self . imgs = imgs self . cats = cats","title":"createIndex"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#create_common_coco_eval","text":"def create_common_coco_eval ( coco_eval , img_ids , eval_imgs ) View Source def create_common_coco_eval ( coco_eval , img_ids , eval_imgs ): img_ids , eval_imgs = merge ( img_ids , eval_imgs ) img_ids = list ( img_ids ) eval_imgs = list ( eval_imgs . flatten ()) coco_eval . evalImgs = eval_imgs coco_eval . params . imgIds = img_ids coco_eval . _paramsEval = copy . deepcopy ( coco_eval . params )","title":"create_common_coco_eval"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#evaluate","text":"def evaluate ( self ) Run per image evaluation on given images and store results (a list of dict) in self.evalImgs :return: None View Source def evaluate ( self ): \"\"\" Run per image evaluation on given images and store results (a list of dict) in self.evalImgs :return: None \"\"\" # tic = time.time() # print('Running per image evaluation...') p = self . params # add backward compatibility if useSegm is specified in params if p . useSegm is not None : p . iouType = \"segm\" if p . useSegm == 1 else \"bbox\" print ( \"useSegm (deprecated) is not None. Running {} evaluation\" . format ( p . iouType )) # print('Evaluate annotation type *{}*'.format(p.iouType)) p . imgIds = list ( np . unique ( p . imgIds )) if p . useCats : p . catIds = list ( np . unique ( p . catIds )) p . maxDets = sorted ( p . maxDets ) self . params = p self . _prepare () # loop through images, area range, max detection number catIds = p . catIds if p . useCats else [ - 1 ] if p . iouType == \"segm\" or p . iouType == \"bbox\" : computeIoU = self . computeIoU elif p . iouType == \"keypoints\" : computeIoU = self . computeOks self . ious = {( imgId , catId ): computeIoU ( imgId , catId ) for imgId in p . imgIds for catId in catIds } evaluateImg = self . evaluateImg maxDet = p . maxDets [ - 1 ] evalImgs = [ evaluateImg ( imgId , catId , areaRng , maxDet ) for catId in catIds for areaRng in p . areaRng for imgId in p . imgIds ] # this is NOT in the pycocotools code, but could be done outside evalImgs = np . asarray ( evalImgs ) . reshape ( len ( catIds ), len ( p . areaRng ), len ( p . imgIds )) self . _paramsEval = copy . deepcopy ( self . params ) # toc = time.time() # print('DONE (t={:0.2f}s).'.format(toc-tic)) return p . imgIds , evalImgs","title":"evaluate"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#get_world_size","text":"def get_world_size ( ) View Source def get_world_size (): if not is_dist_avail_and_initialized (): return 1 return dist . get_world_size ()","title":"get_world_size"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#is_dist_avail_and_initialized","text":"def is_dist_avail_and_initialized ( ) View Source def is_dist_avail_and_initialized (): if not dist . is_available (): return False if not dist . is_initialized (): return False return True","title":"is_dist_avail_and_initialized"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#loadres","text":"def loadRes ( self , resFile ) Load result file and return a result api object. :param resFile (str) : file name of result file :return: res (obj) : result api object View Source def loadRes ( self , resFile ): \"\"\" Load result file and return a result api object. :param resFile (str) : file name of result file :return: res (obj) : result api object \"\"\" res = COCO () res . dataset [ \"images\" ] = [ img for img in self . dataset [ \"images\" ]] # print('Loading and preparing results...') # tic = time.time() if isinstance ( resFile , torch . _six . string_classes ): anns = json . load ( open ( resFile )) elif type ( resFile ) == np . ndarray : anns = self . loadNumpyAnnotations ( resFile ) else : anns = resFile assert type ( anns ) == list , \"results in not an array of objects\" annsImgIds = [ ann [ \"image_id\" ] for ann in anns ] assert set ( annsImgIds ) == ( set ( annsImgIds ) & set ( self . getImgIds ()) ), \"Results do not correspond to current coco set\" if \"caption\" in anns [ 0 ]: imgIds = set ([ img [ \"id\" ] for img in res . dataset [ \"images\" ]]) & set ([ ann [ \"image_id\" ] for ann in anns ]) res . dataset [ \"images\" ] = [ img for img in res . dataset [ \"images\" ] if img [ \"id\" ] in imgIds ] for id , ann in enumerate ( anns ): ann [ \"id\" ] = id + 1 elif \"bbox\" in anns [ 0 ] and not anns [ 0 ][ \"bbox\" ] == []: res . dataset [ \"categories\" ] = copy . deepcopy ( self . dataset [ \"categories\" ]) for id , ann in enumerate ( anns ): bb = ann [ \"bbox\" ] x1 , x2 , y1 , y2 = [ bb [ 0 ], bb [ 0 ] + bb [ 2 ], bb [ 1 ], bb [ 1 ] + bb [ 3 ]] if \"segmentation\" not in ann : ann [ \"segmentation\" ] = [[ x1 , y1 , x1 , y2 , x2 , y2 , x2 , y1 ]] ann [ \"area\" ] = bb [ 2 ] * bb [ 3 ] ann [ \"id\" ] = id + 1 ann [ \"iscrowd\" ] = 0 res . dataset [ \"annotations\" ] = anns createIndex ( res ) return res","title":"loadRes"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#merge","text":"def merge ( img_ids , eval_imgs ) View Source def merge ( img_ids , eval_imgs ): all_img_ids = all_gather ( img_ids ) all_eval_imgs = all_gather ( eval_imgs ) merged_img_ids = [] for p in all_img_ids : merged_img_ids . extend ( p ) merged_eval_imgs = [] for p in all_eval_imgs : merged_eval_imgs . append ( p ) merged_img_ids = np . array ( merged_img_ids ) merged_eval_imgs = np . concatenate ( merged_eval_imgs , 2 ) # keep only unique ( and in sorted order ) images merged_img_ids , idx = np . unique ( merged_img_ids , return_index = True ) merged_eval_imgs = merged_eval_imgs [..., idx ] return merged_img_ids , merged_eval_imgs","title":"merge"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#classes","text":"","title":"Classes"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#cocoevaluator","text":"class CocoEvaluator ( coco_gt , iou_types )","title":"CocoEvaluator"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#methods","text":"","title":"Methods"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#accumulate","text":"def accumulate ( self ) View Source def accumulate ( self ): for coco_eval in self . coco_eval . values (): coco_eval . accumulate ()","title":"accumulate"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#prepare","text":"def prepare ( self , predictions , iou_type ) View Source def prepare ( self , predictions , iou_type ): return self . prepare_for_coco_detection ( predictions )","title":"prepare"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#prepare_for_coco_detection","text":"def prepare_for_coco_detection ( self , predictions ) View Source def prepare_for_coco_detection ( self , predictions ) : coco_results = [] for original_id , prediction in predictions . items () : if len ( prediction ) == 0 : continue boxes = prediction [ \"boxes\" ] boxes = convert_to_xywh ( boxes ). tolist () scores = prediction [ \"scores\" ] . tolist () labels = prediction [ \"labels\" ] . tolist () coco_results . extend ( [ { \"image_id\": original_id, \"category_id\": labels[k ] , \"bbox\" : box , \"score\" : scores [ k ] , } for k , box in enumerate ( boxes ) ] ) return coco_results","title":"prepare_for_coco_detection"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#summarize","text":"def summarize ( self ) View Source def summarize ( self ): for iou_type , coco_eval in self . coco_eval . items (): print ( \"IoU metric: {}\" . format ( iou_type )) coco_eval . summarize ()","title":"summarize"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#synchronize_between_processes","text":"def synchronize_between_processes ( self ) View Source def synchronize_between_processes ( self ) : for iou_type in self . iou_types : self . eval_imgs [ iou_type ] = np . concatenate ( self . eval_imgs [ iou_type ] , 2 ) create_common_coco_eval ( self . coco_eval [ iou_type ] , self . img_ids , self . eval_imgs [ iou_type ] )","title":"synchronize_between_processes"},{"location":"reference/sagemaker_defect_detection/utils/coco_eval/#update","text":"def update ( self , predictions ) View Source def update ( self , predictions ) : img_ids = list ( np . unique ( list ( predictions . keys ()))) self . img_ids . extend ( img_ids ) for iou_type in self . iou_types : results = self . prepare ( predictions , iou_type ) coco_dt = loadRes ( self . coco_gt , results ) if results else COCO () coco_eval = self . coco_eval [ iou_type ] coco_eval . cocoDt = coco_dt coco_eval . params . imgIds = list ( img_ids ) img_ids , eval_imgs = evaluate ( coco_eval ) if isinstance ( self . eval_imgs [ iou_type ] , np . ndarray ) : self . eval_imgs [ iou_type ] = self . eval_imgs [ iou_type ] . tolist () self . eval_imgs [ iou_type ] . append ( eval_imgs )","title":"update"},{"location":"reference/sagemaker_defect_detection/utils/coco_utils/","text":"Module sagemaker_defect_detection.utils.coco_utils BSD 3-Clause License Copyright (c) Soumith Chintala 2016, All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. This module is a modified version of https://github.com/pytorch/vision/tree/03b1d38ba3c67703e648fb067570eeb1a1e61265/references/detection View Source \"\"\" BSD 3-Clause License Copyright (c) Soumith Chintala 2016, All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \" AS IS \" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. This module is a modified version of https://github.com/pytorch/vision/tree/03b1d38ba3c67703e648fb067570eeb1a1e61265/references/detection \"\"\" from pycocotools . coco import COCO def convert_to_coco_api ( ds ) : coco_ds = COCO () # annotation IDs need to start at 1 , not 0 , see torchvision issue #1530 ann_id = 1 dataset = { \"images\" : [] , \"categories\" : [] , \"annotations\" : []} categories = set () for img_idx in range ( len ( ds )) : # find better way to get target # targets = ds . get_annotations ( img_idx ) img , targets , _ = ds [ img_idx ] image_id = targets [ \"image_id\" ] . item () img_dict = {} img_dict [ \"id\" ] = image_id img_dict [ \"height\" ] = img . shape [ -2 ] img_dict [ \"width\" ] = img . shape [ -1 ] dataset [ \"images\" ] . append ( img_dict ) bboxes = targets [ \"boxes\" ] bboxes [ :, 2: ] -= bboxes [ :, :2 ] bboxes = bboxes . tolist () labels = targets [ \"labels\" ] . tolist () areas = targets [ \"area\" ] . tolist () iscrowd = targets [ \"iscrowd\" ] . tolist () num_objs = len ( bboxes ) for i in range ( num_objs ) : ann = {} ann [ \"image_id\" ] = image_id ann [ \"bbox\" ] = bboxes [ i ] ann [ \"category_id\" ] = labels [ i ] categories . add ( labels [ i ] ) ann [ \"area\" ] = areas [ i ] ann [ \"iscrowd\" ] = iscrowd [ i ] ann [ \"id\" ] = ann_id dataset [ \"annotations\" ] . append ( ann ) ann_id += 1 dataset [ \"categories\" ] = [ {\"id\": i} for i in sorted(categories) ] coco_ds . dataset = dataset coco_ds . createIndex () return coco_ds Functions convert_to_coco_api def convert_to_coco_api ( ds ) View Source def convert_to_coco_api ( ds ) : coco_ds = COCO () # annotation IDs need to start at 1 , not 0 , see torchvision issue #1530 ann_id = 1 dataset = { \"images\" : [] , \"categories\" : [] , \"annotations\" : []} categories = set () for img_idx in range ( len ( ds )) : # find better way to get target # targets = ds . get_annotations ( img_idx ) img , targets , _ = ds [ img_idx ] image_id = targets [ \"image_id\" ] . item () img_dict = {} img_dict [ \"id\" ] = image_id img_dict [ \"height\" ] = img . shape [ -2 ] img_dict [ \"width\" ] = img . shape [ -1 ] dataset [ \"images\" ] . append ( img_dict ) bboxes = targets [ \"boxes\" ] bboxes [ :, 2: ] -= bboxes [ :, :2 ] bboxes = bboxes . tolist () labels = targets [ \"labels\" ] . tolist () areas = targets [ \"area\" ] . tolist () iscrowd = targets [ \"iscrowd\" ] . tolist () num_objs = len ( bboxes ) for i in range ( num_objs ) : ann = {} ann [ \"image_id\" ] = image_id ann [ \"bbox\" ] = bboxes [ i ] ann [ \"category_id\" ] = labels [ i ] categories . add ( labels [ i ] ) ann [ \"area\" ] = areas [ i ] ann [ \"iscrowd\" ] = iscrowd [ i ] ann [ \"id\" ] = ann_id dataset [ \"annotations\" ] . append ( ann ) ann_id += 1 dataset [ \"categories\" ] = [ {\"id\": i} for i in sorted(categories) ] coco_ds . dataset = dataset coco_ds . createIndex () return coco_ds","title":"Coco Utils"},{"location":"reference/sagemaker_defect_detection/utils/coco_utils/#module-sagemaker_defect_detectionutilscoco_utils","text":"BSD 3-Clause License Copyright (c) Soumith Chintala 2016, All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. This module is a modified version of https://github.com/pytorch/vision/tree/03b1d38ba3c67703e648fb067570eeb1a1e61265/references/detection View Source \"\"\" BSD 3-Clause License Copyright (c) Soumith Chintala 2016, All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \" AS IS \" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. This module is a modified version of https://github.com/pytorch/vision/tree/03b1d38ba3c67703e648fb067570eeb1a1e61265/references/detection \"\"\" from pycocotools . coco import COCO def convert_to_coco_api ( ds ) : coco_ds = COCO () # annotation IDs need to start at 1 , not 0 , see torchvision issue #1530 ann_id = 1 dataset = { \"images\" : [] , \"categories\" : [] , \"annotations\" : []} categories = set () for img_idx in range ( len ( ds )) : # find better way to get target # targets = ds . get_annotations ( img_idx ) img , targets , _ = ds [ img_idx ] image_id = targets [ \"image_id\" ] . item () img_dict = {} img_dict [ \"id\" ] = image_id img_dict [ \"height\" ] = img . shape [ -2 ] img_dict [ \"width\" ] = img . shape [ -1 ] dataset [ \"images\" ] . append ( img_dict ) bboxes = targets [ \"boxes\" ] bboxes [ :, 2: ] -= bboxes [ :, :2 ] bboxes = bboxes . tolist () labels = targets [ \"labels\" ] . tolist () areas = targets [ \"area\" ] . tolist () iscrowd = targets [ \"iscrowd\" ] . tolist () num_objs = len ( bboxes ) for i in range ( num_objs ) : ann = {} ann [ \"image_id\" ] = image_id ann [ \"bbox\" ] = bboxes [ i ] ann [ \"category_id\" ] = labels [ i ] categories . add ( labels [ i ] ) ann [ \"area\" ] = areas [ i ] ann [ \"iscrowd\" ] = iscrowd [ i ] ann [ \"id\" ] = ann_id dataset [ \"annotations\" ] . append ( ann ) ann_id += 1 dataset [ \"categories\" ] = [ {\"id\": i} for i in sorted(categories) ] coco_ds . dataset = dataset coco_ds . createIndex () return coco_ds","title":"Module sagemaker_defect_detection.utils.coco_utils"},{"location":"reference/sagemaker_defect_detection/utils/coco_utils/#functions","text":"","title":"Functions"},{"location":"reference/sagemaker_defect_detection/utils/coco_utils/#convert_to_coco_api","text":"def convert_to_coco_api ( ds ) View Source def convert_to_coco_api ( ds ) : coco_ds = COCO () # annotation IDs need to start at 1 , not 0 , see torchvision issue #1530 ann_id = 1 dataset = { \"images\" : [] , \"categories\" : [] , \"annotations\" : []} categories = set () for img_idx in range ( len ( ds )) : # find better way to get target # targets = ds . get_annotations ( img_idx ) img , targets , _ = ds [ img_idx ] image_id = targets [ \"image_id\" ] . item () img_dict = {} img_dict [ \"id\" ] = image_id img_dict [ \"height\" ] = img . shape [ -2 ] img_dict [ \"width\" ] = img . shape [ -1 ] dataset [ \"images\" ] . append ( img_dict ) bboxes = targets [ \"boxes\" ] bboxes [ :, 2: ] -= bboxes [ :, :2 ] bboxes = bboxes . tolist () labels = targets [ \"labels\" ] . tolist () areas = targets [ \"area\" ] . tolist () iscrowd = targets [ \"iscrowd\" ] . tolist () num_objs = len ( bboxes ) for i in range ( num_objs ) : ann = {} ann [ \"image_id\" ] = image_id ann [ \"bbox\" ] = bboxes [ i ] ann [ \"category_id\" ] = labels [ i ] categories . add ( labels [ i ] ) ann [ \"area\" ] = areas [ i ] ann [ \"iscrowd\" ] = iscrowd [ i ] ann [ \"id\" ] = ann_id dataset [ \"annotations\" ] . append ( ann ) ann_id += 1 dataset [ \"categories\" ] = [ {\"id\": i} for i in sorted(categories) ] coco_ds . dataset = dataset coco_ds . createIndex () return coco_ds","title":"convert_to_coco_api"},{"location":"reference/sagemaker_defect_detection/utils/visualize/","text":"Module sagemaker_defect_detection.utils.visualize View Source from typing import Iterable , List , Union , Tuple import numpy as np import torch from matplotlib import pyplot as plt import cv2 import torch TEXT_COLOR = ( 255 , 255 , 255 ) # White CLASSES = { \"crazing\" : \"Cr\" , \"inclusion\" : \"In\" , \"pitted_surface\" : \"PS\" , \"patches\" : \"Pa\" , \"rolled-in_scale\" : \"RS\" , \"scratches\" : \"Sc\" , } CATEGORY_ID_TO_NAME = { i : name for i , name in enumerate ( CLASSES . keys (), start = 1 )} def unnormalize_to_hwc ( image : torch . Tensor , mean : List [ float ] = [ 0.485 , 0.456 , 0.406 ], std : List [ float ] = [ 0.229 , 0.224 , 0.225 ] ) -> np . ndarray : \"\"\" Unnormalizes and a normlized image tensor [0, 1] CHW -> HWC [0, 255] Parameters ---------- image : torch.Tensor Normalized image mean : List[float], optional RGB averages used in normalization, by default [0.485, 0.456, 0.406] from imagenet1k std : List[float], optional RGB standard deviations used in normalization, by default [0.229, 0.224, 0.225] from imagenet1k Returns ------- np.ndarray Unnormalized image as numpy array \"\"\" image = image . numpy () . transpose ( 1 , 2 , 0 ) # HWC image = ( image * std + mean ) . clip ( 0 , 1 ) image = ( image * 255 ) . astype ( np . uint8 ) return image def visualize_bbox ( img : np . ndarray , bbox : np . ndarray , class_name : str , color , thickness : int = 2 ) -> np . ndarray : \"\"\" Uses cv2 to draw colored bounding boxes and class names in an image Parameters ---------- img : np.ndarray [description] bbox : np.ndarray [description] class_name : str Class name color : tuple BGR tuple thickness : int, optional Bouding box thickness, by default 2 \"\"\" x_min , y_min , x_max , y_max = tuple ( map ( int , bbox )) cv2 . rectangle ( img , ( x_min , y_min ), ( x_max , y_max ), color = color , thickness = thickness ) (( text_width , text_height ), _ ) = cv2 . getTextSize ( class_name , cv2 . FONT_HERSHEY_SIMPLEX , 0.35 , 1 ) cv2 . rectangle ( img , ( x_min , y_min - int ( 1.3 * text_height )), ( x_min + text_width , y_min ), color , - 1 ) cv2 . putText ( img , text = class_name , org = ( x_min , y_min - int ( 0.3 * text_height )), fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 0.35 , color = TEXT_COLOR , lineType = cv2 . LINE_AA , ) return img def visualize ( image : np . ndarray , bboxes : Iterable [ Union [ torch . Tensor , np . ndarray ]] = [], category_ids : Iterable [ Union [ torch . Tensor , np . ndarray ]] = [], colors : Iterable [ Tuple [ int , int , int ]] = [], titles : Iterable [ str ] = [], category_id_to_name = CATEGORY_ID_TO_NAME , dpi = 150 , ) -> None : \"\"\" Applies the bounding boxes and category ids to an image Parameters ---------- image : np.ndarray Image as numpy array bboxes : Iterable[Union[torch.Tensor, np.ndarray]], optional Bouding boxes, by default [] category_ids : Iterable[Union[torch.Tensor, np.ndarray]], optional Category ids, by default [] colors : Iterable[Tuple[int, int, int]], optional Colors for each bounding box, by default [()] titles : Iterable[str], optional Titles for each image, by default [] category_id_to_name : Dict[str, str], optional Dictionary of category ids to names, by default CATEGORY_ID_TO_NAME dpi : int, optional DPI for clarity, by default 150 \"\"\" bboxes , category_ids , colors , titles = list ( map ( list , [ bboxes , category_ids , colors , titles ])) # type: ignore n = len ( bboxes ) assert ( n == len ( category_ids ) == len ( colors ) == len ( titles ) - 1 ), f \"number of bboxes, category ids, colors and titles (minus one) do not match\" plt . figure ( dpi = dpi ) ncols = n + 1 plt . subplot ( 1 , ncols , 1 ) img = image . copy () plt . axis ( \"off\" ) plt . title ( titles [ 0 ]) plt . imshow ( image ) if not len ( bboxes ): return titles = titles [ 1 :] for i in range ( 2 , ncols + 1 ): img = image . copy () plt . subplot ( 1 , ncols , i ) plt . axis ( \"off\" ) j = i - 2 plt . title ( titles [ j ]) for bbox , category_id in zip ( bboxes [ j ], category_ids [ j ]): # type: ignore if isinstance ( bbox , torch . Tensor ): bbox = bbox . numpy () if isinstance ( category_id , torch . Tensor ): category_id = category_id . numpy () if isinstance ( category_id , np . ndarray ): category_id = category_id . item () class_name = category_id_to_name [ category_id ] img = visualize_bbox ( img , bbox , class_name , color = colors [ j ]) plt . imshow ( img ) return Variables CATEGORY_ID_TO_NAME CLASSES TEXT_COLOR Functions unnormalize_to_hwc def unnormalize_to_hwc ( image : torch . Tensor , mean : List [ float ] = [ 0.485 , 0.456 , 0.406 ], std : List [ float ] = [ 0.229 , 0.224 , 0.225 ] ) -> numpy . ndarray Unnormalizes and a normlized image tensor [0, 1] CHW -> HWC [0, 255] Parameters image : torch.Tensor Normalized image mean : List[float], optional RGB averages used in normalization, by default [0.485, 0.456, 0.406] from imagenet1k std : List[float], optional RGB standard deviations used in normalization, by default [0.229, 0.224, 0.225] from imagenet1k Returns np.ndarray Unnormalized image as numpy array View Source def unnormalize_to_hwc ( image : torch . Tensor , mean : List [ float ] = [ 0.485, 0.456, 0.406 ] , std : List [ float ] = [ 0.229, 0.224, 0.225 ] ) -> np . ndarray : \"\"\" Unnormalizes and a normlized image tensor [0, 1] CHW -> HWC [0, 255] Parameters ---------- image : torch.Tensor Normalized image mean : List[float], optional RGB averages used in normalization, by default [0.485, 0.456, 0.406] from imagenet1k std : List[float], optional RGB standard deviations used in normalization, by default [0.229, 0.224, 0.225] from imagenet1k Returns ------- np.ndarray Unnormalized image as numpy array \"\"\" image = image . numpy (). transpose ( 1 , 2 , 0 ) # HWC image = ( image * std + mean ). clip ( 0 , 1 ) image = ( image * 255 ). astype ( np . uint8 ) return image visualize def visualize ( image : numpy . ndarray , bboxes : Iterable [ Union [ torch . Tensor , numpy . ndarray ]] = [], category_ids : Iterable [ Union [ torch . Tensor , numpy . ndarray ]] = [], colors : Iterable [ Tuple [ int , int , int ]] = [], titles : Iterable [ str ] = [], category_id_to_name = { 1 : 'crazing' , 2 : 'inclusion' , 3 : 'pitted_surface' , 4 : 'patches' , 5 : 'rolled-in_scale' , 6 : 'scratches' }, dpi = 150 ) -> None Applies the bounding boxes and category ids to an image Parameters image : np.ndarray Image as numpy array bboxes : Iterable[Union[torch.Tensor, np.ndarray]], optional Bouding boxes, by default [] category_ids : Iterable[Union[torch.Tensor, np.ndarray]], optional Category ids, by default [] colors : Iterable[Tuple[int, int, int]], optional Colors for each bounding box, by default [()] titles : Iterable[str], optional Titles for each image, by default [] category_id_to_name : Dict[str, str], optional Dictionary of category ids to names, by default CATEGORY_ID_TO_NAME dpi : int, optional DPI for clarity, by default 150 View Source def visualize ( image : np . ndarray , bboxes : Iterable [ Union[torch.Tensor, np.ndarray ] ] = [] , category_ids : Iterable [ Union[torch.Tensor, np.ndarray ] ] = [] , colors : Iterable [ Tuple[int, int, int ] ] = [] , titles : Iterable [ str ] = [] , category_id_to_name = CATEGORY_ID_TO_NAME , dpi = 150 , ) -> None : \"\"\" Applies the bounding boxes and category ids to an image Parameters ---------- image : np.ndarray Image as numpy array bboxes : Iterable[Union[torch.Tensor, np.ndarray]], optional Bouding boxes, by default [] category_ids : Iterable[Union[torch.Tensor, np.ndarray]], optional Category ids, by default [] colors : Iterable[Tuple[int, int, int]], optional Colors for each bounding box, by default [()] titles : Iterable[str], optional Titles for each image, by default [] category_id_to_name : Dict[str, str], optional Dictionary of category ids to names, by default CATEGORY_ID_TO_NAME dpi : int, optional DPI for clarity, by default 150 \"\"\" bboxes , category_ids , colors , titles = list ( map ( list , [ bboxes, category_ids, colors, titles ] )) # type : ignore n = len ( bboxes ) assert ( n == len ( category_ids ) == len ( colors ) == len ( titles ) - 1 ), f \"number of bboxes, category ids, colors and titles (minus one) do not match\" plt . figure ( dpi = dpi ) ncols = n + 1 plt . subplot ( 1 , ncols , 1 ) img = image . copy () plt . axis ( \"off\" ) plt . title ( titles [ 0 ] ) plt . imshow ( image ) if not len ( bboxes ) : return titles = titles [ 1: ] for i in range ( 2 , ncols + 1 ) : img = image . copy () plt . subplot ( 1 , ncols , i ) plt . axis ( \"off\" ) j = i - 2 plt . title ( titles [ j ] ) for bbox , category_id in zip ( bboxes [ j ] , category_ids [ j ] ) : # type : ignore if isinstance ( bbox , torch . Tensor ) : bbox = bbox . numpy () if isinstance ( category_id , torch . Tensor ) : category_id = category_id . numpy () if isinstance ( category_id , np . ndarray ) : category_id = category_id . item () class_name = category_id_to_name [ category_id ] img = visualize_bbox ( img , bbox , class_name , color = colors [ j ] ) plt . imshow ( img ) return visualize_bbox def visualize_bbox ( img : numpy . ndarray , bbox : numpy . ndarray , class_name : str , color , thickness : int = 2 ) -> numpy . ndarray Uses cv2 to draw colored bounding boxes and class names in an image Parameters img : np.ndarray [description] bbox : np.ndarray [description] class_name : str Class name color : tuple BGR tuple thickness : int, optional Bouding box thickness, by default 2 View Source def visualize_bbox ( img : np . ndarray , bbox : np . ndarray , class_name : str , color , thickness : int = 2 ) -> np . ndarray : \"\"\" Uses cv2 to draw colored bounding boxes and class names in an image Parameters ---------- img : np.ndarray [description] bbox : np.ndarray [description] class_name : str Class name color : tuple BGR tuple thickness : int, optional Bouding box thickness, by default 2 \"\"\" x_min , y_min , x_max , y_max = tuple ( map ( int , bbox )) cv2 . rectangle ( img , ( x_min , y_min ), ( x_max , y_max ), color = color , thickness = thickness ) (( text_width , text_height ), _ ) = cv2 . getTextSize ( class_name , cv2 . FONT_HERSHEY_SIMPLEX , 0.35 , 1 ) cv2 . rectangle ( img , ( x_min , y_min - int ( 1.3 * text_height )), ( x_min + text_width , y_min ), color , - 1 ) cv2 . putText ( img , text = class_name , org = ( x_min , y_min - int ( 0.3 * text_height )), fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 0.35 , color = TEXT_COLOR , lineType = cv2 . LINE_AA , ) return img","title":"Visualize"},{"location":"reference/sagemaker_defect_detection/utils/visualize/#module-sagemaker_defect_detectionutilsvisualize","text":"View Source from typing import Iterable , List , Union , Tuple import numpy as np import torch from matplotlib import pyplot as plt import cv2 import torch TEXT_COLOR = ( 255 , 255 , 255 ) # White CLASSES = { \"crazing\" : \"Cr\" , \"inclusion\" : \"In\" , \"pitted_surface\" : \"PS\" , \"patches\" : \"Pa\" , \"rolled-in_scale\" : \"RS\" , \"scratches\" : \"Sc\" , } CATEGORY_ID_TO_NAME = { i : name for i , name in enumerate ( CLASSES . keys (), start = 1 )} def unnormalize_to_hwc ( image : torch . Tensor , mean : List [ float ] = [ 0.485 , 0.456 , 0.406 ], std : List [ float ] = [ 0.229 , 0.224 , 0.225 ] ) -> np . ndarray : \"\"\" Unnormalizes and a normlized image tensor [0, 1] CHW -> HWC [0, 255] Parameters ---------- image : torch.Tensor Normalized image mean : List[float], optional RGB averages used in normalization, by default [0.485, 0.456, 0.406] from imagenet1k std : List[float], optional RGB standard deviations used in normalization, by default [0.229, 0.224, 0.225] from imagenet1k Returns ------- np.ndarray Unnormalized image as numpy array \"\"\" image = image . numpy () . transpose ( 1 , 2 , 0 ) # HWC image = ( image * std + mean ) . clip ( 0 , 1 ) image = ( image * 255 ) . astype ( np . uint8 ) return image def visualize_bbox ( img : np . ndarray , bbox : np . ndarray , class_name : str , color , thickness : int = 2 ) -> np . ndarray : \"\"\" Uses cv2 to draw colored bounding boxes and class names in an image Parameters ---------- img : np.ndarray [description] bbox : np.ndarray [description] class_name : str Class name color : tuple BGR tuple thickness : int, optional Bouding box thickness, by default 2 \"\"\" x_min , y_min , x_max , y_max = tuple ( map ( int , bbox )) cv2 . rectangle ( img , ( x_min , y_min ), ( x_max , y_max ), color = color , thickness = thickness ) (( text_width , text_height ), _ ) = cv2 . getTextSize ( class_name , cv2 . FONT_HERSHEY_SIMPLEX , 0.35 , 1 ) cv2 . rectangle ( img , ( x_min , y_min - int ( 1.3 * text_height )), ( x_min + text_width , y_min ), color , - 1 ) cv2 . putText ( img , text = class_name , org = ( x_min , y_min - int ( 0.3 * text_height )), fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 0.35 , color = TEXT_COLOR , lineType = cv2 . LINE_AA , ) return img def visualize ( image : np . ndarray , bboxes : Iterable [ Union [ torch . Tensor , np . ndarray ]] = [], category_ids : Iterable [ Union [ torch . Tensor , np . ndarray ]] = [], colors : Iterable [ Tuple [ int , int , int ]] = [], titles : Iterable [ str ] = [], category_id_to_name = CATEGORY_ID_TO_NAME , dpi = 150 , ) -> None : \"\"\" Applies the bounding boxes and category ids to an image Parameters ---------- image : np.ndarray Image as numpy array bboxes : Iterable[Union[torch.Tensor, np.ndarray]], optional Bouding boxes, by default [] category_ids : Iterable[Union[torch.Tensor, np.ndarray]], optional Category ids, by default [] colors : Iterable[Tuple[int, int, int]], optional Colors for each bounding box, by default [()] titles : Iterable[str], optional Titles for each image, by default [] category_id_to_name : Dict[str, str], optional Dictionary of category ids to names, by default CATEGORY_ID_TO_NAME dpi : int, optional DPI for clarity, by default 150 \"\"\" bboxes , category_ids , colors , titles = list ( map ( list , [ bboxes , category_ids , colors , titles ])) # type: ignore n = len ( bboxes ) assert ( n == len ( category_ids ) == len ( colors ) == len ( titles ) - 1 ), f \"number of bboxes, category ids, colors and titles (minus one) do not match\" plt . figure ( dpi = dpi ) ncols = n + 1 plt . subplot ( 1 , ncols , 1 ) img = image . copy () plt . axis ( \"off\" ) plt . title ( titles [ 0 ]) plt . imshow ( image ) if not len ( bboxes ): return titles = titles [ 1 :] for i in range ( 2 , ncols + 1 ): img = image . copy () plt . subplot ( 1 , ncols , i ) plt . axis ( \"off\" ) j = i - 2 plt . title ( titles [ j ]) for bbox , category_id in zip ( bboxes [ j ], category_ids [ j ]): # type: ignore if isinstance ( bbox , torch . Tensor ): bbox = bbox . numpy () if isinstance ( category_id , torch . Tensor ): category_id = category_id . numpy () if isinstance ( category_id , np . ndarray ): category_id = category_id . item () class_name = category_id_to_name [ category_id ] img = visualize_bbox ( img , bbox , class_name , color = colors [ j ]) plt . imshow ( img ) return","title":"Module sagemaker_defect_detection.utils.visualize"},{"location":"reference/sagemaker_defect_detection/utils/visualize/#variables","text":"CATEGORY_ID_TO_NAME CLASSES TEXT_COLOR","title":"Variables"},{"location":"reference/sagemaker_defect_detection/utils/visualize/#functions","text":"","title":"Functions"},{"location":"reference/sagemaker_defect_detection/utils/visualize/#unnormalize_to_hwc","text":"def unnormalize_to_hwc ( image : torch . Tensor , mean : List [ float ] = [ 0.485 , 0.456 , 0.406 ], std : List [ float ] = [ 0.229 , 0.224 , 0.225 ] ) -> numpy . ndarray Unnormalizes and a normlized image tensor [0, 1] CHW -> HWC [0, 255]","title":"unnormalize_to_hwc"},{"location":"reference/sagemaker_defect_detection/utils/visualize/#parameters","text":"image : torch.Tensor Normalized image mean : List[float], optional RGB averages used in normalization, by default [0.485, 0.456, 0.406] from imagenet1k std : List[float], optional RGB standard deviations used in normalization, by default [0.229, 0.224, 0.225] from imagenet1k","title":"Parameters"},{"location":"reference/sagemaker_defect_detection/utils/visualize/#returns","text":"np.ndarray Unnormalized image as numpy array View Source def unnormalize_to_hwc ( image : torch . Tensor , mean : List [ float ] = [ 0.485, 0.456, 0.406 ] , std : List [ float ] = [ 0.229, 0.224, 0.225 ] ) -> np . ndarray : \"\"\" Unnormalizes and a normlized image tensor [0, 1] CHW -> HWC [0, 255] Parameters ---------- image : torch.Tensor Normalized image mean : List[float], optional RGB averages used in normalization, by default [0.485, 0.456, 0.406] from imagenet1k std : List[float], optional RGB standard deviations used in normalization, by default [0.229, 0.224, 0.225] from imagenet1k Returns ------- np.ndarray Unnormalized image as numpy array \"\"\" image = image . numpy (). transpose ( 1 , 2 , 0 ) # HWC image = ( image * std + mean ). clip ( 0 , 1 ) image = ( image * 255 ). astype ( np . uint8 ) return image","title":"Returns"},{"location":"reference/sagemaker_defect_detection/utils/visualize/#visualize","text":"def visualize ( image : numpy . ndarray , bboxes : Iterable [ Union [ torch . Tensor , numpy . ndarray ]] = [], category_ids : Iterable [ Union [ torch . Tensor , numpy . ndarray ]] = [], colors : Iterable [ Tuple [ int , int , int ]] = [], titles : Iterable [ str ] = [], category_id_to_name = { 1 : 'crazing' , 2 : 'inclusion' , 3 : 'pitted_surface' , 4 : 'patches' , 5 : 'rolled-in_scale' , 6 : 'scratches' }, dpi = 150 ) -> None Applies the bounding boxes and category ids to an image","title":"visualize"},{"location":"reference/sagemaker_defect_detection/utils/visualize/#parameters_1","text":"image : np.ndarray Image as numpy array bboxes : Iterable[Union[torch.Tensor, np.ndarray]], optional Bouding boxes, by default [] category_ids : Iterable[Union[torch.Tensor, np.ndarray]], optional Category ids, by default [] colors : Iterable[Tuple[int, int, int]], optional Colors for each bounding box, by default [()] titles : Iterable[str], optional Titles for each image, by default [] category_id_to_name : Dict[str, str], optional Dictionary of category ids to names, by default CATEGORY_ID_TO_NAME dpi : int, optional DPI for clarity, by default 150 View Source def visualize ( image : np . ndarray , bboxes : Iterable [ Union[torch.Tensor, np.ndarray ] ] = [] , category_ids : Iterable [ Union[torch.Tensor, np.ndarray ] ] = [] , colors : Iterable [ Tuple[int, int, int ] ] = [] , titles : Iterable [ str ] = [] , category_id_to_name = CATEGORY_ID_TO_NAME , dpi = 150 , ) -> None : \"\"\" Applies the bounding boxes and category ids to an image Parameters ---------- image : np.ndarray Image as numpy array bboxes : Iterable[Union[torch.Tensor, np.ndarray]], optional Bouding boxes, by default [] category_ids : Iterable[Union[torch.Tensor, np.ndarray]], optional Category ids, by default [] colors : Iterable[Tuple[int, int, int]], optional Colors for each bounding box, by default [()] titles : Iterable[str], optional Titles for each image, by default [] category_id_to_name : Dict[str, str], optional Dictionary of category ids to names, by default CATEGORY_ID_TO_NAME dpi : int, optional DPI for clarity, by default 150 \"\"\" bboxes , category_ids , colors , titles = list ( map ( list , [ bboxes, category_ids, colors, titles ] )) # type : ignore n = len ( bboxes ) assert ( n == len ( category_ids ) == len ( colors ) == len ( titles ) - 1 ), f \"number of bboxes, category ids, colors and titles (minus one) do not match\" plt . figure ( dpi = dpi ) ncols = n + 1 plt . subplot ( 1 , ncols , 1 ) img = image . copy () plt . axis ( \"off\" ) plt . title ( titles [ 0 ] ) plt . imshow ( image ) if not len ( bboxes ) : return titles = titles [ 1: ] for i in range ( 2 , ncols + 1 ) : img = image . copy () plt . subplot ( 1 , ncols , i ) plt . axis ( \"off\" ) j = i - 2 plt . title ( titles [ j ] ) for bbox , category_id in zip ( bboxes [ j ] , category_ids [ j ] ) : # type : ignore if isinstance ( bbox , torch . Tensor ) : bbox = bbox . numpy () if isinstance ( category_id , torch . Tensor ) : category_id = category_id . numpy () if isinstance ( category_id , np . ndarray ) : category_id = category_id . item () class_name = category_id_to_name [ category_id ] img = visualize_bbox ( img , bbox , class_name , color = colors [ j ] ) plt . imshow ( img ) return","title":"Parameters"},{"location":"reference/sagemaker_defect_detection/utils/visualize/#visualize_bbox","text":"def visualize_bbox ( img : numpy . ndarray , bbox : numpy . ndarray , class_name : str , color , thickness : int = 2 ) -> numpy . ndarray Uses cv2 to draw colored bounding boxes and class names in an image","title":"visualize_bbox"},{"location":"reference/sagemaker_defect_detection/utils/visualize/#parameters_2","text":"img : np.ndarray [description] bbox : np.ndarray [description] class_name : str Class name color : tuple BGR tuple thickness : int, optional Bouding box thickness, by default 2 View Source def visualize_bbox ( img : np . ndarray , bbox : np . ndarray , class_name : str , color , thickness : int = 2 ) -> np . ndarray : \"\"\" Uses cv2 to draw colored bounding boxes and class names in an image Parameters ---------- img : np.ndarray [description] bbox : np.ndarray [description] class_name : str Class name color : tuple BGR tuple thickness : int, optional Bouding box thickness, by default 2 \"\"\" x_min , y_min , x_max , y_max = tuple ( map ( int , bbox )) cv2 . rectangle ( img , ( x_min , y_min ), ( x_max , y_max ), color = color , thickness = thickness ) (( text_width , text_height ), _ ) = cv2 . getTextSize ( class_name , cv2 . FONT_HERSHEY_SIMPLEX , 0.35 , 1 ) cv2 . rectangle ( img , ( x_min , y_min - int ( 1.3 * text_height )), ( x_min + text_width , y_min ), color , - 1 ) cv2 . putText ( img , text = class_name , org = ( x_min , y_min - int ( 0.3 * text_height )), fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 0.35 , color = TEXT_COLOR , lineType = cv2 . LINE_AA , ) return img","title":"Parameters"}]}