



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="shortcut icon" href="../../../docs/sagemaker.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-5.5.14">
    
    
      
        <title>Detector - sagemaker-defect-detection</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.d3202873.min.css">
      
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.ff0a5ce4.min.css">
      
      
        
        
        <meta name="theme-color" content="">
      
    
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
    
    
  </head>
  
  
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="dark-blue" data-md-color-accent="">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#module-sagemaker_defect_detectiondetector" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href="../../.." title="sagemaker-defect-detection" class="md-header-nav__button md-logo" aria-label="sagemaker-defect-detection">
      
  <img src="../../../docs/sagemaker.png" alt="logo">

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            sagemaker-defect-detection
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              Detector
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/awslabs/sagemaker-defect-detection/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    sagemaker-defect-detection
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
        
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="sagemaker-defect-detection" class="md-nav__button md-logo" aria-label="sagemaker-defect-detection">
      
  <img src="../../../docs/sagemaker.png" alt="logo">

    </a>
    sagemaker-defect-detection
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/awslabs/sagemaker-defect-detection/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    sagemaker-defect-detection
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../../.." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../CODE_OF_CONDUCT/" title="Code Of Conduct" class="md-nav__link">
      Code Of Conduct
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../CONTRIBUTING/" title="Contributing" class="md-nav__link">
      Contributing
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
    
    <label class="md-nav__link" for="nav-4">
      Reference
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Reference" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        <span class="md-nav__icon md-icon"></span>
        Reference
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../neu/" title="Neu" class="md-nav__link">
      Neu
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4-2" type="checkbox" id="nav-4-2" checked>
    
    <label class="md-nav__link" for="nav-4-2">
      Sagemaker Defect Detection
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Sagemaker Defect Detection" data-md-level="2">
      <label class="md-nav__title" for="nav-4-2">
        <span class="md-nav__icon md-icon"></span>
        Sagemaker Defect Detection
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../classifier/" title="Classifier" class="md-nav__link">
      Classifier
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Detector
        <span class="md-nav__icon md-icon"></span>
      </label>
    
    <a href="./" title="Detector" class="md-nav__link md-nav__link--active">
      Detector
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#functions" class="md-nav__link">
    Functions
  </a>
  
    <nav class="md-nav" aria-label="Functions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#get_args" class="md-nav__link">
    get_args
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#main" class="md-nav__link">
    main
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model_fn" class="md-nav__link">
    model_fn
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ddndetection" class="md-nav__link">
    DDNDetection
  </a>
  
    <nav class="md-nav" aria-label="DDNDetection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#static-methods" class="md-nav__link">
    Static methods
  </a>
  
    <nav class="md-nav" aria-label="Static methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#add_model_specific_args" class="md-nav__link">
    add_model_specific_args
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#load_from_checkpoint" class="md-nav__link">
    load_from_checkpoint
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#load_from_metrics" class="md-nav__link">
    load_from_metrics
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav" aria-label="Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#add_module" class="md-nav__link">
    add_module
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#amp_scale_loss" class="md-nav__link">
    amp_scale_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apply" class="md-nav__link">
    apply
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward" class="md-nav__link">
    backward
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bfloat16" class="md-nav__link">
    bfloat16
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#buffers" class="md-nav__link">
    buffers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#children" class="md-nav__link">
    children
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configure_apex" class="md-nav__link">
    configure_apex
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configure_ddp" class="md-nav__link">
    configure_ddp
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configure_optimizers" class="md-nav__link">
    configure_optimizers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cpu" class="md-nav__link">
    cpu
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cuda" class="md-nav__link">
    cuda
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#double" class="md-nav__link">
    double
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#eval" class="md-nav__link">
    eval
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#extra_repr" class="md-nav__link">
    extra_repr
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#float" class="md-nav__link">
    float
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#forward" class="md-nav__link">
    forward
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#freeze" class="md-nav__link">
    freeze
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_progress_bar_dict" class="md-nav__link">
    get_progress_bar_dict
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_tqdm_dict" class="md-nav__link">
    get_tqdm_dict
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grad_norm" class="md-nav__link">
    grad_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#half" class="md-nav__link">
    half
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#init_ddp_connection" class="md-nav__link">
    init_ddp_connection
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#load_state_dict" class="md-nav__link">
    load_state_dict
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modules" class="md-nav__link">
    modules
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#named_buffers" class="md-nav__link">
    named_buffers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#named_children" class="md-nav__link">
    named_children
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#named_modules" class="md-nav__link">
    named_modules
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#named_parameters" class="md-nav__link">
    named_parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_after_backward" class="md-nav__link">
    on_after_backward
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_batch_end" class="md-nav__link">
    on_batch_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_batch_start" class="md-nav__link">
    on_batch_start
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_before_zero_grad" class="md-nav__link">
    on_before_zero_grad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_epoch_end" class="md-nav__link">
    on_epoch_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_epoch_start" class="md-nav__link">
    on_epoch_start
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_fit_end" class="md-nav__link">
    on_fit_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_fit_start" class="md-nav__link">
    on_fit_start
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_hpc_load" class="md-nav__link">
    on_hpc_load
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_hpc_save" class="md-nav__link">
    on_hpc_save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_load_checkpoint" class="md-nav__link">
    on_load_checkpoint
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_post_performance_check" class="md-nav__link">
    on_post_performance_check
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_pre_performance_check" class="md-nav__link">
    on_pre_performance_check
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_sanity_check_start" class="md-nav__link">
    on_sanity_check_start
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_save_checkpoint" class="md-nav__link">
    on_save_checkpoint
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_end" class="md-nav__link">
    on_train_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_start" class="md-nav__link">
    on_train_start
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizer_step" class="md-nav__link">
    optimizer_step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizer_zero_grad" class="md-nav__link">
    optimizer_zero_grad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parameters" class="md-nav__link">
    parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prepare_data" class="md-nav__link">
    prepare_data
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print" class="md-nav__link">
    print
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_backward_hook" class="md-nav__link">
    register_backward_hook
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_buffer" class="md-nav__link">
    register_buffer
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_forward_hook" class="md-nav__link">
    register_forward_hook
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_forward_pre_hook" class="md-nav__link">
    register_forward_pre_hook
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_parameter" class="md-nav__link">
    register_parameter
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#requires_grad_" class="md-nav__link">
    requires_grad_
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save_hyperparameters" class="md-nav__link">
    save_hyperparameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup" class="md-nav__link">
    setup
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#share_memory" class="md-nav__link">
    share_memory
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#state_dict" class="md-nav__link">
    state_dict
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summarize" class="md-nav__link">
    summarize
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tbptt_split_batch" class="md-nav__link">
    tbptt_split_batch
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#teardown" class="md-nav__link">
    teardown
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_dataloader" class="md-nav__link">
    test_dataloader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_end" class="md-nav__link">
    test_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_epoch_end" class="md-nav__link">
    test_epoch_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_step" class="md-nav__link">
    test_step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_step_end" class="md-nav__link">
    test_step_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tng_dataloader" class="md-nav__link">
    tng_dataloader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#to" class="md-nav__link">
    to
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train" class="md-nav__link">
    train
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_dataloader" class="md-nav__link">
    train_dataloader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training_end" class="md-nav__link">
    training_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training_epoch_end" class="md-nav__link">
    training_epoch_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training_step" class="md-nav__link">
    training_step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training_step_end" class="md-nav__link">
    training_step_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transfer_batch_to_device" class="md-nav__link">
    transfer_batch_to_device
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#type" class="md-nav__link">
    type
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#unfreeze" class="md-nav__link">
    unfreeze
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#val_dataloader" class="md-nav__link">
    val_dataloader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#validation_end" class="md-nav__link">
    validation_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#validation_epoch_end" class="md-nav__link">
    validation_epoch_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#validation_step" class="md-nav__link">
    validation_step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#validation_step_end" class="md-nav__link">
    validation_step_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero_grad" class="md-nav__link">
    zero_grad
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../" title="Index" class="md-nav__link">
      Index
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../transforms/" title="Transforms" class="md-nav__link">
      Transforms
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4-2-5" type="checkbox" id="nav-4-2-5">
    
    <label class="md-nav__link" for="nav-4-2-5">
      Dataset
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Dataset" data-md-level="3">
      <label class="md-nav__title" for="nav-4-2-5">
        <span class="md-nav__icon md-icon"></span>
        Dataset
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../dataset/" title="Index" class="md-nav__link">
      Index
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../dataset/neu/" title="Neu" class="md-nav__link">
      Neu
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4-2-6" type="checkbox" id="nav-4-2-6">
    
    <label class="md-nav__link" for="nav-4-2-6">
      Models
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Models" data-md-level="3">
      <label class="md-nav__title" for="nav-4-2-6">
        <span class="md-nav__icon md-icon"></span>
        Models
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../models/ddn/" title="Ddn" class="md-nav__link">
      Ddn
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../models/" title="Index" class="md-nav__link">
      Index
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4-2-7" type="checkbox" id="nav-4-2-7">
    
    <label class="md-nav__link" for="nav-4-2-7">
      Utils
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Utils" data-md-level="3">
      <label class="md-nav__title" for="nav-4-2-7">
        <span class="md-nav__icon md-icon"></span>
        Utils
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../utils/coco_eval/" title="Coco Eval" class="md-nav__link">
      Coco Eval
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../utils/coco_utils/" title="Coco Utils" class="md-nav__link">
      Coco Utils
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../utils/" title="Index" class="md-nav__link">
      Index
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../utils/visualize/" title="Visualize" class="md-nav__link">
      Visualize
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#functions" class="md-nav__link">
    Functions
  </a>
  
    <nav class="md-nav" aria-label="Functions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#get_args" class="md-nav__link">
    get_args
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#main" class="md-nav__link">
    main
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model_fn" class="md-nav__link">
    model_fn
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ddndetection" class="md-nav__link">
    DDNDetection
  </a>
  
    <nav class="md-nav" aria-label="DDNDetection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#static-methods" class="md-nav__link">
    Static methods
  </a>
  
    <nav class="md-nav" aria-label="Static methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#add_model_specific_args" class="md-nav__link">
    add_model_specific_args
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#load_from_checkpoint" class="md-nav__link">
    load_from_checkpoint
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#load_from_metrics" class="md-nav__link">
    load_from_metrics
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav" aria-label="Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#add_module" class="md-nav__link">
    add_module
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#amp_scale_loss" class="md-nav__link">
    amp_scale_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apply" class="md-nav__link">
    apply
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward" class="md-nav__link">
    backward
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bfloat16" class="md-nav__link">
    bfloat16
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#buffers" class="md-nav__link">
    buffers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#children" class="md-nav__link">
    children
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configure_apex" class="md-nav__link">
    configure_apex
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configure_ddp" class="md-nav__link">
    configure_ddp
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configure_optimizers" class="md-nav__link">
    configure_optimizers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cpu" class="md-nav__link">
    cpu
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cuda" class="md-nav__link">
    cuda
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#double" class="md-nav__link">
    double
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#eval" class="md-nav__link">
    eval
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#extra_repr" class="md-nav__link">
    extra_repr
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#float" class="md-nav__link">
    float
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#forward" class="md-nav__link">
    forward
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#freeze" class="md-nav__link">
    freeze
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_progress_bar_dict" class="md-nav__link">
    get_progress_bar_dict
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_tqdm_dict" class="md-nav__link">
    get_tqdm_dict
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grad_norm" class="md-nav__link">
    grad_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#half" class="md-nav__link">
    half
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#init_ddp_connection" class="md-nav__link">
    init_ddp_connection
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#load_state_dict" class="md-nav__link">
    load_state_dict
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modules" class="md-nav__link">
    modules
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#named_buffers" class="md-nav__link">
    named_buffers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#named_children" class="md-nav__link">
    named_children
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#named_modules" class="md-nav__link">
    named_modules
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#named_parameters" class="md-nav__link">
    named_parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_after_backward" class="md-nav__link">
    on_after_backward
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_batch_end" class="md-nav__link">
    on_batch_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_batch_start" class="md-nav__link">
    on_batch_start
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_before_zero_grad" class="md-nav__link">
    on_before_zero_grad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_epoch_end" class="md-nav__link">
    on_epoch_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_epoch_start" class="md-nav__link">
    on_epoch_start
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_fit_end" class="md-nav__link">
    on_fit_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_fit_start" class="md-nav__link">
    on_fit_start
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_hpc_load" class="md-nav__link">
    on_hpc_load
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_hpc_save" class="md-nav__link">
    on_hpc_save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_load_checkpoint" class="md-nav__link">
    on_load_checkpoint
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_post_performance_check" class="md-nav__link">
    on_post_performance_check
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_pre_performance_check" class="md-nav__link">
    on_pre_performance_check
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_sanity_check_start" class="md-nav__link">
    on_sanity_check_start
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_save_checkpoint" class="md-nav__link">
    on_save_checkpoint
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_end" class="md-nav__link">
    on_train_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_start" class="md-nav__link">
    on_train_start
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizer_step" class="md-nav__link">
    optimizer_step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizer_zero_grad" class="md-nav__link">
    optimizer_zero_grad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parameters" class="md-nav__link">
    parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prepare_data" class="md-nav__link">
    prepare_data
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print" class="md-nav__link">
    print
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_backward_hook" class="md-nav__link">
    register_backward_hook
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_buffer" class="md-nav__link">
    register_buffer
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_forward_hook" class="md-nav__link">
    register_forward_hook
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_forward_pre_hook" class="md-nav__link">
    register_forward_pre_hook
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register_parameter" class="md-nav__link">
    register_parameter
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#requires_grad_" class="md-nav__link">
    requires_grad_
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save_hyperparameters" class="md-nav__link">
    save_hyperparameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup" class="md-nav__link">
    setup
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#share_memory" class="md-nav__link">
    share_memory
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#state_dict" class="md-nav__link">
    state_dict
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summarize" class="md-nav__link">
    summarize
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tbptt_split_batch" class="md-nav__link">
    tbptt_split_batch
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#teardown" class="md-nav__link">
    teardown
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_dataloader" class="md-nav__link">
    test_dataloader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_end" class="md-nav__link">
    test_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_epoch_end" class="md-nav__link">
    test_epoch_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_step" class="md-nav__link">
    test_step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test_step_end" class="md-nav__link">
    test_step_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tng_dataloader" class="md-nav__link">
    tng_dataloader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#to" class="md-nav__link">
    to
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train" class="md-nav__link">
    train
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_dataloader" class="md-nav__link">
    train_dataloader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training_end" class="md-nav__link">
    training_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training_epoch_end" class="md-nav__link">
    training_epoch_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training_step" class="md-nav__link">
    training_step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training_step_end" class="md-nav__link">
    training_step_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transfer_batch_to_device" class="md-nav__link">
    transfer_batch_to_device
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#type" class="md-nav__link">
    type
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#unfreeze" class="md-nav__link">
    unfreeze
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#val_dataloader" class="md-nav__link">
    val_dataloader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#validation_end" class="md-nav__link">
    validation_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#validation_epoch_end" class="md-nav__link">
    validation_epoch_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#validation_step" class="md-nav__link">
    validation_step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#validation_step_end" class="md-nav__link">
    validation_step_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero_grad" class="md-nav__link">
    zero_grad
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/awslabs/sagemaker-defect-detection/edit/master/reference/sagemaker_defect_detection/detector.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                  
                
                
                <h1 id="module-sagemaker_defect_detectiondetector">Module sagemaker_defect_detection.detector</h1>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code><span class="c1"># mypy: ignore-errors</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">from</span> <span class="nn">os</span> <span class="kn">import</span> <span class="n">path</span> <span class="k">as</span> <span class="n">osp</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>

<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">ArgumentParser</span><span class="p">,</span> <span class="n">Namespace</span>

<span class="kn">from</span> <span class="nn">multiprocessing</span> <span class="kn">import</span> <span class="n">cpu_count</span>

<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">math</span>

<span class="kn">import</span> <span class="nn">sys</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">lr_scheduler</span>

<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">from</span> <span class="nn">torchvision.models.detection.image_list</span> <span class="kn">import</span> <span class="n">ImageList</span>

<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>

<span class="kn">from</span> <span class="nn">pytorch_lightning.core.decorators</span> <span class="kn">import</span> <span class="n">auto_move_data</span>

<span class="kn">from</span> <span class="nn">pytorch_lightning.callbacks</span> <span class="kn">import</span> <span class="n">EarlyStopping</span><span class="p">,</span> <span class="n">ModelCheckpoint</span>

<span class="kn">from</span> <span class="nn">sagemaker_defect_detection</span> <span class="kn">import</span> <span class="n">Detection</span><span class="p">,</span> <span class="n">NEUDET</span><span class="p">,</span> <span class="n">Classification</span><span class="p">,</span> <span class="n">RPN</span><span class="p">,</span> <span class="n">RoI</span><span class="p">,</span> <span class="n">get_augmentation</span><span class="p">,</span> <span class="n">get_preprocess</span>

<span class="kn">from</span> <span class="nn">sagemaker_defect_detection.utils.coco_eval</span> <span class="kn">import</span> <span class="n">CocoEvaluator</span>

<span class="kn">from</span> <span class="nn">sagemaker_defect_detection.utils.coco_utils</span> <span class="kn">import</span> <span class="n">convert_to_coco_api</span>

<span class="kn">from</span> <span class="nn">sagemaker_defect_detection.utils</span> <span class="kn">import</span> <span class="n">freeze</span><span class="p">,</span> <span class="n">load_checkpoint</span>

<span class="k">class</span> <span class="nc">DDNDetection</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>

        <span class="bp">self</span><span class="p">,</span>

        <span class="n">train_rpn</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>

        <span class="n">train_roi</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>

        <span class="n">finetune_rpn</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>

        <span class="n">finetune_roi</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>

        <span class="n">data_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>

        <span class="n">backbone</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>

        <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>

        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>

        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>

        <span class="n">momentum</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>

        <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>

        <span class="kp">seed</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>

        <span class="n">pretrained_mfn_ckpt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>

        <span class="n">pretrained_rpn_ckpt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>

        <span class="n">pretrained_roi_ckpt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>

        <span class="n">finetuned_rpn_ckpt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>

        <span class="n">finetuned_roi_ckpt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>

        <span class="n">resume_sagemaker_from_checkpoint</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>

        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>

    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">train_rpn</span> <span class="o">=</span> <span class="n">train_rpn</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">train_roi</span> <span class="o">=</span> <span class="n">train_roi</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">finetune_rpn</span> <span class="o">=</span> <span class="n">finetune_rpn</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">finetune_roi</span> <span class="o">=</span> <span class="n">finetune_roi</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">data_path</span> <span class="o">=</span> <span class="n">data_path</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">backbone</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="n">weight_decay</span>

        <span class="bp">self</span><span class="o">.</span><span class="kp">seed</span> <span class="o">=</span> <span class="kp">seed</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">NEUDET</span><span class="p">(</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span>

            <span class="kp">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span>

            <span class="n">augmentation</span><span class="o">=</span><span class="n">get_augmentation</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">),</span>

            <span class="n">preprocess</span><span class="o">=</span><span class="n">get_preprocess</span><span class="p">(),</span>

            <span class="kp">seed</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="kp">seed</span><span class="p">,</span>

        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">val_dataset</span> <span class="o">=</span> <span class="n">NEUDET</span><span class="p">(</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span>

            <span class="kp">split</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">,</span>

            <span class="n">augmentation</span><span class="o">=</span><span class="n">get_augmentation</span><span class="p">(</span><span class="s2">&quot;val&quot;</span><span class="p">),</span>

            <span class="n">preprocess</span><span class="o">=</span><span class="n">get_preprocess</span><span class="p">(),</span>

            <span class="kp">seed</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="kp">seed</span><span class="p">,</span>

        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_mfn_ckpt</span> <span class="o">=</span> <span class="n">pretrained_mfn_ckpt</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_rpn_ckpt</span> <span class="o">=</span> <span class="n">pretrained_rpn_ckpt</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_roi_ckpt</span> <span class="o">=</span> <span class="n">pretrained_roi_ckpt</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_rpn_ckpt</span> <span class="o">=</span> <span class="n">finetuned_rpn_ckpt</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_roi_ckpt</span> <span class="o">=</span> <span class="n">finetuned_roi_ckpt</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">resume_sagemaker_from_checkpoint</span> <span class="o">=</span> <span class="n">resume_sagemaker_from_checkpoint</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">coco_evaluator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_evaluator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">val_dataset</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_rpn</span><span class="p">:</span>  <span class="c1"># step 2</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span>

                <span class="n">Classification</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_mfn_ckpt</span><span class="p">,</span> <span class="s2">&quot;model.mfn&quot;</span>

            <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span> <span class="o">=</span> <span class="n">RPN</span><span class="p">()</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_roi</span><span class="p">:</span>  <span class="c1"># step 3</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span>

                <span class="n">Classification</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;mfn&quot;</span>

            <span class="p">)</span>

            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RPN</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;rpn&quot;</span><span class="p">)</span>

            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">roi</span> <span class="o">=</span> <span class="n">RoI</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetune_rpn</span><span class="p">:</span>  <span class="c1"># step 4 or extra finetune rpn</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_rpn_ckpt</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_roi_ckpt</span><span class="p">:</span>  <span class="c1"># extra finetune rpn</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span>

                    <span class="n">Classification</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;mfn&quot;</span>

                <span class="p">)</span>

                <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RPN</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;rpn&quot;</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">roi</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RoI</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_roi_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;roi&quot;</span><span class="p">)</span>

                <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">roi</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Detection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">roi</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span>

                    <span class="n">Classification</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;mfn&quot;</span>

                <span class="p">)</span>

                <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RPN</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;rpn&quot;</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">roi</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RoI</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_roi_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;roi&quot;</span><span class="p">)</span>

                <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">roi</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Detection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">roi</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetune_roi</span><span class="p">:</span>  <span class="c1"># step 5 or extra finetune roi</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_rpn_ckpt</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_roi_ckpt</span><span class="p">:</span>  <span class="c1"># extra finetune roi</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span>

                    <span class="n">Classification</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;mfn&quot;</span>

                <span class="p">)</span>

                <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RPN</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;rpn&quot;</span><span class="p">)</span>

                <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">roi</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RoI</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_roi_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;roi&quot;</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Detection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">roi</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span>

                    <span class="n">Classification</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;mfn&quot;</span>

                <span class="p">)</span>

                <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RPN</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;rpn&quot;</span><span class="p">)</span>

                <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">roi</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RoI</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_roi_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;roi&quot;</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Detection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">roi</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>  <span class="c1"># step 6: final/joint model</span>

            <span class="n">load_checkpoint_fn</span> <span class="o">=</span> <span class="n">load_checkpoint</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_roi_ckpt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

                <span class="n">ckpt_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_rpn_ckpt</span>

            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">resume_sagemaker_from_checkpoint</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

                <span class="n">ckpt_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resume_sagemaker_from_checkpoint</span>

            <span class="k">else</span><span class="p">:</span>

                <span class="n">ckpt_path</span> <span class="o">=</span> <span class="kc">None</span>

                <span class="c1"># ignore load_checkpoint</span>

                <span class="n">load_checkpoint_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span> <span class="o">=</span> <span class="n">load_checkpoint_fn</span><span class="p">(</span><span class="n">Classification</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="n">ckpt_path</span><span class="p">,</span> <span class="s2">&quot;mfn&quot;</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span> <span class="o">=</span> <span class="n">load_checkpoint_fn</span><span class="p">(</span><span class="n">RPN</span><span class="p">(),</span> <span class="n">ckpt_path</span><span class="p">,</span> <span class="s2">&quot;rpn&quot;</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">roi</span> <span class="o">=</span> <span class="n">load_checkpoint_fn</span><span class="p">(</span><span class="n">RoI</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">),</span> <span class="n">ckpt_path</span><span class="p">,</span> <span class="s2">&quot;roi&quot;</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Detection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">roi</span><span class="p">)</span>

        <span class="k">return</span>

    <span class="nd">@auto_move_data</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_rpn</span><span class="p">:</span>  <span class="c1"># step 2</span>

            <span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

            <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

            <span class="n">features</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">({</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">):</span> <span class="n">t</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">features</span><span class="p">)})</span>

            <span class="n">images</span> <span class="o">=</span> <span class="n">ImageList</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="p">[(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)])</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;targets&quot;</span><span class="p">))</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_roi</span><span class="p">:</span>  <span class="c1"># step 3</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

            <span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

            <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

            <span class="n">features</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">({</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">):</span> <span class="n">t</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">features</span><span class="p">)})</span>

            <span class="n">images</span> <span class="o">=</span> <span class="n">ImageList</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="p">[(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)])</span>

            <span class="n">proposals</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">roi</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">proposals</span><span class="p">,</span> <span class="p">[(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)],</span> <span class="n">targets</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;targets&quot;</span><span class="p">))</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetune_rpn</span><span class="p">:</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">roi_heads</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;targets&quot;</span><span class="p">))</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetune_roi</span><span class="p">:</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">rpn</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;targets&quot;</span><span class="p">))</span>

        <span class="k">else</span><span class="p">:</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;targets&quot;</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_get_evaluator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">):</span>

        <span class="n">coco</span> <span class="o">=</span> <span class="n">convert_to_coco_api</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">CocoEvaluator</span><span class="p">(</span><span class="n">coco</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;bbox&quot;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>

            <span class="n">dataset</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">,</span>

            <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>

            <span class="n">collate_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">,</span>

            <span class="kp">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>

            <span class="n">num_workers</span><span class="o">=</span><span class="n">cpu_count</span><span class="p">(),</span>

        <span class="p">)</span>

        <span class="k">return</span> <span class="n">train_loader</span>

    <span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="n">val_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">val_dataset</span><span class="p">,</span>

            <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>

            <span class="n">collate_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">val_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">,</span>

            <span class="kp">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>

            <span class="n">num_workers</span><span class="o">=</span><span class="n">cpu_count</span><span class="p">()</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span>

        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">coco_evaluator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_evaluator</span><span class="p">(</span><span class="n">val_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">val_loader</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">]</span>

        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">optimizer</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>

        <span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_rpn</span><span class="p">:</span>

            <span class="n">targets</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;boxes&quot;</span><span class="p">:</span> <span class="n">t</span><span class="p">[</span><span class="s2">&quot;boxes&quot;</span><span class="p">]}</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">targets</span><span class="p">]</span>

            <span class="n">_</span><span class="p">,</span> <span class="n">loss_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">loss</span> <span class="k">for</span> <span class="n">loss</span> <span class="ow">in</span> <span class="n">loss_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

            <span class="k">return</span> <span class="n">OrderedDict</span><span class="p">({</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s2">&quot;progress_bar&quot;</span><span class="p">:</span> <span class="n">loss_dict</span><span class="p">,</span> <span class="s2">&quot;log&quot;</span><span class="p">:</span> <span class="n">loss_dict</span><span class="p">})</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_roi</span><span class="p">:</span>

            <span class="n">_</span><span class="p">,</span> <span class="n">loss_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">loss</span> <span class="k">for</span> <span class="n">loss</span> <span class="ow">in</span> <span class="n">loss_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

            <span class="k">return</span> <span class="n">OrderedDict</span><span class="p">({</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s2">&quot;progress_bar&quot;</span><span class="p">:</span> <span class="n">loss_dict</span><span class="p">,</span> <span class="s2">&quot;log&quot;</span><span class="p">:</span> <span class="n">loss_dict</span><span class="p">})</span>

        <span class="k">else</span><span class="p">:</span>

            <span class="n">images</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">image</span> <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">images</span><span class="p">)</span>

            <span class="n">targets</span> <span class="o">=</span> <span class="p">[{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">targets</span><span class="p">]</span>

            <span class="n">loss_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>

            <span class="c1"># loss keys: [&#39;loss_classifier&#39;, &#39;loss_box_reg&#39;, &#39;loss_objectness&#39;, &#39;loss_rpn_box_reg&#39;]</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">loss</span> <span class="k">for</span> <span class="n">loss</span> <span class="ow">in</span> <span class="n">loss_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">math</span><span class="o">.</span><span class="kp">isfinite</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="kp">item</span><span class="p">()):</span>

                <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">OrderedDict</span><span class="p">({</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s2">&quot;progress_bar&quot;</span><span class="p">:</span> <span class="n">loss_dict</span><span class="p">,</span> <span class="s2">&quot;log&quot;</span><span class="p">:</span> <span class="n">loss_dict</span><span class="p">})</span>

    <span class="nd">@auto_move_data</span>

    <span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>

        <span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_rpn</span><span class="p">:</span>  <span class="c1"># rpn doesn&#39;t compute loss for val</span>

            <span class="k">return</span> <span class="p">{}</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_roi</span><span class="p">:</span>

            <span class="c1"># TODO: scores are predictions scores, not a metric! iou? + acc?</span>

            <span class="k">return</span> <span class="p">{}</span>

        <span class="k">else</span><span class="p">:</span>

            <span class="n">images</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">image</span> <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">images</span><span class="p">)</span>

            <span class="n">targets</span> <span class="o">=</span> <span class="p">[{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">targets</span><span class="p">]</span>

            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>

            <span class="n">ret</span> <span class="o">=</span> <span class="p">{</span><span class="n">target</span><span class="p">[</span><span class="s2">&quot;image_id&quot;</span><span class="p">]</span><span class="o">.</span><span class="kp">item</span><span class="p">():</span> <span class="n">output</span> <span class="k">for</span> <span class="n">target</span><span class="p">,</span> <span class="n">output</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)}</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">coco_evaluator</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>

            <span class="k">return</span> <span class="p">{}</span>

    <span class="nd">@auto_move_data</span>

    <span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_rpn</span><span class="p">:</span>

            <span class="k">return</span> <span class="p">{}</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_roi</span><span class="p">:</span>

            <span class="c1"># TODO: above</span>

            <span class="k">return</span> <span class="p">{}</span>

        <span class="k">else</span><span class="p">:</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">coco_evaluator</span><span class="o">.</span><span class="n">synchronize_between_processes</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">coco_evaluator</span><span class="o">.</span><span class="kp">accumulate</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">coco_evaluator</span><span class="o">.</span><span class="n">summarize</span><span class="p">()</span>

            <span class="n">metric</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coco_evaluator</span><span class="o">.</span><span class="n">coco_eval</span><span class="p">[</span><span class="s2">&quot;bbox&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">stats</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">metric</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">metric</span><span class="p">)</span>

            <span class="n">tensorboard_logs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;main_score&quot;</span><span class="p">:</span> <span class="n">metric</span><span class="p">}</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">coco_evaluator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_evaluator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">val_dataset</span><span class="p">)</span>  <span class="c1"># need to update for the new evaluation</span>

            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;val_loss&quot;</span><span class="p">:</span> <span class="n">metric</span><span class="p">,</span> <span class="s2">&quot;log&quot;</span><span class="p">:</span> <span class="n">tensorboard_logs</span><span class="p">,</span> <span class="s2">&quot;progress_bar&quot;</span><span class="p">:</span> <span class="n">tensorboard_logs</span><span class="p">}</span>

    <span class="nd">@staticmethod</span>

    <span class="k">def</span> <span class="nf">add_model_specific_args</span><span class="p">(</span><span class="n">parent_parser</span><span class="p">):</span>  <span class="c1"># pragma: no-cover</span>

        <span class="n">parser</span> <span class="o">=</span> <span class="n">ArgumentParser</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="p">[</span><span class="n">parent_parser</span><span class="p">],</span> <span class="n">add_help</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">aa</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span>

        <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--train-rpn&quot;</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">)</span>

        <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--train-roi&quot;</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">)</span>

        <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--finetune-rpn&quot;</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">)</span>

        <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--finetune-roi&quot;</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">)</span>

        <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--data-path&quot;</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s2">&quot;DIR&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;SM_CHANNEL_TRAINING&quot;</span><span class="p">])</span>

        <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--backbone&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;resnet34&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;backbone model either resnet34 (default) or resnet50&quot;</span><span class="p">)</span>

        <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--num-classes&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s2">&quot;N&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;number of classes including the background&quot;</span><span class="p">)</span>

        <span class="n">aa</span><span class="p">(</span>

            <span class="s2">&quot;-b&quot;</span><span class="p">,</span>

            <span class="s2">&quot;--batch-size&quot;</span><span class="p">,</span>

            <span class="n">default</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>

            <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>

            <span class="n">metavar</span><span class="o">=</span><span class="s2">&quot;N&quot;</span><span class="p">,</span>

            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;mini-batch size (default: 16), this is the total &quot;</span>

            <span class="s2">&quot;batch size of all GPUs on the current node when &quot;</span>

            <span class="s2">&quot;using Data Parallel or Distributed Data Parallel&quot;</span><span class="p">,</span>

        <span class="p">)</span>

        <span class="n">aa</span><span class="p">(</span>

            <span class="s2">&quot;--lr&quot;</span><span class="p">,</span>

            <span class="s2">&quot;--learning-rate&quot;</span><span class="p">,</span>

            <span class="n">default</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>

            <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>

            <span class="n">metavar</span><span class="o">=</span><span class="s2">&quot;LR&quot;</span><span class="p">,</span>

            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;initial learning rate&quot;</span><span class="p">,</span>

            <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">,</span>

        <span class="p">)</span>

        <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--momentum&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s2">&quot;M&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;momentum&quot;</span><span class="p">)</span>

        <span class="n">aa</span><span class="p">(</span>

            <span class="s2">&quot;--wd&quot;</span><span class="p">,</span>

            <span class="s2">&quot;--weight-decay&quot;</span><span class="p">,</span>

            <span class="n">default</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>

            <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>

            <span class="n">metavar</span><span class="o">=</span><span class="s2">&quot;W&quot;</span><span class="p">,</span>

            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;weight decay (default: 1e-4)&quot;</span><span class="p">,</span>

            <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">,</span>

        <span class="p">)</span>

        <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--seed&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;seed for initializing training&quot;</span><span class="p">)</span>

        <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--pretrained-mfn-ckpt&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>

        <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--pretrained-rpn-ckpt&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>

        <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--pretrained-roi-ckpt&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>

        <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--finetuned-rpn-ckpt&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>

        <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--finetuned-roi-ckpt&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>

        <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--resume-from-checkpoint&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>

        <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--resume-sagemaker-from-checkpoint&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;SM_CHANNEL_PRETRAINED_CHECKPOINT&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">parser</span>

<span class="k">def</span> <span class="nf">get_args</span><span class="p">():</span>

    <span class="n">parent_parser</span> <span class="o">=</span> <span class="n">ArgumentParser</span><span class="p">(</span><span class="n">add_help</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">aa</span> <span class="o">=</span> <span class="n">parent_parser</span><span class="o">.</span><span class="n">add_argument</span>

    <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--epochs&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;number of training epochs&quot;</span><span class="p">)</span>

    <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--save-path&quot;</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s2">&quot;DIR&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;SM_MODEL_DIR&quot;</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;path to save output&quot;</span><span class="p">)</span>

    <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--gpus&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;SM_NUM_GPUS&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;how many gpus&quot;</span><span class="p">)</span>

    <span class="n">aa</span><span class="p">(</span>

        <span class="s2">&quot;--distributed-backend&quot;</span><span class="p">,</span>

        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>

        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>

        <span class="n">choices</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;dp&quot;</span><span class="p">,</span> <span class="s2">&quot;ddp&quot;</span><span class="p">,</span> <span class="s2">&quot;ddp2&quot;</span><span class="p">),</span>

        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;supports three options dp, ddp, ddp2&quot;</span><span class="p">,</span>

    <span class="p">)</span>

    <span class="c1"># aa(&quot;--use-16bit&quot;, dest=&quot;use_16bit&quot;, action=&quot;store_true&quot;, help=&quot;if true uses 16 bit precision&quot;)</span>

    <span class="n">parser</span> <span class="o">=</span> <span class="n">DDNDetection</span><span class="o">.</span><span class="n">add_model_specific_args</span><span class="p">(</span><span class="n">parent_parser</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">model_fn</span><span class="p">(</span><span class="n">model_dir</span><span class="p">):</span>

    <span class="c1"># TODO: `model_fn` doesn&#39;t get more args</span>

    <span class="c1"># see: https://github.com/aws/sagemaker-inference-toolkit/issues/65</span>

    <span class="n">backbone</span> <span class="o">=</span> <span class="s2">&quot;resnet34&quot;</span>

    <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">7</span>  <span class="c1"># including the background</span>

    <span class="n">mfn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">Classification</span><span class="p">(</span><span class="n">backbone</span><span class="p">,</span> <span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="n">model_dir</span><span class="p">,</span> <span class="s2">&quot;mfn&quot;</span><span class="p">)</span>

    <span class="n">rpn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RPN</span><span class="p">(),</span> <span class="n">model_dir</span><span class="p">,</span> <span class="s2">&quot;rpn&quot;</span><span class="p">)</span>

    <span class="n">roi</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RoI</span><span class="p">(</span><span class="n">num_classes</span><span class="p">),</span> <span class="n">model_dir</span><span class="p">,</span> <span class="s2">&quot;roi&quot;</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">Detection</span><span class="p">(</span><span class="n">mfn</span><span class="p">,</span> <span class="n">rpn</span><span class="p">,</span> <span class="n">roi</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="n">freeze</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">args</span><span class="p">:</span> <span class="n">Namespace</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>

    <span class="n">ddn</span> <span class="o">=</span> <span class="n">DDNDetection</span><span class="p">(</span><span class="o">**</span><span class="nb">vars</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="kp">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

        <span class="n">pl</span><span class="o">.</span><span class="n">seed_everything</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="kp">seed</span><span class="p">)</span>  <span class="c1"># doesn&#39;t do multi-gpu</span>

        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>

            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="kp">seed</span><span class="p">)</span>

    <span class="c1"># TODO: add deterministic training</span>

    <span class="c1"># torch.backends.cudnn.deterministic = True</span>

    <span class="k">if</span> <span class="n">ddn</span><span class="o">.</span><span class="n">train_rpn</span><span class="p">:</span>

        <span class="n">checkpoint_callback</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span>

            <span class="n">filepath</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">save_path</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{epoch}</span><span class="s2">-</span><span class="si">{loss:.3f}</span><span class="s2">&quot;</span><span class="p">),</span>

            <span class="n">save_top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>

            <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>

            <span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span>

            <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>

        <span class="p">)</span>

        <span class="n">early_stop_callback</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">elif</span> <span class="n">ddn</span><span class="o">.</span><span class="n">train_roi</span><span class="p">:</span>

        <span class="n">checkpoint_callback</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span>

            <span class="n">filepath</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">save_path</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{epoch}</span><span class="s2">-</span><span class="si">{loss:.3f}</span><span class="s2">&quot;</span><span class="p">),</span>

            <span class="n">save_top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>

            <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>

            <span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span>

            <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>

        <span class="p">)</span>

        <span class="n">early_stop_callback</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">else</span><span class="p">:</span>

        <span class="n">checkpoint_callback</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span>

            <span class="n">filepath</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">save_path</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{epoch}</span><span class="s2">-</span><span class="si">{loss:.3f}</span><span class="s2">-</span><span class="si">{main_score:.3f}</span><span class="s2">&quot;</span><span class="p">),</span>

            <span class="n">save_top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>

            <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>

            <span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;main_score&quot;</span><span class="p">,</span>

            <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">,</span>

        <span class="p">)</span>

        <span class="n">early_stop_callback</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="s2">&quot;main_score&quot;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">)</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>

        <span class="n">default_root_dir</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">save_path</span><span class="p">,</span>

        <span class="n">num_sanity_val_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>

        <span class="n">limit_val_batches</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>

        <span class="n">gpus</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">,</span>

        <span class="n">max_epochs</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span>

        <span class="n">early_stop_callback</span><span class="o">=</span><span class="n">early_stop_callback</span><span class="p">,</span>

        <span class="n">checkpoint_callback</span><span class="o">=</span><span class="n">checkpoint_callback</span><span class="p">,</span>

        <span class="n">distributed_backend</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">distributed_backend</span> <span class="ow">or</span> <span class="kc">None</span><span class="p">,</span>

        <span class="c1"># precision=16 if args.use_16bit else 32, # TODO: apex</span>

        <span class="n">weights_summary</span><span class="o">=</span><span class="s2">&quot;top&quot;</span><span class="p">,</span>

        <span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="kc">None</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">resume_from_checkpoint</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span> <span class="k">else</span> <span class="n">args</span><span class="o">.</span><span class="n">resume_from_checkpoint</span><span class="p">,</span>

    <span class="p">)</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">ddn</span><span class="p">)</span>

    <span class="k">return</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>

    <span class="n">main</span><span class="p">(</span><span class="n">get_args</span><span class="p">())</span>
</code></pre></div>

</details>
<h2 id="functions">Functions</h2>
<h3 id="get_args">get_args</h3>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_args</span><span class="p">(</span>

<span class="p">)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code><span class="n">def</span> <span class="n">get_args</span><span class="p">():</span>

    <span class="n">parent_parser</span> <span class="o">=</span> <span class="n">ArgumentParser</span><span class="p">(</span><span class="n">add_help</span><span class="o">=</span><span class="n">False</span><span class="p">)</span>

    <span class="n">aa</span> <span class="o">=</span> <span class="n">parent_parser</span><span class="o">.</span><span class="n">add_argument</span>

    <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--epochs&quot;</span><span class="p">,</span> <span class="n">type</span><span class="o">=</span><span class="nb nb-Type">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;number of training epochs&quot;</span><span class="p">)</span>

    <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--save-path&quot;</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s2">&quot;DIR&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;SM_MODEL_DIR&quot;</span><span class="p">],</span> <span class="n">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;path to save output&quot;</span><span class="p">)</span>

    <span class="n">aa</span><span class="p">(</span><span class="s2">&quot;--gpus&quot;</span><span class="p">,</span> <span class="n">type</span><span class="o">=</span><span class="nb nb-Type">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;SM_NUM_GPUS&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;how many gpus&quot;</span><span class="p">)</span>

    <span class="n">aa</span><span class="p">(</span>

        <span class="s2">&quot;--distributed-backend&quot;</span><span class="p">,</span>

        <span class="n">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>

        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>

        <span class="n">choices</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;dp&quot;</span><span class="p">,</span> <span class="s2">&quot;ddp&quot;</span><span class="p">,</span> <span class="s2">&quot;ddp2&quot;</span><span class="p">),</span>

        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;supports three options dp, ddp, ddp2&quot;</span><span class="p">,</span>

    <span class="p">)</span>

    <span class="c1"># aa(&quot;--use-16bit&quot;, dest=&quot;use_16bit&quot;, action=&quot;store_true&quot;, help=&quot;if true uses 16 bit precision&quot;)</span>

    <span class="n">parser</span> <span class="o">=</span> <span class="n">DDNDetection</span><span class="o">.</span><span class="n">add_model_specific_args</span><span class="p">(</span><span class="n">parent_parser</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</code></pre></div>

</details>
<h3 id="main">main</h3>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">main</span><span class="p">(</span>
    <span class="n">args</span><span class="p">:</span><span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code><span class="n">def</span> <span class="n">main</span><span class="p">(</span><span class="n">args</span><span class="p">:</span> <span class="n">Namespace</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">None</span><span class="p">:</span>

    <span class="n">ddn</span> <span class="o">=</span> <span class="n">DDNDetection</span><span class="p">(</span><span class="o">**</span><span class="n">vars</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">seed</span> <span class="k">is</span> <span class="ow">not</span> <span class="n">None</span><span class="p">:</span>

        <span class="n">pl</span><span class="o">.</span><span class="n">seed_everything</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># doesn&#39;t do multi-gpu</span>

        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>

            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># TODO: add deterministic training</span>

    <span class="c1"># torch.backends.cudnn.deterministic = True</span>

    <span class="k">if</span> <span class="n">ddn</span><span class="o">.</span><span class="n">train_rpn</span><span class="p">:</span>

        <span class="n">checkpoint_callback</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span>

            <span class="n">filepath</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">save_path</span><span class="p">,</span> <span class="s2">&quot;{epoch}-{loss:.3f}&quot;</span><span class="p">),</span>

            <span class="n">save_top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>

            <span class="n">verbose</span><span class="o">=</span><span class="n">True</span><span class="p">,</span>

            <span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span>

            <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>

        <span class="p">)</span>

        <span class="n">early_stop_callback</span> <span class="o">=</span> <span class="n">None</span>

    <span class="k">elif</span> <span class="n">ddn</span><span class="o">.</span><span class="n">train_roi</span><span class="p">:</span>

        <span class="n">checkpoint_callback</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span>

            <span class="n">filepath</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">save_path</span><span class="p">,</span> <span class="s2">&quot;{epoch}-{loss:.3f}&quot;</span><span class="p">),</span>

            <span class="n">save_top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>

            <span class="n">verbose</span><span class="o">=</span><span class="n">True</span><span class="p">,</span>

            <span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span>

            <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>

        <span class="p">)</span>

        <span class="n">early_stop_callback</span> <span class="o">=</span> <span class="n">None</span>

    <span class="k">else</span><span class="p">:</span>

        <span class="n">checkpoint_callback</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span>

            <span class="n">filepath</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">save_path</span><span class="p">,</span> <span class="s2">&quot;{epoch}-{loss:.3f}-{main_score:.3f}&quot;</span><span class="p">),</span>

            <span class="n">save_top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>

            <span class="n">verbose</span><span class="o">=</span><span class="n">True</span><span class="p">,</span>

            <span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;main_score&quot;</span><span class="p">,</span>

            <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">,</span>

        <span class="p">)</span>

        <span class="n">early_stop_callback</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="s2">&quot;main_score&quot;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">)</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>

        <span class="n">default_root_dir</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">save_path</span><span class="p">,</span>

        <span class="n">num_sanity_val_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>

        <span class="n">limit_val_batches</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>

        <span class="n">gpus</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">,</span>

        <span class="n">max_epochs</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span>

        <span class="n">early_stop_callback</span><span class="o">=</span><span class="n">early_stop_callback</span><span class="p">,</span>

        <span class="n">checkpoint_callback</span><span class="o">=</span><span class="n">checkpoint_callback</span><span class="p">,</span>

        <span class="n">distributed_backend</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">distributed_backend</span> <span class="ow">or</span> <span class="n">None</span><span class="p">,</span>

        <span class="c1"># precision=16 if args.use_16bit else 32, # TODO: apex</span>

        <span class="n">weights_summary</span><span class="o">=</span><span class="s2">&quot;top&quot;</span><span class="p">,</span>

        <span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="n">None</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">resume_from_checkpoint</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span> <span class="k">else</span> <span class="n">args</span><span class="o">.</span><span class="n">resume_from_checkpoint</span><span class="p">,</span>

    <span class="p">)</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">ddn</span><span class="p">)</span>

    <span class="k">return</span>
</code></pre></div>

</details>
<h3 id="model_fn">model_fn</h3>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">model_fn</span><span class="p">(</span>
    <span class="n">model_dir</span>
<span class="p">)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code><span class="n">def</span> <span class="n">model_fn</span><span class="p">(</span><span class="n">model_dir</span><span class="p">):</span>

    <span class="c1"># TODO: `model_fn` doesn&#39;t get more args</span>

    <span class="c1"># see: https://github.com/aws/sagemaker-inference-toolkit/issues/65</span>

    <span class="n">backbone</span> <span class="o">=</span> <span class="s2">&quot;resnet34&quot;</span>

    <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">7</span>  <span class="c1"># including the background</span>

    <span class="n">mfn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">Classification</span><span class="p">(</span><span class="n">backbone</span><span class="p">,</span> <span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="n">model_dir</span><span class="p">,</span> <span class="s2">&quot;mfn&quot;</span><span class="p">)</span>

    <span class="n">rpn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RPN</span><span class="p">(),</span> <span class="n">model_dir</span><span class="p">,</span> <span class="s2">&quot;rpn&quot;</span><span class="p">)</span>

    <span class="n">roi</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RoI</span><span class="p">(</span><span class="n">num_classes</span><span class="p">),</span> <span class="n">model_dir</span><span class="p">,</span> <span class="s2">&quot;roi&quot;</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">Detection</span><span class="p">(</span><span class="n">mfn</span><span class="p">,</span> <span class="n">rpn</span><span class="p">,</span> <span class="n">roi</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="n">freeze</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>
</code></pre></div>

</details>
<h2 id="classes">Classes</h2>
<h3 id="ddndetection">DDNDetection</h3>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">DDNDetection</span><span class="p">(</span>
    <span class="n">train_rpn</span><span class="p">:</span><span class="nb">bool</span><span class="p">,</span>
    <span class="n">train_roi</span><span class="p">:</span><span class="nb">bool</span><span class="p">,</span>
    <span class="n">finetune_rpn</span><span class="p">:</span><span class="nb">bool</span><span class="p">,</span>
    <span class="n">finetune_roi</span><span class="p">:</span><span class="nb">bool</span><span class="p">,</span>
    <span class="n">data_path</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span>
    <span class="n">backbone</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="p">:</span><span class="nb">float</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span>
    <span class="n">momentum</span><span class="p">:</span><span class="nb">float</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="p">:</span><span class="nb">float</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span>
    <span class="n">pretrained_mfn_ckpt</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">pretrained_rpn_ckpt</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">pretrained_roi_ckpt</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">finetuned_rpn_ckpt</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">finetuned_roi_ckpt</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">resume_sagemaker_from_checkpoint</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<h4 id="ancestors-in-mro">Ancestors (in MRO)</h4>
<ul>
<li>pytorch_lightning.core.lightning.LightningModule</li>
<li>abc.ABC</li>
<li>pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin</li>
<li>pytorch_lightning.core.grads.GradInformation</li>
<li>pytorch_lightning.core.saving.ModelIO</li>
<li>pytorch_lightning.core.hooks.ModelHooks</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h4 id="class-variables">Class variables</h4>
<div class="codehilite"><pre><span></span><code><span class="n">CHECKPOINT_HYPER_PARAMS_KEY</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">CHECKPOINT_HYPER_PARAMS_NAME</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">CHECKPOINT_HYPER_PARAMS_TYPE</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">T_destination</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">dump_patches</span>
</code></pre></div>

<h4 id="static-methods">Static methods</h4>
<h5 id="add_model_specific_args">add_model_specific_args</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_model_specific_args</span><span class="p">(</span>
    <span class="n">parent_parser</span>
<span class="p">)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@staticmethod</span><span class="w"></span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">add_model_specific_args</span><span class="p">(</span><span class="n">parent_parser</span><span class="p">)</span><span class="err">:</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="nl">pragma</span><span class="p">:</span><span class="w"> </span><span class="k">no</span><span class="o">-</span><span class="n">cover</span><span class="w"></span>

<span class="w">        </span><span class="n">parser</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">parents</span><span class="o">=[</span><span class="n">parent_parser</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">add_help</span><span class="o">=</span><span class="k">False</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="p">(</span><span class="ss">&quot;--train-rpn&quot;</span><span class="p">,</span><span class="w"> </span><span class="k">action</span><span class="o">=</span><span class="ss">&quot;store_true&quot;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="p">(</span><span class="ss">&quot;--train-roi&quot;</span><span class="p">,</span><span class="w"> </span><span class="k">action</span><span class="o">=</span><span class="ss">&quot;store_true&quot;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="p">(</span><span class="ss">&quot;--finetune-rpn&quot;</span><span class="p">,</span><span class="w"> </span><span class="k">action</span><span class="o">=</span><span class="ss">&quot;store_true&quot;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="p">(</span><span class="ss">&quot;--finetune-roi&quot;</span><span class="p">,</span><span class="w"> </span><span class="k">action</span><span class="o">=</span><span class="ss">&quot;store_true&quot;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="p">(</span><span class="ss">&quot;--data-path&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">metavar</span><span class="o">=</span><span class="ss">&quot;DIR&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="nf">str</span><span class="p">,</span><span class="w"> </span><span class="k">default</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="o">[</span><span class="n">&quot;SM_CHANNEL_TRAINING&quot;</span><span class="o">]</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="p">(</span><span class="ss">&quot;--backbone&quot;</span><span class="p">,</span><span class="w"> </span><span class="k">default</span><span class="o">=</span><span class="ss">&quot;resnet34&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">help</span><span class="o">=</span><span class="ss">&quot;backbone model either resnet34 (default) or resnet50&quot;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="p">(</span><span class="ss">&quot;--num-classes&quot;</span><span class="p">,</span><span class="w"> </span><span class="k">default</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="nc">int</span><span class="p">,</span><span class="w"> </span><span class="n">metavar</span><span class="o">=</span><span class="ss">&quot;N&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">help</span><span class="o">=</span><span class="ss">&quot;number of classes including the background&quot;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="ss">&quot;-b&quot;</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="ss">&quot;--batch-size&quot;</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="k">default</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="n">type</span><span class="o">=</span><span class="nc">int</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="n">metavar</span><span class="o">=</span><span class="ss">&quot;N&quot;</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="n">help</span><span class="o">=</span><span class="ss">&quot;mini-batch size (default: 16), this is the total &quot;</span><span class="w"></span>

<span class="w">            </span><span class="ss">&quot;batch size of all GPUs on the current node when &quot;</span><span class="w"></span>

<span class="w">            </span><span class="ss">&quot;using Data Parallel or Distributed Data Parallel&quot;</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="ss">&quot;--lr&quot;</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="ss">&quot;--learning-rate&quot;</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="k">default</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="n">type</span><span class="o">=</span><span class="nc">float</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="n">metavar</span><span class="o">=</span><span class="ss">&quot;LR&quot;</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="n">help</span><span class="o">=</span><span class="ss">&quot;initial learning rate&quot;</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="n">dest</span><span class="o">=</span><span class="ss">&quot;learning_rate&quot;</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="p">(</span><span class="ss">&quot;--momentum&quot;</span><span class="p">,</span><span class="w"> </span><span class="k">default</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="nc">float</span><span class="p">,</span><span class="w"> </span><span class="n">metavar</span><span class="o">=</span><span class="ss">&quot;M&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">help</span><span class="o">=</span><span class="ss">&quot;momentum&quot;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="ss">&quot;--wd&quot;</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="ss">&quot;--weight-decay&quot;</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="k">default</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="n">type</span><span class="o">=</span><span class="nc">float</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="n">metavar</span><span class="o">=</span><span class="ss">&quot;W&quot;</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="n">help</span><span class="o">=</span><span class="ss">&quot;weight decay (default: 1e-4)&quot;</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="n">dest</span><span class="o">=</span><span class="ss">&quot;weight_decay&quot;</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="p">(</span><span class="ss">&quot;--seed&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="nc">int</span><span class="p">,</span><span class="w"> </span><span class="k">default</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span><span class="w"> </span><span class="n">help</span><span class="o">=</span><span class="ss">&quot;seed for initializing training&quot;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="p">(</span><span class="ss">&quot;--pretrained-mfn-ckpt&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="nf">str</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="p">(</span><span class="ss">&quot;--pretrained-rpn-ckpt&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="nf">str</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="p">(</span><span class="ss">&quot;--pretrained-roi-ckpt&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="nf">str</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="p">(</span><span class="ss">&quot;--finetuned-rpn-ckpt&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="nf">str</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="p">(</span><span class="ss">&quot;--finetuned-roi-ckpt&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="nf">str</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="p">(</span><span class="ss">&quot;--resume-from-checkpoint&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="nf">str</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">aa</span><span class="p">(</span><span class="ss">&quot;--resume-sagemaker-from-checkpoint&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="nf">str</span><span class="p">,</span><span class="w"> </span><span class="k">default</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">getenv</span><span class="p">(</span><span class="ss">&quot;SM_CHANNEL_PRETRAINED_CHECKPOINT&quot;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">parser</span><span class="w"></span>
</code></pre></div>

</details>
<h5 id="load_from_checkpoint">load_from_checkpoint</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">load_from_checkpoint</span><span class="p">(</span>
    <span class="n">checkpoint_path</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="n">map_location</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hparams_file</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">tags_csv</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint
it stores the arguments passed to <code>__init__</code>  in the checkpoint under <code>module_arguments</code></p>
<p>Any arguments specified through *args and **kwargs will override args stored in <code>hparams</code>.</p>
<p>Args:
    checkpoint_path: Path to checkpoint. This can also be a URL.
    args: Any positional args needed to init the model.
    map_location:
        If your checkpoint saved a GPU model and you now load on CPUs
        or a different number of GPUs, use this to map to the new setup.
        The behaviour is the same as in :func:<code>torch.load</code>.
    hparams_file: Optional path to a .yaml file with hierarchical structure
        as in this example::</p>
<div class="codehilite"><pre><span></span><code>        <span class="n">drop_prob</span><span class="o">:</span> <span class="mf">0.2</span>
        <span class="n">dataloader</span><span class="o">:</span>
            <span class="n">batch_size</span><span class="o">:</span> <span class="mi">32</span>

    <span class="n">You</span> <span class="n">most</span> <span class="n">likely</span> <span class="n">won</span><span class="s1">&#39;t need this since Lightning will always save the hyperparameters</span>
<span class="s1">    to the checkpoint.</span>
<span class="s1">    However, if your checkpoint weights don&#39;</span><span class="n">t</span> <span class="n">have</span> <span class="n">the</span> <span class="n">hyperparameters</span> <span class="n">saved</span><span class="p">,</span>
    <span class="k">use</span> <span class="n">this</span> <span class="n">method</span> <span class="k">to</span> <span class="n">pass</span> <span class="k">in</span> <span class="n">a</span> <span class="p">.</span><span class="n">yaml</span> <span class="k">file</span> <span class="k">with</span> <span class="n">the</span> <span class="n">hparams</span> <span class="n">you</span><span class="s1">&#39;d like to use.</span>
<span class="s1">    These will be converted into a :class:`~dict` and passed into your</span>
<span class="s1">    :class:`LightningModule` for use.</span>

<span class="s1">    If your model&#39;</span><span class="n">s</span> <span class="n">`hparams`</span> <span class="n">argument</span> <span class="k">is</span> <span class="o">:</span><span class="n">class</span><span class="o">:</span><span class="n">`~argparse.Namespace`</span>
    <span class="k">and</span> <span class="p">.</span><span class="n">yaml</span> <span class="k">file</span> <span class="n">has</span> <span class="n">hierarchical</span> <span class="n">structure</span><span class="p">,</span> <span class="n">you</span> <span class="n">need</span> <span class="k">to</span> <span class="n">refactor</span> <span class="n">your</span> <span class="n">model</span> <span class="k">to</span> <span class="n">treat</span>
    <span class="n">`hparams`</span> <span class="k">as</span> <span class="o">:</span><span class="n">class</span><span class="o">:</span><span class="n">`~dict`</span><span class="p">.</span>

    <span class="p">.</span><span class="n">csv</span> <span class="n">files</span> <span class="n">are</span> <span class="n">acceptable</span> <span class="n">here</span> <span class="n">till</span> <span class="n">v0</span><span class="mf">.9.0</span><span class="p">,</span> <span class="n">see</span> <span class="n">tags_csv</span> <span class="n">argument</span> <span class="k">for</span> <span class="n">detailed</span> <span class="k">usage</span><span class="p">.</span>
<span class="n">tags_csv</span><span class="o">:</span>
    <span class="p">..</span> <span class="n">warning</span><span class="o">::</span> <span class="p">..</span> <span class="n">deprecated</span><span class="o">::</span> <span class="mf">0.7.6</span>

        <span class="n">`tags_csv`</span> <span class="n">argument</span> <span class="k">is</span> <span class="n">deprecated</span> <span class="k">in</span> <span class="n">v0</span><span class="mf">.7.6</span><span class="p">.</span> <span class="n">Will</span> <span class="n">be</span> <span class="n">removed</span> <span class="n">v0</span><span class="mf">.9.0</span><span class="p">.</span>

    <span class="k">Optional</span> <span class="k">path</span> <span class="k">to</span> <span class="n">a</span> <span class="p">.</span><span class="n">csv</span> <span class="k">file</span> <span class="k">with</span> <span class="n">two</span> <span class="k">columns</span> <span class="p">(</span><span class="k">key</span><span class="p">,</span> <span class="k">value</span><span class="p">)</span>
    <span class="k">as</span> <span class="k">in</span> <span class="n">this</span> <span class="n">example</span><span class="o">::</span>

        <span class="k">key</span><span class="p">,</span><span class="k">value</span>
        <span class="n">drop_prob</span><span class="p">,</span><span class="mf">0.2</span>
        <span class="n">batch_size</span><span class="p">,</span><span class="mi">32</span>

    <span class="k">Use</span> <span class="n">this</span> <span class="n">method</span> <span class="k">to</span> <span class="n">pass</span> <span class="k">in</span> <span class="n">a</span> <span class="p">.</span><span class="n">csv</span> <span class="k">file</span> <span class="k">with</span> <span class="n">the</span> <span class="n">hparams</span> <span class="n">you</span><span class="s1">&#39;d like to use.</span>
<span class="s1">hparam_overrides: A dictionary with keys to override in the hparams</span>
<span class="s1">kwargs: Any keyword args needed to init the model.</span>
</code></pre></div>

<p>Return:
    :class:<code>LightningModule</code> with loaded weights and hyperparameters (if available).</p>
<p>Example:
    .. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>    <span class="c1"># load weights without mapping ...</span>
    <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">)</span>

    <span class="c1"># or load weights mapping all weights from GPU 1 to GPU 0 ...</span>
    <span class="n">map_location</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">:</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">}</span>
    <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
        <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
        <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span>
    <span class="p">)</span>

    <span class="c1"># or load weights and hyperparameters from separate files.</span>
    <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
        <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
        <span class="n">hparams_file</span><span class="o">=</span><span class="s1">&#39;/path/to/hparams_file.yaml&#39;</span>
    <span class="p">)</span>

    <span class="c1"># override some of the params with new values</span>
    <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
        <span class="n">PATH</span><span class="p">,</span>
        <span class="n">num_layers</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
        <span class="n">pretrained_ckpt_path</span><span class="p">:</span> <span class="n">NEW_PATH</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># predict</span>
    <span class="n">pretrained_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">pretrained_model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="nv">@classmethod</span>

    <span class="n">def</span> <span class="n">load_from_checkpoint</span><span class="p">(</span>

            <span class="n">cls</span><span class="p">,</span>

            <span class="n">checkpoint_path</span><span class="o">:</span> <span class="n">str</span><span class="p">,</span>

            <span class="o">*</span><span class="n">args</span><span class="p">,</span>

            <span class="n">map_location</span><span class="o">:</span> <span class="k">Optional</span><span class="err">[</span><span class="k">Union</span><span class="err">[</span><span class="n">Dict</span><span class="err">[</span><span class="n">str</span><span class="p">,</span> <span class="n">str</span><span class="err">]</span><span class="p">,</span> <span class="n">str</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="kt">int</span><span class="p">,</span> <span class="n">Callable</span><span class="err">]]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

            <span class="n">hparams_file</span><span class="o">:</span> <span class="k">Optional</span><span class="err">[</span><span class="n">str</span><span class="err">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>

            <span class="n">tags_csv</span><span class="o">:</span> <span class="k">Optional</span><span class="err">[</span><span class="n">str</span><span class="err">]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span>  <span class="c1"># backward compatible, todo: remove in v0.9.0</span>

            <span class="o">**</span><span class="n">kwargs</span>

    <span class="p">)</span><span class="o">:</span>

        <span class="n">r</span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2"></span>

<span class="s2">        Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint</span>

<span class="s2">        it stores the arguments passed to `__init__`  in the checkpoint under `module_arguments`</span>

<span class="s2">        Any arguments specified through </span><span class="err">\</span><span class="s2">*args and </span><span class="err">\</span><span class="s2">*</span><span class="err">\</span><span class="s2">*kwargs will override args stored in `hparams`.</span>

<span class="s2">        Args:</span>

<span class="s2">            checkpoint_path: Path to checkpoint. This can also be a URL.</span>

<span class="s2">            args: Any positional args needed to init the model.</span>

<span class="s2">            map_location:</span>

<span class="s2">                If your checkpoint saved a GPU model and you now load on CPUs</span>

<span class="s2">                or a different number of GPUs, use this to map to the new setup.</span>

<span class="s2">                The behaviour is the same as in :func:`torch.load`.</span>

<span class="s2">            hparams_file: Optional path to a .yaml file with hierarchical structure</span>

<span class="s2">                as in this example::</span>

<span class="s2">                    drop_prob: 0.2</span>

<span class="s2">                    dataloader:</span>

<span class="s2">                        batch_size: 32</span>

<span class="s2">                You most likely won&#39;t need this since Lightning will always save the hyperparameters</span>

<span class="s2">                to the checkpoint.</span>

<span class="s2">                However, if your checkpoint weights don&#39;t have the hyperparameters saved,</span>

<span class="s2">                use this method to pass in a .yaml file with the hparams you&#39;d like to use.</span>

<span class="s2">                These will be converted into a :class:`~dict` and passed into your</span>

<span class="s2">                :class:`LightningModule` for use.</span>

<span class="s2">                If your model&#39;s `hparams` argument is :class:`~argparse.Namespace`</span>

<span class="s2">                and .yaml file has hierarchical structure, you need to refactor your model to treat</span>

<span class="s2">                `hparams` as :class:`~dict`.</span>

<span class="s2">                .csv files are acceptable here till v0.9.0, see tags_csv argument for detailed usage.</span>

<span class="s2">            tags_csv:</span>

<span class="s2">                .. warning:: .. deprecated:: 0.7.6</span>

<span class="s2">                    `tags_csv` argument is deprecated in v0.7.6. Will be removed v0.9.0.</span>

<span class="s2">                Optional path to a .csv file with two columns (key, value)</span>

<span class="s2">                as in this example::</span>

<span class="s2">                    key,value</span>

<span class="s2">                    drop_prob,0.2</span>

<span class="s2">                    batch_size,32</span>

<span class="s2">                Use this method to pass in a .csv file with the hparams you&#39;d like to use.</span>

<span class="s2">            hparam_overrides: A dictionary with keys to override in the hparams</span>

<span class="s2">            kwargs: Any keyword args needed to init the model.</span>

<span class="s2">        Return:</span>

<span class="s2">            :class:`LightningModule` with loaded weights and hyperparameters (if available).</span>

<span class="s2">        Example:</span>

<span class="s2">            .. code-block:: python</span>

<span class="s2">                # load weights without mapping ...</span>

<span class="s2">                MyLightningModule.load_from_checkpoint(&#39;path/to/checkpoint.ckpt&#39;)</span>

<span class="s2">                # or load weights mapping all weights from GPU 1 to GPU 0 ...</span>

<span class="s2">                map_location = {&#39;cuda:1&#39;:&#39;cuda:0&#39;}</span>

<span class="s2">                MyLightningModule.load_from_checkpoint(</span>

<span class="s2">                    &#39;path/to/checkpoint.ckpt&#39;,</span>

<span class="s2">                    map_location=map_location</span>

<span class="s2">                )</span>

<span class="s2">                # or load weights and hyperparameters from separate files.</span>

<span class="s2">                MyLightningModule.load_from_checkpoint(</span>

<span class="s2">                    &#39;path/to/checkpoint.ckpt&#39;,</span>

<span class="s2">                    hparams_file=&#39;/path/to/hparams_file.yaml&#39;</span>

<span class="s2">                )</span>

<span class="s2">                # override some of the params with new values</span>

<span class="s2">                MyLightningModule.load_from_checkpoint(</span>

<span class="s2">                    PATH,</span>

<span class="s2">                    num_layers=128,</span>

<span class="s2">                    pretrained_ckpt_path: NEW_PATH,</span>

<span class="s2">                )</span>

<span class="s2">                # predict</span>

<span class="s2">                pretrained_model.eval()</span>

<span class="s2">                pretrained_model.freeze()</span>

<span class="s2">                y_hat = pretrained_model(x)</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="k">if</span> <span class="n">map_location</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="o">:</span>

            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">pl_load</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">)</span>

        <span class="k">else</span><span class="o">:</span>

            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">pl_load</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">lambda</span> <span class="k">storage</span><span class="p">,</span> <span class="n">loc</span><span class="o">:</span> <span class="k">storage</span><span class="p">)</span>

        <span class="c1"># add the hparams from csv file to checkpoint</span>

        <span class="k">if</span> <span class="n">tags_csv</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="o">:</span>

            <span class="n">hparams_file</span> <span class="o">=</span> <span class="n">tags_csv</span>

            <span class="n">rank_zero_warn</span><span class="p">(</span><span class="s1">&#39;`tags_csv` argument is deprecated in v0.7.6. Will be removed v0.9.0&#39;</span><span class="p">,</span> <span class="n">DeprecationWarning</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">hparams_file</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="o">:</span>

            <span class="n">extension</span> <span class="o">=</span> <span class="n">hparams_file</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span><span class="err">[</span><span class="o">-</span><span class="mi">1</span><span class="err">]</span>

            <span class="k">if</span> <span class="n">extension</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span> <span class="k">in</span> <span class="p">(</span><span class="s1">&#39;csv&#39;</span><span class="p">)</span><span class="o">:</span>

                <span class="n">hparams</span> <span class="o">=</span> <span class="n">load_hparams_from_tags_csv</span><span class="p">(</span><span class="n">hparams_file</span><span class="p">)</span>

            <span class="n">elif</span> <span class="n">extension</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span> <span class="k">in</span> <span class="p">(</span><span class="s1">&#39;yml&#39;</span><span class="p">,</span> <span class="s1">&#39;yaml&#39;</span><span class="p">)</span><span class="o">:</span>

                <span class="n">hparams</span> <span class="o">=</span> <span class="n">load_hparams_from_yaml</span><span class="p">(</span><span class="n">hparams_file</span><span class="p">)</span>

            <span class="k">else</span><span class="o">:</span>

                <span class="n">raise</span> <span class="n">ValueError</span><span class="p">(</span><span class="s1">&#39;.csv, .yml or .yaml is required for `hparams_file`&#39;</span><span class="p">)</span>

            <span class="n">hparams</span><span class="err">[</span><span class="s1">&#39;on_gpu&#39;</span><span class="err">]</span> <span class="o">=</span> <span class="no">False</span>

            <span class="c1"># overwrite hparams by the given file</span>

            <span class="n">checkpoint</span><span class="err">[</span><span class="n">cls</span><span class="p">.</span><span class="n">CHECKPOINT_HYPER_PARAMS_KEY</span><span class="err">]</span> <span class="o">=</span> <span class="n">hparams</span>

        <span class="c1"># for past checkpoint need to add the new key</span>

        <span class="k">if</span> <span class="n">cls</span><span class="p">.</span><span class="n">CHECKPOINT_HYPER_PARAMS_KEY</span> <span class="k">not</span> <span class="k">in</span> <span class="n">checkpoint</span><span class="o">:</span>

            <span class="n">checkpoint</span><span class="err">[</span><span class="n">cls</span><span class="p">.</span><span class="n">CHECKPOINT_HYPER_PARAMS_KEY</span><span class="err">]</span> <span class="o">=</span> <span class="err">{}</span>

        <span class="c1"># override the hparams with values that were passed in</span>

        <span class="n">checkpoint</span><span class="err">[</span><span class="n">cls</span><span class="p">.</span><span class="n">CHECKPOINT_HYPER_PARAMS_KEY</span><span class="err">]</span><span class="p">.</span><span class="k">update</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">model</span> <span class="o">=</span> <span class="n">cls</span><span class="p">.</span><span class="n">_load_model_state</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">model</span>
</code></pre></div>

</details>
<h5 id="load_from_metrics">load_from_metrics</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">load_from_metrics</span><span class="p">(</span>
    <span class="n">weights_path</span><span class="p">,</span>
    <span class="n">tags_csv</span><span class="p">,</span>
    <span class="n">map_location</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

<p>Warning:
Deprecated in version 0.7.0. You should use :meth:<code>load_from_checkpoint</code> instead.
Will be removed in v0.9.0.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="nv">@classmethod</span>

    <span class="n">def</span> <span class="n">load_from_metrics</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">weights_path</span><span class="p">,</span> <span class="n">tags_csv</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="k">None</span><span class="p">)</span><span class="o">:</span>

        <span class="n">r</span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2"></span>

<span class="s2">        Warning:</span>

<span class="s2">            Deprecated in version 0.7.0. You should use :meth:`load_from_checkpoint` instead.</span>

<span class="s2">            Will be removed in v0.9.0.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="n">rank_zero_warn</span><span class="p">(</span>

            <span class="s2">&quot;`load_from_metrics` method has been unified with `load_from_checkpoint` in v0.7.0.&quot;</span>

            <span class="s2">&quot; The deprecated method will be removed in v0.9.0.&quot;</span><span class="p">,</span> <span class="n">DeprecationWarning</span>

        <span class="p">)</span>

        <span class="k">return</span> <span class="n">cls</span><span class="p">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="n">weights_path</span><span class="p">,</span> <span class="n">tags_csv</span><span class="o">=</span><span class="n">tags_csv</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">)</span>
</code></pre></div>

</details>
<h4 id="instance-variables">Instance variables</h4>
<div class="codehilite"><pre><span></span><code><span class="n">device</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">dtype</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">example_input_array</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">hparams</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">on_gpu</span>
</code></pre></div>

<p>True if your model is currently running on GPUs.
Useful to set flags around the LightningModule for different CPU vs GPU behavior.</p>
<h4 id="methods">Methods</h4>
<h5 id="add_module">add_module</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_module</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span>
    <span class="n">module</span><span class="p">:</span><span class="s1">&#39;Module&#39;</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<p>Args:
    name (string): name of the child module. The child module can be
        accessed from this module using the given name
    module (Module): child module to be added to the module.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">add_module</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span><span class="p">,</span><span class="w"> </span><span class="k">module</span><span class="err">:</span><span class="w"> </span><span class="s1">&#39;Module&#39;</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="n">r</span><span class="ss">&quot;&quot;&quot;Adds a child module to the current module.</span>

<span class="ss">        The module can be accessed as an attribute using the given name.</span>

<span class="ss">        Args:</span>

<span class="ss">            name (string): name of the child module. The child module can be</span>

<span class="ss">                accessed from this module using the given name</span>

<span class="ss">            module (Module): child module to be added to the module.</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="k">module</span><span class="p">,</span><span class="w"> </span><span class="k">Module</span><span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">module</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span><span class="ss">&quot;{} is not a Module subclass&quot;</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="w"></span>

<span class="w">                </span><span class="n">torch</span><span class="p">.</span><span class="n">typename</span><span class="p">(</span><span class="k">module</span><span class="p">)))</span><span class="w"></span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="n">_six</span><span class="p">.</span><span class="n">string_classes</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span><span class="ss">&quot;module name should be a string. Got {}&quot;</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="w"></span>

<span class="w">                </span><span class="n">torch</span><span class="p">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span><span class="w"></span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="n">hasattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="nl">_modules</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">KeyError</span><span class="p">(</span><span class="ss">&quot;attribute &#39;{}&#39; already exists&quot;</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="s1">&#39;.&#39;</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nl">name</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">KeyError</span><span class="p">(</span><span class="ss">&quot;module name can&#39;t contain \&quot;</span><span class="p">.</span><span class="err">\</span><span class="ss">&quot;&quot;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s1">&#39;&#39;</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">KeyError</span><span class="p">(</span><span class="ss">&quot;module name can&#39;t be empty string \&quot;</span><span class="err">\</span><span class="ss">&quot;&quot;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_modules</span><span class="o">[</span><span class="n">name</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">module</span><span class="w"></span>
</code></pre></div>

</details>
<h5 id="amp_scale_loss">amp_scale_loss</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">amp_scale_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">unscaled_loss</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">optimizer_idx</span>
<span class="p">)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">amp_scale_loss</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">unscaled_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>

        <span class="k">if</span> <span class="n">NATIVE_AMP_AVALAIBLE</span><span class="p">:</span>

            <span class="n">scaled_loss</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">trainer</span><span class="p">.</span><span class="n">scaler</span><span class="p">.</span><span class="k">scale</span><span class="p">(</span><span class="n">unscaled_loss</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>

            <span class="n">scaled_loss</span> <span class="o">=</span> <span class="n">amp</span><span class="p">.</span><span class="n">scale_loss</span><span class="p">(</span><span class="n">unscaled_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">scaled_loss</span>
</code></pre></div>

</details>
<h5 id="apply">apply</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">apply</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">:</span><span class="o">~</span><span class="n">T</span><span class="p">,</span>
    <span class="n">fn</span><span class="p">:</span><span class="n">Callable</span><span class="p">[[</span><span class="n">_ForwardRef</span><span class="p">(</span><span class="s1">&#39;Module&#39;</span><span class="p">)],</span> <span class="n">NoneType</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="o">~</span><span class="n">T</span>
</code></pre></div>

<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>)
as well as self. Typical use includes initializing the parameters of a model
(see also :ref:<code>nn-init-doc</code>).</p>
<p>Args:
    fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule</p>
<p>Returns:
    Module: self</p>
<p>Example::</p>
<div class="codehilite"><pre><span></span><code><span class="err">&gt;&gt;&gt; @torch.no_grad()</span>
<span class="err">&gt;&gt;&gt; def init_weights(m):</span>
<span class="err">&gt;&gt;&gt;     print(m)</span>
<span class="err">&gt;&gt;&gt;     if type(m) == nn.Linear:</span>
<span class="err">&gt;&gt;&gt;         m.weight.fill_(1.0)</span>
<span class="err">&gt;&gt;&gt;         print(m.weight)</span>
<span class="err">&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))</span>
<span class="err">&gt;&gt;&gt; net.apply(init_weights)</span>
<span class="err">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="err">Parameter containing:</span>
<span class="err">tensor([[ 1.,  1.],</span>
<span class="err">        [ 1.,  1.]])</span>
<span class="err">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="err">Parameter containing:</span>
<span class="err">tensor([[ 1.,  1.],</span>
<span class="err">        [ 1.,  1.]])</span>
<span class="err">Sequential(</span>
<span class="err">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="err">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="err">)</span>
<span class="err">Sequential(</span>
<span class="err">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="err">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="err">)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">apply</span><span class="p">(</span><span class="n">self</span><span class="o">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">fn</span><span class="o">:</span> <span class="n">Callable</span><span class="err">[[</span><span class="s1">&#39;Module&#39;</span><span class="err">]</span><span class="p">,</span> <span class="k">None</span><span class="err">]</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="o">:</span>

        <span class="n">r</span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Applies ``fn`` recursively to every submodule (as returned by ``.children()``)</span>

<span class="s2">        as well as self. Typical use includes initializing the parameters of a model</span>

<span class="s2">        (see also :ref:`nn-init-doc`).</span>

<span class="s2">        Args:</span>

<span class="s2">            fn (:class:`Module` -&gt; None): function to be applied to each submodule</span>

<span class="s2">        Returns:</span>

<span class="s2">            Module: self</span>

<span class="s2">        Example::</span>

<span class="s2">            &gt;&gt;&gt; @torch.no_grad()</span>

<span class="s2">            &gt;&gt;&gt; def init_weights(m):</span>

<span class="s2">            &gt;&gt;&gt;     print(m)</span>

<span class="s2">            &gt;&gt;&gt;     if type(m) == nn.Linear:</span>

<span class="s2">            &gt;&gt;&gt;         m.weight.fill_(1.0)</span>

<span class="s2">            &gt;&gt;&gt;         print(m.weight)</span>

<span class="s2">            &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))</span>

<span class="s2">            &gt;&gt;&gt; net.apply(init_weights)</span>

<span class="s2">            Linear(in_features=2, out_features=2, bias=True)</span>

<span class="s2">            Parameter containing:</span>

<span class="s2">            tensor([[ 1.,  1.],</span>

<span class="s2">                    [ 1.,  1.]])</span>

<span class="s2">            Linear(in_features=2, out_features=2, bias=True)</span>

<span class="s2">            Parameter containing:</span>

<span class="s2">            tensor([[ 1.,  1.],</span>

<span class="s2">                    [ 1.,  1.]])</span>

<span class="s2">            Sequential(</span>

<span class="s2">              (0): Linear(in_features=2, out_features=2, bias=True)</span>

<span class="s2">              (1): Linear(in_features=2, out_features=2, bias=True)</span>

<span class="s2">            )</span>

<span class="s2">            Sequential(</span>

<span class="s2">              (0): Linear(in_features=2, out_features=2, bias=True)</span>

<span class="s2">              (1): Linear(in_features=2, out_features=2, bias=True)</span>

<span class="s2">            )</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="k">for</span> <span class="n">module</span> <span class="k">in</span> <span class="n">self</span><span class="p">.</span><span class="n">children</span><span class="p">()</span><span class="o">:</span>

            <span class="n">module</span><span class="p">.</span><span class="n">apply</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

        <span class="n">fn</span><span class="p">(</span><span class="n">self</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">self</span>
</code></pre></div>

</details>
<h5 id="backward">backward</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">trainer</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
    <span class="n">optimizer_idx</span><span class="p">:</span><span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Override backward with your own implementation if you need to.</p>
<p>Args:
    trainer: Pointer to the trainer
    loss: Loss is already scaled by accumulated grads
    optimizer: Current optimizer being used
    optimizer_idx: Index of the current optimizer being used</p>
<p>Called to perform backward step.
Feel free to override as needed.</p>
<p>The loss passed in has already been scaled for accumulated gradients if requested.</p>
<p>Example::</p>
<div class="codehilite"><pre><span></span><code><span class="err">def backward(self, trainer, loss, optimizer, optimizer_idx):</span>
<span class="err">    loss.backward()</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="k">backward</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">None</span><span class="p">:</span>

        <span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Override backward with your own implementation if you need to.</span>

<span class="ss">        Args:</span>

<span class="ss">            trainer: Pointer to the trainer</span>

<span class="ss">            loss: Loss is already scaled by accumulated grads</span>

<span class="ss">            optimizer: Current optimizer being used</span>

<span class="ss">            optimizer_idx: Index of the current optimizer being used</span>

<span class="ss">        Called to perform backward step.</span>

<span class="ss">        Feel free to override as needed.</span>

<span class="ss">        The loss passed in has already been scaled for accumulated gradients if requested.</span>

<span class="ss">        Example::</span>

<span class="ss">            def backward(self, trainer, loss, optimizer, optimizer_idx):</span>

<span class="ss">                loss.backward()</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">loss</span><span class="p">.</span><span class="k">backward</span><span class="p">()</span>
</code></pre></div>

</details>
<h5 id="bfloat16">bfloat16</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">bfloat16</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">:</span><span class="o">~</span><span class="n">T</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="o">~</span><span class="n">T</span>
</code></pre></div>

<p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p>
<p>Returns:
    Module: self</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">bfloat16</span><span class="p">(</span><span class="n">self</span><span class="o">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="o">:</span>

        <span class="n">r</span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Casts all floating point parameters and buffers to ``bfloat16`` datatype.</span>

<span class="s2">        Returns:</span>

<span class="s2">            Module: self</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">_apply</span><span class="p">(</span><span class="n">lambda</span> <span class="n">t</span><span class="o">:</span> <span class="n">t</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">()</span> <span class="k">if</span> <span class="n">t</span><span class="p">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="k">else</span> <span class="n">t</span><span class="p">)</span>
</code></pre></div>

</details>
<h5 id="buffers">buffers</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">buffers</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">recurse</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
</code></pre></div>

<p>Returns an iterator over module buffers.</p>
<p>Args:
    recurse (bool): if True, then yields buffers of this module
        and all submodules. Otherwise, yields only buffers that
        are direct members of this module.</p>
<p>Yields:
    torch.Tensor: module buffer</p>
<p>Example::</p>
<div class="codehilite"><pre><span></span><code><span class="err">&gt;&gt;&gt; for buf in model.buffers():</span>
<span class="err">&gt;&gt;&gt;     print(type(buf), buf.size())</span>
<span class="err">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="err">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">buffers</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">recurse</span><span class="p">:</span><span class="w"> </span><span class="n">bool</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">True</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Iterator</span><span class="o">[</span><span class="n">Tensor</span><span class="o">]</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="n">r</span><span class="ss">&quot;&quot;&quot;Returns an iterator over module buffers.</span>

<span class="ss">        Args:</span>

<span class="ss">            recurse (bool): if True, then yields buffers of this module</span>

<span class="ss">                and all submodules. Otherwise, yields only buffers that</span>

<span class="ss">                are direct members of this module.</span>

<span class="ss">        Yields:</span>

<span class="ss">            torch.Tensor: module buffer</span>

<span class="ss">        Example::</span>

<span class="ss">            &gt;&gt;&gt; for buf in model.buffers():</span>

<span class="ss">            &gt;&gt;&gt;     print(type(buf), buf.size())</span>

<span class="ss">            &lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>

<span class="ss">            &lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="n">buf</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">yield</span><span class="w"> </span><span class="n">buf</span><span class="w"></span>
</code></pre></div>

</details>
<h5 id="children">children</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">children</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">_ForwardRef</span><span class="p">(</span><span class="s1">&#39;Module&#39;</span><span class="p">)]</span>
</code></pre></div>

<p>Returns an iterator over immediate children modules.</p>
<p>Yields:
    Module: a child module</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">children</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="o">[</span><span class="c">&#39;Module&#39;]:</span>

        <span class="n">r</span><span class="s">&quot;&quot;&quot;Returns an iterator over immediate children modules.</span>

<span class="s">        Yields:</span>

<span class="s">            Module: a child module</span>

<span class="s">        &quot;&quot;&quot;</span>

        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="k">module</span> <span class="nn">in</span> <span class="n">self</span><span class="p">.</span><span class="n">named_children</span><span class="p">():</span>

            <span class="n">yield</span> <span class="k">module</span>
</code></pre></div>

</details>
<h5 id="configure_apex">configure_apex</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">configure_apex</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">amp</span><span class="p">:</span><span class="nb">object</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span><span class="s1">&#39;LightningModule&#39;</span><span class="p">,</span>
    <span class="n">optimizers</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">],</span>
    <span class="n">amp_level</span><span class="p">:</span><span class="nb">str</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">_ForwardRef</span><span class="p">(</span><span class="s1">&#39;LightningModule&#39;</span><span class="p">),</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">]]</span>
</code></pre></div>

<p>Override to init AMP your own way.
Must return a model and list of optimizers.</p>
<p>Args:
    amp: pointer to amp library object.
    model: pointer to current :class:<code>LightningModule</code>.
    optimizers: list of optimizers passed in :meth:<code>configure_optimizers</code>.
    amp_level: AMP mode chosen ('O1', 'O2', etc...)</p>
<p>Return:
    Apex wrapped model and optimizers</p>
<p>Examples:
    .. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>    <span class="o">#</span> <span class="k">Default</span> <span class="k">implementation</span> <span class="n">used</span> <span class="k">by</span> <span class="n">Trainer</span><span class="p">.</span>
    <span class="n">def</span> <span class="n">configure_apex</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">amp</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">,</span> <span class="n">amp_level</span><span class="p">):</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">optimizers</span> <span class="o">=</span> <span class="n">amp</span><span class="p">.</span><span class="k">initialize</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">,</span> <span class="n">opt_level</span><span class="o">=</span><span class="n">amp_level</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizers</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">configure_apex</span><span class="p">(</span>

            <span class="n">self</span><span class="p">,</span>

            <span class="n">amp</span><span class="o">:</span> <span class="n">object</span><span class="p">,</span>

            <span class="n">model</span><span class="o">:</span> <span class="s1">&#39;LightningModule&#39;</span><span class="p">,</span>

            <span class="n">optimizers</span><span class="o">:</span> <span class="k">List</span><span class="err">[</span><span class="n">Optimizer</span><span class="err">]</span><span class="p">,</span>

            <span class="n">amp_level</span><span class="o">:</span> <span class="n">str</span>

    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="err">[</span><span class="s1">&#39;LightningModule&#39;</span><span class="p">,</span> <span class="k">List</span><span class="err">[</span><span class="n">Optimizer</span><span class="err">]]</span><span class="o">:</span>

        <span class="n">r</span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2"></span>

<span class="s2">        Override to init AMP your own way.</span>

<span class="s2">        Must return a model and list of optimizers.</span>

<span class="s2">        Args:</span>

<span class="s2">            amp: pointer to amp library object.</span>

<span class="s2">            model: pointer to current :class:`LightningModule`.</span>

<span class="s2">            optimizers: list of optimizers passed in :meth:`configure_optimizers`.</span>

<span class="s2">            amp_level: AMP mode chosen (&#39;O1&#39;, &#39;O2&#39;, etc...)</span>

<span class="s2">        Return:</span>

<span class="s2">            Apex wrapped model and optimizers</span>

<span class="s2">        Examples:</span>

<span class="s2">            .. code-block:: python</span>

<span class="s2">                # Default implementation used by Trainer.</span>

<span class="s2">                def configure_apex(self, amp, model, optimizers, amp_level):</span>

<span class="s2">                    model, optimizers = amp.initialize(</span>

<span class="s2">                        model, optimizers, opt_level=amp_level,</span>

<span class="s2">                    )</span>

<span class="s2">                    return model, optimizers</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="n">model</span><span class="p">,</span> <span class="n">optimizers</span> <span class="o">=</span> <span class="n">amp</span><span class="p">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">,</span> <span class="n">opt_level</span><span class="o">=</span><span class="n">amp_level</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizers</span>
</code></pre></div>

</details>
<h5 id="configure_ddp">configure_ddp</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">configure_ddp</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span><span class="s1">&#39;LightningModule&#39;</span><span class="p">,</span>
    <span class="n">device_ids</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedDataParallel</span>
</code></pre></div>

<p>Override to init DDP in your own way or with your own wrapper.
The only requirements are that:</p>
<ol>
<li>On a validation batch the call goes to <code>model.validation_step</code>.</li>
<li>On a training batch the call goes to <code>model.training_step</code>.</li>
<li>On a testing batch, the call goes to <code>model.test_step</code>.+</li>
</ol>
<p>Args:
    model: the :class:<code>LightningModule</code> currently being optimized.
    device_ids: the list of GPU ids.</p>
<p>Return:
    DDP wrapped model</p>
<p>Examples:
    .. code-block:: python</p>
<div class="codehilite"><pre><span></span><code><span class="err">    # default implementation used in Trainer</span>
<span class="err">    def configure_ddp(self, model, device_ids):</span>
<span class="err">        # Lightning DDP simply routes to test_step, val_step, etc...</span>
<span class="err">        model = LightningDistributedDataParallel(</span>
<span class="err">            model,</span>
<span class="err">            device_ids=device_ids,</span>
<span class="err">            find_unused_parameters=True</span>
<span class="err">        )</span>
<span class="err">        return model</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">configure_ddp</span><span class="p">(</span>

            <span class="n">self</span><span class="p">,</span>

            <span class="n">model</span><span class="o">:</span> <span class="s1">&#39;LightningModule&#39;</span><span class="p">,</span>

            <span class="n">device_ids</span><span class="o">:</span> <span class="k">List</span><span class="err">[</span><span class="kt">int</span><span class="err">]</span>

    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DistributedDataParallel</span><span class="o">:</span>

        <span class="n">r</span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2"></span>

<span class="s2">        Override to init DDP in your own way or with your own wrapper.</span>

<span class="s2">        The only requirements are that:</span>

<span class="s2">        1. On a validation batch the call goes to ``model.validation_step``.</span>

<span class="s2">        2. On a training batch the call goes to ``model.training_step``.</span>

<span class="s2">        3. On a testing batch, the call goes to ``model.test_step``.+</span>

<span class="s2">        Args:</span>

<span class="s2">            model: the :class:`LightningModule` currently being optimized.</span>

<span class="s2">            device_ids: the list of GPU ids.</span>

<span class="s2">        Return:</span>

<span class="s2">            DDP wrapped model</span>

<span class="s2">        Examples:</span>

<span class="s2">            .. code-block:: python</span>

<span class="s2">                # default implementation used in Trainer</span>

<span class="s2">                def configure_ddp(self, model, device_ids):</span>

<span class="s2">                    # Lightning DDP simply routes to test_step, val_step, etc...</span>

<span class="s2">                    model = LightningDistributedDataParallel(</span>

<span class="s2">                        model,</span>

<span class="s2">                        device_ids=device_ids,</span>

<span class="s2">                        find_unused_parameters=True</span>

<span class="s2">                    )</span>

<span class="s2">                    return model</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="n">model</span> <span class="o">=</span> <span class="n">LightningDistributedDataParallel</span><span class="p">(</span>

            <span class="n">model</span><span class="p">,</span>

            <span class="n">device_ids</span><span class="o">=</span><span class="n">device_ids</span><span class="p">,</span>

            <span class="n">find_unused_parameters</span><span class="o">=</span><span class="no">True</span>

        <span class="p">)</span>

        <span class="k">return</span> <span class="n">model</span>
</code></pre></div>

</details>
<h5 id="configure_optimizers">configure_optimizers</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">configure_optimizers</span><span class="p">(</span><span class="k">self</span><span class="p">):</span>

        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="k">in</span> <span class="k">self</span><span class="p">.</span><span class="k">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">]</span>

        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="k">self</span><span class="p">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="k">self</span><span class="p">.</span><span class="n">momentum</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="k">self</span><span class="p">.</span><span class="n">weight_decay</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">optimizer</span>
</code></pre></div>

</details>
<h5 id="cpu">cpu</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">cpu</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span>
</code></pre></div>

<p>Moves all model parameters and buffers to the CPU.
Returns:
    Module: self</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">cpu</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Module</span><span class="p">:</span>

        <span class="s">&quot;&quot;&quot;Moves all model parameters and buffers to the CPU.</span>

<span class="s">        Returns:</span>

<span class="s">            Module: self</span>

<span class="s">        &quot;&quot;&quot;</span>

        <span class="n">self</span><span class="p">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="c">&#39;cpu&#39;)</span>

        <span class="k">return</span> <span class="n">super</span><span class="p">().</span><span class="n">cpu</span><span class="p">()</span>
</code></pre></div>

</details>
<h5 id="cuda">cuda</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">cuda</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span>
</code></pre></div>

<p>Moves all model parameters and buffers to the GPU.
This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on GPU while being optimized.</p>
<p>Arguments:
    device: if specified, all parameters will be
        copied to that device</p>
<p>Returns:
    Module: self</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">cuda</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">device</span><span class="p">:</span><span class="w"> </span><span class="n">Optional</span><span class="o">[</span><span class="n">int</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="k">Module</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="ss">&quot;&quot;&quot;Moves all model parameters and buffers to the GPU.</span>

<span class="ss">        This also makes associated parameters and buffers different objects. So</span>

<span class="ss">        it should be called before constructing optimizer if the module will</span>

<span class="ss">        live on GPU while being optimized.</span>

<span class="ss">        Arguments:</span>

<span class="ss">            device: if specified, all parameters will be</span>

<span class="ss">                copied to that device</span>

<span class="ss">        Returns:</span>

<span class="ss">            Module: self</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">index</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">super</span><span class="p">().</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h5 id="double">double</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">double</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span>
</code></pre></div>

<p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p>
<p>Returns:
    Module: self</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="kt">double</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Module</span><span class="o">:</span>

        <span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Casts all floating point parameters and buffers to ``double`` datatype.</span>

<span class="s2">        Returns:</span>

<span class="s2">            Module: self</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="n">self</span><span class="p">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="kt">double</span>

        <span class="k">return</span> <span class="k">super</span><span class="p">().</span><span class="kt">double</span><span class="p">()</span>
</code></pre></div>

</details>
<h5 id="eval">eval</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">eval</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">:</span><span class="o">~</span><span class="n">T</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="o">~</span><span class="n">T</span>
</code></pre></div>

<p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>,
etc.</p>
<p>This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.</p>
<p>Returns:
    Module: self</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">eval</span><span class="p">(</span><span class="n">self</span><span class="o">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="o">:</span>

        <span class="n">r</span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Sets the module in evaluation mode.</span>

<span class="s2">        This has any effect only on certain modules. See documentations of</span>

<span class="s2">        particular modules for details of their behaviors in training/evaluation</span>

<span class="s2">        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,</span>

<span class="s2">        etc.</span>

<span class="s2">        This is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.</span>

<span class="s2">        Returns:</span>

<span class="s2">            Module: self</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="no">False</span><span class="p">)</span>
</code></pre></div>

</details>
<h5 id="extra_repr">extra_repr</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span>
</code></pre></div>

<p>Set the extra representation of the module</p>
<p>To print customized extra information, you should reimplement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">extra_repr</span><span class="p">(</span><span class="k">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">str</span><span class="p">:</span>

        <span class="n">r</span><span class="ss">&quot;&quot;&quot;Set the extra representation of the module</span>

<span class="ss">        To print customized extra information, you should reimplement</span>

<span class="ss">        this method in your own modules. Both single-line and multi-line</span>

<span class="ss">        strings are acceptable.</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="s1">&#39;&#39;</span>
</code></pre></div>

</details>
<h5 id="float">float</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">float</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span>
</code></pre></div>

<p>Casts all floating point parameters and buffers to float datatype.</p>
<p>Returns:
    Module: self</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">float</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Module</span><span class="p">:</span>

        <span class="s">&quot;&quot;&quot;Casts all floating point parameters and buffers to float datatype.</span>

<span class="s">        Returns:</span>

<span class="s">            Module: self</span>

<span class="s">        &quot;&quot;&quot;</span>

        <span class="n">self</span><span class="p">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">float</span>

        <span class="k">return</span> <span class="n">super</span><span class="p">().</span><span class="n">float</span><span class="p">()</span>
</code></pre></div>

</details>
<h5 id="forward">forward</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">images</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="err">@</span><span class="n">auto_move_data</span>

    <span class="n">def</span> <span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_rpn</span><span class="p">:</span>  <span class="c1"># step 2</span>

            <span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

            <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

            <span class="n">features</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">({</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">):</span> <span class="n">t</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">enumerate</span><span class="p">(</span><span class="n">features</span><span class="p">)})</span>

            <span class="n">images</span> <span class="o">=</span> <span class="n">ImageList</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="p">[(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)])</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;targets&quot;</span><span class="p">))</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_roi</span><span class="p">:</span>  <span class="c1"># step 3</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

            <span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

            <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

            <span class="n">features</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">({</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">):</span> <span class="n">t</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">enumerate</span><span class="p">(</span><span class="n">features</span><span class="p">)})</span>

            <span class="n">images</span> <span class="o">=</span> <span class="n">ImageList</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="p">[(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)])</span>

            <span class="n">proposals</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">None</span><span class="p">)</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">roi</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">proposals</span><span class="p">,</span> <span class="p">[(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)],</span> <span class="n">targets</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;targets&quot;</span><span class="p">))</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetune_rpn</span><span class="p">:</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">roi_heads</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;targets&quot;</span><span class="p">))</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetune_roi</span><span class="p">:</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">rpn</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;targets&quot;</span><span class="p">))</span>

        <span class="k">else</span><span class="p">:</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;targets&quot;</span><span class="p">))</span>
</code></pre></div>

</details>
<h5 id="freeze">freeze</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">freeze</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Freeze all params for inference.</p>
<p>Example:
    .. code-block:: python</p>
<div class="codehilite"><pre><span></span><code><span class="err">    model = MyLightningModule(...)</span>
<span class="err">    model.freeze()</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="k">freeze</span><span class="p">(</span><span class="k">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">None</span><span class="p">:</span>

        <span class="n">r</span><span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Freeze all params for inference.</span>

<span class="ss">        Example:</span>

<span class="ss">            .. code-block:: python</span>

<span class="ss">                model = MyLightningModule(...)</span>

<span class="ss">                model.freeze()</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="k">for</span> <span class="n">param</span> <span class="k">in</span> <span class="k">self</span><span class="p">.</span><span class="k">parameters</span><span class="p">():</span>

            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="k">False</span>

        <span class="k">self</span><span class="p">.</span><span class="n">eval</span><span class="p">()</span>
</code></pre></div>

</details>
<h5 id="get_progress_bar_dict">get_progress_bar_dict</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_progress_bar_dict</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span>
</code></pre></div>

<p>Additional items to be displayed in the progress bar.</p>
<p>Return:
    Dictionary with the items to be displayed in the progress bar.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">get_progress_bar_dict</span><span class="p">(</span><span class="k">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">str</span><span class="p">,</span> <span class="k">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">str</span><span class="p">]]:</span>

        <span class="n">r</span><span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Additional items to be displayed in the progress bar.</span>

<span class="ss">        Return:</span>

<span class="ss">            Dictionary with the items to be displayed in the progress bar.</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="o">#</span> <span class="k">call</span> <span class="p">.</span><span class="n">item</span><span class="p">()</span> <span class="k">only</span> <span class="n">once</span> <span class="n">but</span> <span class="n">store</span> <span class="n">elements</span> <span class="k">without</span> <span class="n">graphs</span>

        <span class="n">running_train_loss</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">trainer</span><span class="p">.</span><span class="n">running_loss</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="n">avg_training_loss</span> <span class="o">=</span> <span class="n">running_train_loss</span><span class="p">.</span><span class="n">cpu</span><span class="p">().</span><span class="n">item</span><span class="p">()</span> <span class="k">if</span> <span class="n">running_train_loss</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span> <span class="k">else</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;NaN&#39;</span><span class="p">)</span>

        <span class="n">tqdm_dict</span> <span class="o">=</span> <span class="err">{</span>

            <span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="s1">&#39;{:.3f}&#39;</span><span class="p">.</span><span class="n">format</span><span class="p">(</span><span class="n">avg_training_loss</span><span class="p">)</span>

        <span class="err">}</span>

        <span class="k">if</span> <span class="k">self</span><span class="p">.</span><span class="n">trainer</span><span class="p">.</span><span class="n">truncated_bptt_steps</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="p">:</span>

            <span class="n">tqdm_dict</span><span class="p">[</span><span class="s1">&#39;split_idx&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">trainer</span><span class="p">.</span><span class="n">split_idx</span>

        <span class="k">if</span> <span class="k">self</span><span class="p">.</span><span class="n">trainer</span><span class="p">.</span><span class="n">logger</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span> <span class="k">and</span> <span class="k">self</span><span class="p">.</span><span class="n">trainer</span><span class="p">.</span><span class="n">logger</span><span class="p">.</span><span class="k">version</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="p">:</span>

            <span class="n">tqdm_dict</span><span class="p">[</span><span class="s1">&#39;v_num&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">trainer</span><span class="p">.</span><span class="n">logger</span><span class="p">.</span><span class="k">version</span>

        <span class="k">return</span> <span class="n">tqdm_dict</span>
</code></pre></div>

</details>
<h5 id="get_tqdm_dict">get_tqdm_dict</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_tqdm_dict</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span>
</code></pre></div>

<p>Additional items to be displayed in the progress bar.</p>
<p>Return:
    Dictionary with the items to be displayed in the progress bar.</p>
<p>Warning:
    Deprecated since v0.7.3.
    Use :meth:<code>get_progress_bar_dict</code> instead.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">get_tqdm_dict</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="err">[</span><span class="n">str</span><span class="p">,</span> <span class="k">Union</span><span class="err">[</span><span class="kt">int</span><span class="p">,</span> <span class="n">str</span><span class="err">]]</span><span class="o">:</span>

        <span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2"></span>

<span class="s2">        Additional items to be displayed in the progress bar.</span>

<span class="s2">        Return:</span>

<span class="s2">            Dictionary with the items to be displayed in the progress bar.</span>

<span class="s2">        Warning:</span>

<span class="s2">            Deprecated since v0.7.3.</span>

<span class="s2">            Use :meth:`get_progress_bar_dict` instead.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="n">rank_zero_warn</span><span class="p">(</span><span class="s2">&quot;`get_tqdm_dict` was renamed to `get_progress_bar_dict` in v0.7.3&quot;</span>

                       <span class="s2">&quot; and this method will be removed in v1.0.0&quot;</span><span class="p">,</span> <span class="n">DeprecationWarning</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">get_progress_bar_dict</span><span class="p">()</span>
</code></pre></div>

</details>
<h5 id="grad_norm">grad_norm</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">grad_norm</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">norm_type</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span>
</code></pre></div>

<p>Compute each parameter's gradient's norm and their overall norm.</p>
<p>The overall norm is computed over all gradients together, as if they
were concatenated into a single vector.</p>
<p>Args:
    norm_type: The type of the used p-norm, cast to float if necessary.
        Can be <code>'inf'</code> for infinity norm.</p>
<p>Return:
    norms: The dictionary of p-norms of each parameter's gradient and
        a special entry for the total p-norm of the gradients viewed
        as a single vector.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">grad_norm</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">:</span> <span class="k">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>

        <span class="ss">&quot;&quot;&quot;Compute each parameter&#39;s gradient&#39;s norm and their overall norm.</span>

<span class="ss">        The overall norm is computed over all gradients together, as if they</span>

<span class="ss">        were concatenated into a single vector.</span>

<span class="ss">        Args:</span>

<span class="ss">            norm_type: The type of the used p-norm, cast to float if necessary.</span>

<span class="ss">                Can be ``&#39;inf&#39;`` for infinity norm.</span>

<span class="ss">        Return:</span>

<span class="ss">            norms: The dictionary of p-norms of each parameter&#39;s gradient and</span>

<span class="ss">                a special entry for the total p-norm of the gradients viewed</span>

<span class="ss">                as a single vector.</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">norm_type</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">norm_type</span><span class="p">)</span>

        <span class="n">norms</span><span class="p">,</span> <span class="n">all_norms</span> <span class="o">=</span> <span class="err">{}</span><span class="p">,</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">p</span> <span class="k">in</span> <span class="k">self</span><span class="p">.</span><span class="n">named_parameters</span><span class="p">():</span>

            <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="k">is</span> <span class="k">None</span><span class="p">:</span>

                <span class="k">continue</span>

            <span class="n">param_norm</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="k">data</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">norm_type</span><span class="p">))</span>

            <span class="n">norms</span><span class="p">[</span><span class="n">f</span><span class="s1">&#39;grad_{norm_type}_norm_{name}&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">round</span><span class="p">(</span><span class="n">param_norm</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

            <span class="n">all_norms</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">param_norm</span><span class="p">)</span>

        <span class="n">total_norm</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">all_norms</span><span class="p">).</span><span class="n">norm</span><span class="p">(</span><span class="n">norm_type</span><span class="p">))</span>

        <span class="n">norms</span><span class="p">[</span><span class="n">f</span><span class="s1">&#39;grad_{norm_type}_norm_total&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">round</span><span class="p">(</span><span class="n">total_norm</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">norms</span>
</code></pre></div>

</details>
<h5 id="half">half</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">half</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span>
</code></pre></div>

<p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p>
<p>Returns:
    Module: self</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">half</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Module</span><span class="o">:</span>

        <span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Casts all floating point parameters and buffers to ``half`` datatype.</span>

<span class="s2">        Returns:</span>

<span class="s2">            Module: self</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="n">self</span><span class="p">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">half</span>

        <span class="k">return</span> <span class="k">super</span><span class="p">().</span><span class="n">half</span><span class="p">()</span>
</code></pre></div>

</details>
<h5 id="init_ddp_connection">init_ddp_connection</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">init_ddp_connection</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">global_rank</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span>
    <span class="n">world_size</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span>
    <span class="n">is_slurm_managing_tasks</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Override to define your custom way of setting up a distributed environment.</p>
<p>Lightning's implementation uses env:// init by default and sets the first node as root
for SLURM managed cluster.</p>
<p>Args:
    global_rank: The global process idx.
    world_size: Number of GPUs being use across all nodes. (num_nodes * num_gpus).
    is_slurm_managing_tasks: is cluster managed by SLURM.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">init_ddp_connection</span><span class="p">(</span>

            <span class="bp">self</span><span class="p">,</span>

            <span class="n">global_rank</span><span class="p">:</span> <span class="nb nb-Type">int</span><span class="p">,</span>

            <span class="n">world_size</span><span class="p">:</span> <span class="nb nb-Type">int</span><span class="p">,</span>

            <span class="n">is_slurm_managing_tasks</span><span class="p">:</span> <span class="nb nb-Type">bool</span> <span class="o">=</span> <span class="n">True</span>

    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">None</span><span class="p">:</span>

        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Override to define your custom way of setting up a distributed environment.</span>

<span class="sd">        Lightning&#39;s implementation uses env:// init by default and sets the first node as root</span>

<span class="sd">        for SLURM managed cluster.</span>

<span class="sd">        Args:</span>

<span class="sd">            global_rank: The global process idx.</span>

<span class="sd">            world_size: Number of GPUs being use across all nodes. (num_nodes * num_gpus).</span>

<span class="sd">            is_slurm_managing_tasks: is cluster managed by SLURM.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">is_slurm_managing_tasks</span><span class="p">:</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_init_slurm_connection</span><span class="p">()</span>

        <span class="k">if</span> <span class="s1">&#39;MASTER_ADDR&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>

            <span class="n">rank_zero_warn</span><span class="p">(</span><span class="s2">&quot;MASTER_ADDR environment variable is not defined. Set as localhost&quot;</span><span class="p">)</span>

            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_ADDR&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;127.0.0.1&#39;</span>

        <span class="nb">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;MASTER_ADDR: {os.environ[&#39;MASTER_ADDR&#39;]}&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s1">&#39;MASTER_PORT&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>

            <span class="n">rank_zero_warn</span><span class="p">(</span><span class="s2">&quot;MASTER_PORT environment variable is not defined. Set as 12910&quot;</span><span class="p">)</span>

            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_PORT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;12910&#39;</span>

        <span class="nb">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;MASTER_PORT: {os.environ[&#39;MASTER_PORT&#39;]}&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s1">&#39;WORLD_SIZE&#39;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span> <span class="ow">and</span> <span class="nb nb-Type">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;WORLD_SIZE&#39;</span><span class="p">])</span> <span class="o">!=</span> <span class="n">world_size</span><span class="p">:</span>

            <span class="n">rank_zero_warn</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;WORLD_SIZE environment variable ({os.environ[&#39;WORLD_SIZE&#39;]}) &quot;</span>

                           <span class="n">f</span><span class="s2">&quot;is not equal to the computed world size ({world_size}). Ignored.&quot;</span><span class="p">)</span>

        <span class="n">torch_backend</span> <span class="o">=</span> <span class="s2">&quot;nccl&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">on_gpu</span> <span class="k">else</span> <span class="s2">&quot;gloo&quot;</span>

        <span class="nb">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;initializing ddp: GLOBAL_RANK: {global_rank}, MEMBER: {global_rank+1}/{world_size}&quot;</span><span class="p">)</span>

        <span class="n">torch_distrib</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">torch_backend</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">global_rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>
</code></pre></div>

</details>
<h5 id="load_state_dict">load_state_dict</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">:</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">strict</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</code></pre></div>

<p>Copies parameters and buffers from :attr:<code>state_dict</code> into
this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then
the keys of :attr:<code>state_dict</code> must exactly match the keys returned
by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p>
<p>Arguments:
    state_dict (dict): a dict containing parameters and
        persistent buffers.
    strict (bool, optional): whether to strictly enforce that the keys
        in :attr:<code>state_dict</code> match the keys returned by this module's
        :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code></p>
<p>Returns:
    <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:
        * <strong>missing_keys</strong> is a list of str containing the missing keys
        * <strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="s s-Atom">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="s s-Atom">self</span><span class="p">,</span> <span class="s s-Atom">state_dict:</span> <span class="nv">Union</span><span class="p">[</span><span class="nv">Dict</span><span class="p">[</span><span class="s s-Atom">str</span><span class="p">,</span> <span class="nv">Tensor</span><span class="p">],</span> <span class="nv">Dict</span><span class="p">[</span><span class="s s-Atom">str</span><span class="p">,</span> <span class="nv">Tensor</span><span class="p">]],</span>

                        <span class="nn">strict</span><span class="p">:</span> <span class="s s-Atom">bool</span> <span class="o">=</span> <span class="nv">True</span><span class="p">)</span><span class="s s-Atom">:</span>

        <span class="s s-Atom">r</span><span class="s2">&quot;&quot;&quot;Copies parameters and buffers from :attr:`state_dict` into</span>

<span class="s2">        this module and its descendants. If :attr:`strict` is ``True``, then</span>

<span class="s2">        the keys of :attr:`state_dict` must exactly match the keys returned</span>

<span class="s2">        by this module&#39;s :meth:`~torch.nn.Module.state_dict` function.</span>

<span class="s2">        Arguments:</span>

<span class="s2">            state_dict (dict): a dict containing parameters and</span>

<span class="s2">                persistent buffers.</span>

<span class="s2">            strict (bool, optional): whether to strictly enforce that the keys</span>

<span class="s2">                in :attr:`state_dict` match the keys returned by this module&#39;s</span>

<span class="s2">                :meth:`~torch.nn.Module.state_dict` function. Default: ``True``</span>

<span class="s2">        Returns:</span>

<span class="s2">            ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:</span>

<span class="s2">                * **missing_keys** is a list of str containing the missing keys</span>

<span class="s2">                * **unexpected_keys** is a list of str containing the unexpected keys</span>

<span class="s2">        &quot;&quot;&quot;</span>

        <span class="s s-Atom">missing_keys</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="s s-Atom">unexpected_keys</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="s s-Atom">error_msgs</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="s s-Atom">#</span> <span class="s s-Atom">copy</span> <span class="s s-Atom">state_dict</span> <span class="s s-Atom">so</span> <span class="k">_</span><span class="s s-Atom">load_from_state_dict</span> <span class="s s-Atom">can</span> <span class="s s-Atom">modify</span> <span class="s s-Atom">it</span>

        <span class="s s-Atom">metadata</span> <span class="o">=</span> <span class="nf">getattr</span><span class="p">(</span><span class="s s-Atom">state_dict</span><span class="p">,</span> <span class="s s-Atom">&#39;_metadata&#39;</span><span class="p">,</span> <span class="nv">None</span><span class="p">)</span>

        <span class="s s-Atom">state_dict</span> <span class="o">=</span> <span class="s s-Atom">state_dict</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

        <span class="s s-Atom">if</span> <span class="s s-Atom">metadata</span> <span class="o">is</span> <span class="o">not</span> <span class="nv">None</span><span class="s s-Atom">:</span>

            <span class="s s-Atom">state_dict</span><span class="p">.</span><span class="k">_</span><span class="s s-Atom">metadata</span> <span class="o">=</span> <span class="s s-Atom">metadata</span>

        <span class="s s-Atom">def</span> <span class="nf">load</span><span class="p">(</span><span class="s s-Atom">module</span><span class="p">,</span> <span class="s s-Atom">prefix=&#39;&#39;</span><span class="p">)</span><span class="s s-Atom">:</span>

            <span class="s s-Atom">local_metadata</span> <span class="o">=</span> <span class="p">{}</span> <span class="s s-Atom">if</span> <span class="s s-Atom">metadata</span> <span class="o">is</span> <span class="nv">None</span> <span class="s s-Atom">else</span> <span class="s s-Atom">metadata</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="s s-Atom">prefix</span><span class="p">[:-</span><span class="mi">1</span><span class="p">],</span> <span class="p">{})</span>

            <span class="s s-Atom">module</span><span class="p">.</span><span class="k">_</span><span class="nf">load_from_state_dict</span><span class="p">(</span>

                <span class="s s-Atom">state_dict</span><span class="p">,</span> <span class="s s-Atom">prefix</span><span class="p">,</span> <span class="s s-Atom">local_metadata</span><span class="p">,</span> <span class="nv">True</span><span class="p">,</span> <span class="s s-Atom">missing_keys</span><span class="p">,</span> <span class="s s-Atom">unexpected_keys</span><span class="p">,</span> <span class="s s-Atom">error_msgs</span><span class="p">)</span>

            <span class="s s-Atom">for</span> <span class="s s-Atom">name</span><span class="p">,</span> <span class="s s-Atom">child</span> <span class="s s-Atom">in</span> <span class="s s-Atom">module</span><span class="p">.</span><span class="k">_</span><span class="s s-Atom">modules</span><span class="p">.</span><span class="nf">items</span><span class="p">()</span><span class="s s-Atom">:</span>

                <span class="s s-Atom">if</span> <span class="s s-Atom">child</span> <span class="o">is</span> <span class="o">not</span> <span class="nv">None</span><span class="s s-Atom">:</span>

                    <span class="nf">load</span><span class="p">(</span><span class="s s-Atom">child</span><span class="p">,</span> <span class="s s-Atom">prefix</span> <span class="o">+</span> <span class="s s-Atom">name</span> <span class="o">+</span> <span class="s s-Atom">&#39;.&#39;</span><span class="p">)</span>

        <span class="nf">load</span><span class="p">(</span><span class="s s-Atom">self</span><span class="p">)</span>

        <span class="s s-Atom">load</span> <span class="o">=</span> <span class="nv">None</span>  <span class="s s-Atom">#</span> <span class="s s-Atom">break</span> <span class="s s-Atom">load-&gt;load</span> <span class="s s-Atom">reference</span> <span class="s s-Atom">cycle</span>

        <span class="s s-Atom">if</span> <span class="nn">strict</span><span class="p">:</span>

            <span class="s s-Atom">if</span> <span class="nf">len</span><span class="p">(</span><span class="s s-Atom">unexpected_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="s s-Atom">:</span>

                <span class="s s-Atom">error_msgs</span><span class="p">.</span><span class="nf">insert</span><span class="p">(</span>

                    <span class="mi">0</span><span class="p">,</span> <span class="s s-Atom">&#39;Unexpected key(s) in state_dict: {}. &#39;</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span>

                        <span class="s s-Atom">&#39;, &#39;</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="s s-Atom">&#39;&quot;{}&quot;&#39;</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="s s-Atom">k</span><span class="p">)</span> <span class="s s-Atom">for</span> <span class="s s-Atom">k</span> <span class="s s-Atom">in</span> <span class="s s-Atom">unexpected_keys</span><span class="p">)))</span>

            <span class="s s-Atom">if</span> <span class="nf">len</span><span class="p">(</span><span class="s s-Atom">missing_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="s s-Atom">:</span>

                <span class="s s-Atom">error_msgs</span><span class="p">.</span><span class="nf">insert</span><span class="p">(</span>

                    <span class="mi">0</span><span class="p">,</span> <span class="s s-Atom">&#39;Missing key(s) in state_dict: {}. &#39;</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span>

                        <span class="s s-Atom">&#39;, &#39;</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="s s-Atom">&#39;&quot;{}&quot;&#39;</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="s s-Atom">k</span><span class="p">)</span> <span class="s s-Atom">for</span> <span class="s s-Atom">k</span> <span class="s s-Atom">in</span> <span class="s s-Atom">missing_keys</span><span class="p">)))</span>

        <span class="s s-Atom">if</span> <span class="nf">len</span><span class="p">(</span><span class="s s-Atom">error_msgs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="s s-Atom">:</span>

            <span class="s s-Atom">raise</span> <span class="nv">RuntimeError</span><span class="p">(</span><span class="s s-Atom">&#39;Error(s) in loading state_dict for {}:\n\t{}&#39;</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span>

                               <span class="s s-Atom">self</span><span class="p">.</span><span class="k">__</span><span class="s s-Atom">class__</span><span class="p">.</span><span class="k">__</span><span class="s s-Atom">name__</span><span class="p">,</span> <span class="s2">&quot;\n\t&quot;</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="s s-Atom">error_msgs</span><span class="p">)))</span>

        <span class="s s-Atom">return</span> <span class="k">_</span><span class="nv">IncompatibleKeys</span><span class="p">(</span><span class="s s-Atom">missing_keys</span><span class="p">,</span> <span class="s s-Atom">unexpected_keys</span><span class="p">)</span>
</code></pre></div>

</details>
<h5 id="modules">modules</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">modules</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">_ForwardRef</span><span class="p">(</span><span class="s1">&#39;Module&#39;</span><span class="p">)]</span>
</code></pre></div>

<p>Returns an iterator over all modules in the network.</p>
<p>Yields:
    Module: a module in the network</p>
<p>Note:
    Duplicate modules are returned only once. In the following
    example, <code>l</code> will be returned only once.</p>
<p>Example::</p>
<div class="codehilite"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">modules</span><span class="p">()):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>

<span class="mi">0</span> <span class="o">-&gt;</span> <span class="n">Sequential</span><span class="p">(</span>
  <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">True</span><span class="p">)</span>
  <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">True</span><span class="p">)</span>
<span class="p">)</span>
<span class="mi">1</span> <span class="o">-&gt;</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">True</span><span class="p">)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">modules</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="err">[</span><span class="s1">&#39;Module&#39;</span><span class="err">]</span><span class="o">:</span>

        <span class="n">r</span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Returns an iterator over all modules in the network.</span>

<span class="s2">        Yields:</span>

<span class="s2">            Module: a module in the network</span>

<span class="s2">        Note:</span>

<span class="s2">            Duplicate modules are returned only once. In the following</span>

<span class="s2">            example, ``l`` will be returned only once.</span>

<span class="s2">        Example::</span>

<span class="s2">            &gt;&gt;&gt; l = nn.Linear(2, 2)</span>

<span class="s2">            &gt;&gt;&gt; net = nn.Sequential(l, l)</span>

<span class="s2">            &gt;&gt;&gt; for idx, m in enumerate(net.modules()):</span>

<span class="s2">                    print(idx, &#39;-&gt;&#39;, m)</span>

<span class="s2">            0 -&gt; Sequential(</span>

<span class="s2">              (0): Linear(in_features=2, out_features=2, bias=True)</span>

<span class="s2">              (1): Linear(in_features=2, out_features=2, bias=True)</span>

<span class="s2">            )</span>

<span class="s2">            1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="k">for</span> <span class="k">name</span><span class="p">,</span> <span class="n">module</span> <span class="k">in</span> <span class="n">self</span><span class="p">.</span><span class="n">named_modules</span><span class="p">()</span><span class="o">:</span>

            <span class="n">yield</span> <span class="n">module</span>
</code></pre></div>

</details>
<h5 id="named_buffers">named_buffers</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">named_buffers</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prefix</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span>
    <span class="n">recurse</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
</code></pre></div>

<p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>
<p>Args:
    prefix (str): prefix to prepend to all buffer names.
    recurse (bool): if True, then yields buffers of this module
        and all submodules. Otherwise, yields only buffers that
        are direct members of this module.</p>
<p>Yields:
    (string, torch.Tensor): Tuple containing the name and buffer</p>
<p>Example::</p>
<div class="codehilite"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
<span class="o">&gt;&gt;&gt;</span>    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;running_var&#39;</span><span class="p">]:</span>
<span class="o">&gt;&gt;&gt;</span>        <span class="nb">print</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">named_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb nb-Type">bool</span> <span class="o">=</span> <span class="n">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>

        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns an iterator over module buffers, yielding both the</span>

<span class="sd">        name of the buffer as well as the buffer itself.</span>

<span class="sd">        Args:</span>

<span class="sd">            prefix (str): prefix to prepend to all buffer names.</span>

<span class="sd">            recurse (bool): if True, then yields buffers of this module</span>

<span class="sd">                and all submodules. Otherwise, yields only buffers that</span>

<span class="sd">                are direct members of this module.</span>

<span class="sd">        Yields:</span>

<span class="sd">            (string, torch.Tensor): Tuple containing the name and buffer</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; for name, buf in self.named_buffers():</span>

<span class="sd">            &gt;&gt;&gt;    if name in [&#39;running_var&#39;]:</span>

<span class="sd">            &gt;&gt;&gt;        print(buf.size())</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span>

            <span class="n">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="n">module</span><span class="o">.</span><span class="n">_buffers</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>

            <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">gen</span><span class="p">:</span>

            <span class="nb">yield</span> <span class="n">elem</span>
</code></pre></div>

</details>
<h5 id="named_children">named_children</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">named_children</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_ForwardRef</span><span class="p">(</span><span class="s1">&#39;Module&#39;</span><span class="p">)]]</span>
</code></pre></div>

<p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<p>Yields:
    (string, Module): Tuple containing a name and child module</p>
<p>Example::</p>
<div class="codehilite"><pre><span></span><code><span class="err">&gt;&gt;&gt; for name, module in model.named_children():</span>
<span class="err">&gt;&gt;&gt;     if name in [&#39;conv4&#39;, &#39;conv5&#39;]:</span>
<span class="err">&gt;&gt;&gt;         print(module)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">named_children</span><span class="p">(</span><span class="k">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">str</span><span class="p">,</span> <span class="s1">&#39;Module&#39;</span><span class="p">]]:</span>

        <span class="n">r</span><span class="ss">&quot;&quot;&quot;Returns an iterator over immediate children modules, yielding both</span>

<span class="ss">        the name of the module as well as the module itself.</span>

<span class="ss">        Yields:</span>

<span class="ss">            (string, Module): Tuple containing a name and child module</span>

<span class="ss">        Example::</span>

<span class="ss">            &gt;&gt;&gt; for name, module in model.named_children():</span>

<span class="ss">            &gt;&gt;&gt;     if name in [&#39;conv4&#39;, &#39;conv5&#39;]:</span>

<span class="ss">            &gt;&gt;&gt;         print(module)</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">memo</span> <span class="o">=</span> <span class="k">set</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="k">in</span> <span class="k">self</span><span class="p">.</span><span class="n">_modules</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>

            <span class="k">if</span> <span class="n">module</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span> <span class="k">and</span> <span class="n">module</span> <span class="k">not</span> <span class="k">in</span> <span class="n">memo</span><span class="p">:</span>

                <span class="n">memo</span><span class="p">.</span><span class="k">add</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

                <span class="n">yield</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span>
</code></pre></div>

</details>
<h5 id="named_modules">named_modules</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">named_modules</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">memo</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="n">_ForwardRef</span><span class="p">(</span><span class="s1">&#39;Module&#39;</span><span class="p">)],</span> <span class="n">NoneType</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">prefix</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="s1">&#39;&#39;</span>
<span class="p">)</span>
</code></pre></div>

<p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<p>Yields:
    (string, Module): Tuple of name and module</p>
<p>Note:
    Duplicate modules are returned only once. In the following
    example, <code>l</code> will be returned only once.</p>
<p>Example::</p>
<div class="codehilite"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>

<span class="mi">0</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">Sequential</span><span class="p">(</span>
  <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">True</span><span class="p">)</span>
  <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">True</span><span class="p">)</span>
<span class="p">))</span>
<span class="mi">1</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">True</span><span class="p">))</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">named_modules</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">memo</span><span class="o">:</span> <span class="k">Optional</span><span class="err">[</span><span class="k">Set</span><span class="err">[</span><span class="s1">&#39;Module&#39;</span><span class="err">]]</span> <span class="o">=</span> <span class="k">None</span><span class="p">,</span> <span class="n">prefix</span><span class="o">:</span> <span class="n">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">:</span>

        <span class="n">r</span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Returns an iterator over all modules in the network, yielding</span>

<span class="s2">        both the name of the module as well as the module itself.</span>

<span class="s2">        Yields:</span>

<span class="s2">            (string, Module): Tuple of name and module</span>

<span class="s2">        Note:</span>

<span class="s2">            Duplicate modules are returned only once. In the following</span>

<span class="s2">            example, ``l`` will be returned only once.</span>

<span class="s2">        Example::</span>

<span class="s2">            &gt;&gt;&gt; l = nn.Linear(2, 2)</span>

<span class="s2">            &gt;&gt;&gt; net = nn.Sequential(l, l)</span>

<span class="s2">            &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):</span>

<span class="s2">                    print(idx, &#39;-&gt;&#39;, m)</span>

<span class="s2">            0 -&gt; (&#39;&#39;, Sequential(</span>

<span class="s2">              (0): Linear(in_features=2, out_features=2, bias=True)</span>

<span class="s2">              (1): Linear(in_features=2, out_features=2, bias=True)</span>

<span class="s2">            ))</span>

<span class="s2">            1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="k">if</span> <span class="n">memo</span> <span class="k">is</span> <span class="k">None</span><span class="o">:</span>

            <span class="n">memo</span> <span class="o">=</span> <span class="kt">set</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">self</span> <span class="k">not</span> <span class="k">in</span> <span class="n">memo</span><span class="o">:</span>

            <span class="n">memo</span><span class="p">.</span><span class="k">add</span><span class="p">(</span><span class="n">self</span><span class="p">)</span>

            <span class="n">yield</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">self</span>

            <span class="k">for</span> <span class="k">name</span><span class="p">,</span> <span class="n">module</span> <span class="k">in</span> <span class="n">self</span><span class="p">.</span><span class="n">_modules</span><span class="p">.</span><span class="n">items</span><span class="p">()</span><span class="o">:</span>

                <span class="k">if</span> <span class="n">module</span> <span class="k">is</span> <span class="k">None</span><span class="o">:</span>

                    <span class="k">continue</span>

                <span class="n">submodule_prefix</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="p">(</span><span class="s1">&#39;.&#39;</span> <span class="k">if</span> <span class="n">prefix</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="k">name</span>

                <span class="k">for</span> <span class="n">m</span> <span class="k">in</span> <span class="n">module</span><span class="p">.</span><span class="n">named_modules</span><span class="p">(</span><span class="n">memo</span><span class="p">,</span> <span class="n">submodule_prefix</span><span class="p">)</span><span class="o">:</span>

                    <span class="n">yield</span> <span class="n">m</span>
</code></pre></div>

</details>
<h5 id="named_parameters">named_parameters</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">named_parameters</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prefix</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span>
    <span class="n">recurse</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
</code></pre></div>

<p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>
<p>Args:
    prefix (str): prefix to prepend to all parameter names.
    recurse (bool): if True, then yields parameters of this module
        and all submodules. Otherwise, yields only parameters that
        are direct members of this module.</p>
<p>Yields:
    (string, Parameter): Tuple containing the name and parameter</p>
<p>Example::</p>
<div class="codehilite"><pre><span></span><code><span class="err">&gt;&gt;&gt; for name, param in self.named_parameters():</span>
<span class="err">&gt;&gt;&gt;    if name in [&#39;bias&#39;]:</span>
<span class="err">&gt;&gt;&gt;        print(param.size())</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">named_parameters</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="k">prefix</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="k">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>

        <span class="n">r</span><span class="ss">&quot;&quot;&quot;Returns an iterator over module parameters, yielding both the</span>

<span class="ss">        name of the parameter as well as the parameter itself.</span>

<span class="ss">        Args:</span>

<span class="ss">            prefix (str): prefix to prepend to all parameter names.</span>

<span class="ss">            recurse (bool): if True, then yields parameters of this module</span>

<span class="ss">                and all submodules. Otherwise, yields only parameters that</span>

<span class="ss">                are direct members of this module.</span>

<span class="ss">        Yields:</span>

<span class="ss">            (string, Parameter): Tuple containing the name and parameter</span>

<span class="ss">        Example::</span>

<span class="ss">            &gt;&gt;&gt; for name, param in self.named_parameters():</span>

<span class="ss">            &gt;&gt;&gt;    if name in [&#39;bias&#39;]:</span>

<span class="ss">            &gt;&gt;&gt;        print(param.size())</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">gen</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_named_members</span><span class="p">(</span>

            <span class="n">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="n">module</span><span class="p">.</span><span class="n">_parameters</span><span class="p">.</span><span class="n">items</span><span class="p">(),</span>

            <span class="k">prefix</span><span class="o">=</span><span class="k">prefix</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">elem</span> <span class="k">in</span> <span class="n">gen</span><span class="p">:</span>

            <span class="n">yield</span> <span class="n">elem</span>
</code></pre></div>

</details>
<h5 id="on_after_backward">on_after_backward</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">on_after_backward</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Called in the training loop after loss.backward() and before optimizers do anything.
This is the ideal place to inspect or log gradient information.</p>
<p>Example::</p>
<div class="codehilite"><pre><span></span><code><span class="err">def on_after_backward(self):</span>
<span class="err">    # example to inspect gradient information in tensorboard</span>
<span class="err">    if self.trainer.global_step % 25 == 0:  # don&#39;t make the tf file huge</span>
<span class="err">        params = self.state_dict()</span>
<span class="err">        for k, v in params.items():</span>
<span class="err">            grads = v</span>
<span class="err">            name = k</span>
<span class="err">            self.logger.experiment.add_histogram(tag=name, values=grads,</span>
<span class="err">                                                 global_step=self.trainer.global_step)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">on_after_backward</span><span class="p">(</span><span class="k">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">None</span><span class="p">:</span>

        <span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Called in the training loop after loss.backward() and before optimizers do anything.</span>

<span class="ss">        This is the ideal place to inspect or log gradient information.</span>

<span class="ss">        Example::</span>

<span class="ss">            def on_after_backward(self):</span>

<span class="ss">                # example to inspect gradient information in tensorboard</span>

<span class="ss">                if self.trainer.global_step % 25 == 0:  # don&#39;t make the tf file huge</span>

<span class="ss">                    params = self.state_dict()</span>

<span class="ss">                    for k, v in params.items():</span>

<span class="ss">                        grads = v</span>

<span class="ss">                        name = k</span>

<span class="ss">                        self.logger.experiment.add_histogram(tag=name, values=grads,</span>

<span class="ss">                                                             global_step=self.trainer.global_step)</span>

<span class="ss">        &quot;&quot;&quot;</span>
</code></pre></div>

</details>
<h5 id="on_batch_end">on_batch_end</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">on_batch_end</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Called in the training loop after the batch.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">on_batch_end</span><span class="p">(</span><span class="k">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">None</span><span class="p">:</span>

        <span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Called in the training loop after the batch.</span>

<span class="ss">        &quot;&quot;&quot;</span>
</code></pre></div>

</details>
<h5 id="on_batch_start">on_batch_start</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">on_batch_start</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">batch</span><span class="p">:</span><span class="n">Any</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Called in the training loop before anything happens for that batch.</p>
<p>If you return -1 here, you will skip training for the rest of the current epoch.</p>
<p>Args:
    batch: The batched data as it is returned by the training DataLoader.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">on_batch_start</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="k">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">None</span><span class="p">:</span>

        <span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Called in the training loop before anything happens for that batch.</span>

<span class="ss">        If you return -1 here, you will skip training for the rest of the current epoch.</span>

<span class="ss">        Args:</span>

<span class="ss">            batch: The batched data as it is returned by the training DataLoader.</span>

<span class="ss">        &quot;&quot;&quot;</span>
</code></pre></div>

</details>
<h5 id="on_before_zero_grad">on_before_zero_grad</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">on_before_zero_grad</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Called after optimizer.step() and before optimizer.zero_grad().</p>
<p>Called in the training loop after taking an optimizer step and before zeroing grads.
Good place to inspect weight information with weights updated.</p>
<p>This is where it is called::</p>
<div class="codehilite"><pre><span></span><code><span class="err">for optimizer in optimizers:</span>
<span class="err">    optimizer.step()</span>
<span class="err">    model.on_before_zero_grad(optimizer) # &lt; ---- called here</span>
<span class="err">    optimizer.zero_grad</span>
</code></pre></div>

<p>Args:
    optimizer: The optimizer for which grads should be zeroed.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">on_before_zero_grad</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">None</span><span class="p">:</span>

        <span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Called after optimizer.step() and before optimizer.zero_grad().</span>

<span class="ss">        Called in the training loop after taking an optimizer step and before zeroing grads.</span>

<span class="ss">        Good place to inspect weight information with weights updated.</span>

<span class="ss">        This is where it is called::</span>

<span class="ss">            for optimizer in optimizers:</span>

<span class="ss">                optimizer.step()</span>

<span class="ss">                model.on_before_zero_grad(optimizer) # &lt; ---- called here</span>

<span class="ss">                optimizer.zero_grad</span>

<span class="ss">        Args:</span>

<span class="ss">            optimizer: The optimizer for which grads should be zeroed.</span>

<span class="ss">        &quot;&quot;&quot;</span>
</code></pre></div>

</details>
<h5 id="on_epoch_end">on_epoch_end</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">on_epoch_end</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Called in the training loop at the very end of the epoch.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">on_epoch_end</span><span class="p">(</span><span class="k">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">None</span><span class="p">:</span>

        <span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Called in the training loop at the very end of the epoch.</span>

<span class="ss">        &quot;&quot;&quot;</span>
</code></pre></div>

</details>
<h5 id="on_epoch_start">on_epoch_start</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">on_epoch_start</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Called in the training loop at the very beginning of the epoch.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">on_epoch_start</span><span class="p">(</span><span class="k">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">None</span><span class="p">:</span>

        <span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Called in the training loop at the very beginning of the epoch.</span>

<span class="ss">        &quot;&quot;&quot;</span>
</code></pre></div>

</details>
<h5 id="on_fit_end">on_fit_end</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">on_fit_end</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>

<p>Called at the very end of fit.
If on DDP it is called on every process</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">on_fit_end</span><span class="p">(</span><span class="k">self</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Called at the very end of fit.</span>

<span class="ss">        If on DDP it is called on every process</span>

<span class="ss">        &quot;&quot;&quot;</span>
</code></pre></div>

</details>
<h5 id="on_fit_start">on_fit_start</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">on_fit_start</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>

<p>Called at the very beginning of fit.
If on DDP it is called on every process</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">on_fit_start</span><span class="p">(</span><span class="k">self</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Called at the very beginning of fit.</span>

<span class="ss">        If on DDP it is called on every process</span>

<span class="ss">        &quot;&quot;&quot;</span>
</code></pre></div>

</details>
<h5 id="on_hpc_load">on_hpc_load</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">on_hpc_load</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">checkpoint</span><span class="p">:</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Hook to do whatever you need right before Slurm manager loads the model.</p>
<p>Args:
    checkpoint: A dictionary with variables from the checkpoint.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">on_hpc_load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">None</span><span class="p">:</span>

        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Hook to do whatever you need right before Slurm manager loads the model.</span>

<span class="sd">        Args:</span>

<span class="sd">            checkpoint: A dictionary with variables from the checkpoint.</span>

<span class="sd">        &quot;&quot;&quot;</span>
</code></pre></div>

</details>
<h5 id="on_hpc_save">on_hpc_save</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">on_hpc_save</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">checkpoint</span><span class="p">:</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Hook to do whatever you need right before Slurm manager saves the model.</p>
<p>Args:
    checkpoint: A dictionary in which you can save variables to save in a checkpoint.
        Contents need to be pickleable.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">on_hpc_save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">None</span><span class="p">:</span>

        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Hook to do whatever you need right before Slurm manager saves the model.</span>

<span class="sd">        Args:</span>

<span class="sd">            checkpoint: A dictionary in which you can save variables to save in a checkpoint.</span>

<span class="sd">                Contents need to be pickleable.</span>

<span class="sd">        &quot;&quot;&quot;</span>
</code></pre></div>

</details>
<h5 id="on_load_checkpoint">on_load_checkpoint</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">on_load_checkpoint</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">checkpoint</span><span class="p">:</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Called by Lightning to restore your model.
If you saved something with :meth:<code>on_save_checkpoint</code> this is your chance to restore this.</p>
<p>Args:
    checkpoint: Loaded checkpoint</p>
<p>Example:
    .. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">on_load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
        <span class="c1"># 99% of the time you don&#39;t need to implement this method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">something_cool_i_want_to_save</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span>
</code></pre></div>

<p>Note:
    Lightning auto-restores global step, epoch, and train state including amp scaling.
    There is no need for you to restore anything regarding training.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">on_load_checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="o">:</span> <span class="n">Dict</span><span class="err">[</span><span class="n">str</span><span class="p">,</span> <span class="k">Any</span><span class="err">]</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">None</span><span class="o">:</span>

        <span class="n">r</span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2"></span>

<span class="s2">        Called by Lightning to restore your model.</span>

<span class="s2">        If you saved something with :meth:`on_save_checkpoint` this is your chance to restore this.</span>

<span class="s2">        Args:</span>

<span class="s2">            checkpoint: Loaded checkpoint</span>

<span class="s2">        Example:</span>

<span class="s2">            .. code-block:: python</span>

<span class="s2">                def on_load_checkpoint(self, checkpoint):</span>

<span class="s2">                    # 99% of the time you don&#39;t need to implement this method</span>

<span class="s2">                    self.something_cool_i_want_to_save = checkpoint[&#39;something_cool_i_want_to_save&#39;]</span>

<span class="s2">        Note:</span>

<span class="s2">            Lightning auto-restores global step, epoch, and train state including amp scaling.</span>

<span class="s2">            There is no need for you to restore anything regarding training.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>
</code></pre></div>

</details>
<h5 id="on_post_performance_check">on_post_performance_check</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">on_post_performance_check</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Called at the very end of the validation loop.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">on_post_performance_check</span><span class="p">(</span><span class="k">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">None</span><span class="p">:</span>

        <span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Called at the very end of the validation loop.</span>

<span class="ss">        &quot;&quot;&quot;</span>
</code></pre></div>

</details>
<h5 id="on_pre_performance_check">on_pre_performance_check</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">on_pre_performance_check</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Called at the very beginning of the validation loop.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">on_pre_performance_check</span><span class="p">(</span><span class="k">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">None</span><span class="p">:</span>

        <span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Called at the very beginning of the validation loop.</span>

<span class="ss">        &quot;&quot;&quot;</span>
</code></pre></div>

</details>
<h5 id="on_sanity_check_start">on_sanity_check_start</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">on_sanity_check_start</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>

<p>Called before starting evaluation.</p>
<p>Warning:
    Deprecated. Will be removed in v0.9.0.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">on_sanity_check_start</span><span class="p">(</span><span class="k">self</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Called before starting evaluation.</span>

<span class="ss">        Warning:</span>

<span class="ss">            Deprecated. Will be removed in v0.9.0.</span>

<span class="ss">        &quot;&quot;&quot;</span>
</code></pre></div>

</details>
<h5 id="on_save_checkpoint">on_save_checkpoint</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">on_save_checkpoint</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">checkpoint</span><span class="p">:</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Called by Lightning when saving a checkpoint to give you a chance to store anything
else you might want to save.</p>
<p>Args:
    checkpoint: Checkpoint to be saved</p>
<p>Example:
    .. code-block:: python</p>
<div class="codehilite"><pre><span></span><code><span class="err">    def on_save_checkpoint(self, checkpoint):</span>
<span class="err">        # 99% of use cases you don&#39;t need to implement this method</span>
<span class="err">        checkpoint[&#39;something_cool_i_want_to_save&#39;] = my_cool_pickable_object</span>
</code></pre></div>

<p>Note:
    Lightning saves all aspects of training (epoch, global step, etc...)
    including amp scaling.
    There is no need for you to store anything about training.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">on_save_checkpoint</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="k">checkpoint</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">str</span><span class="p">,</span> <span class="k">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="k">None</span><span class="p">:</span>

        <span class="n">r</span><span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Called by Lightning when saving a checkpoint to give you a chance to store anything</span>

<span class="ss">        else you might want to save.</span>

<span class="ss">        Args:</span>

<span class="ss">            checkpoint: Checkpoint to be saved</span>

<span class="ss">        Example:</span>

<span class="ss">            .. code-block:: python</span>

<span class="ss">                def on_save_checkpoint(self, checkpoint):</span>

<span class="ss">                    # 99% of use cases you don&#39;t need to implement this method</span>

<span class="ss">                    checkpoint[&#39;something_cool_i_want_to_save&#39;] = my_cool_pickable_object</span>

<span class="ss">        Note:</span>

<span class="ss">            Lightning saves all aspects of training (epoch, global step, etc...)</span>

<span class="ss">            including amp scaling.</span>

<span class="ss">            There is no need for you to store anything about training.</span>

<span class="ss">        &quot;&quot;&quot;</span>
</code></pre></div>

</details>
<h5 id="on_train_end">on_train_end</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">on_train_end</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Called at the end of training before logger experiment is closed.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">on_train_end</span><span class="p">(</span><span class="k">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">None</span><span class="p">:</span>

        <span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Called at the end of training before logger experiment is closed.</span>

<span class="ss">        &quot;&quot;&quot;</span>
</code></pre></div>

</details>
<h5 id="on_train_start">on_train_start</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">on_train_start</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Called at the beginning of training before sanity check.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">on_train_start</span><span class="p">(</span><span class="k">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">None</span><span class="p">:</span>

        <span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Called at the beginning of training before sanity check.</span>

<span class="ss">        &quot;&quot;&quot;</span>
</code></pre></div>

</details>
<h5 id="optimizer_step">optimizer_step</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">epoch</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span>
    <span class="n">batch_idx</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
    <span class="n">optimizer_idx</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span>
    <span class="n">second_order_closure</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">on_tpu</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">using_native_amp</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">using_lbfgs</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Override this method to adjust the default way the
:class:<code>~pytorch_lightning.trainer.trainer.Trainer</code> calls each optimizer.
By default, Lightning calls <code>step()</code> and <code>zero_grad()</code> as shown in the example
once per optimizer.</p>
<p>Args:
    epoch: Current epoch
    batch_idx: Index of current batch
    optimizer: A PyTorch optimizer
    optimizer_idx: If you used multiple optimizers this indexes into that list.
    second_order_closure: closure for second order methods
    on_tpu: true if TPU backward is required
    using_native_amp: True if using native amp
    using_lbfgs: True if the matching optimizer is lbfgs</p>
<p>Examples:
    .. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>    <span class="o">#</span> <span class="k">DEFAULT</span>
    <span class="n">def</span> <span class="n">optimizer_step</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">current_epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span>
                       <span class="n">second_order_closure</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="o">#</span> <span class="n">Alternating</span> <span class="n">schedule</span> <span class="k">for</span> <span class="n">optimizer</span> <span class="n">steps</span> <span class="p">(</span><span class="n">i</span><span class="p">.</span><span class="n">e</span><span class="p">.:</span> <span class="n">GANs</span><span class="p">)</span>
    <span class="n">def</span> <span class="n">optimizer_step</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">current_epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span>
                       <span class="n">second_order_closure</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">):</span>
        <span class="o">#</span> <span class="k">update</span> <span class="n">generator</span> <span class="n">opt</span> <span class="k">every</span> <span class="mi">2</span> <span class="n">steps</span>
        <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="o">#</span> <span class="k">update</span> <span class="n">discriminator</span> <span class="n">opt</span> <span class="k">every</span> <span class="mi">4</span> <span class="n">steps</span>
        <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="mi">4</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="o">#</span> <span class="p">...</span>
        <span class="o">#</span> <span class="k">add</span> <span class="k">as</span> <span class="n">many</span> <span class="n">optimizers</span> <span class="k">as</span> <span class="n">you</span> <span class="n">want</span>

<span class="n">Here</span><span class="s1">&#39;s another example showing how to use this for more advanced things such as</span>
<span class="s1">learning rate warm-up:</span>

<span class="s1">.. code-block:: python</span>

<span class="s1">    # learning rate warm-up</span>
<span class="s1">    def optimizer_step(self, current_epoch, batch_idx, optimizer,</span>
<span class="s1">                        optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs):</span>
<span class="s1">        # warm up lr</span>
<span class="s1">        if self.trainer.global_step &lt; 500:</span>
<span class="s1">            lr_scale = min(1., float(self.trainer.global_step + 1) / 500.)</span>
<span class="s1">            for pg in optimizer.param_groups:</span>
<span class="s1">                pg[&#39;</span><span class="n">lr</span><span class="err">&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_scale</span> <span class="o">*</span> <span class="k">self</span><span class="p">.</span><span class="n">learning_rate</span>

        <span class="o">#</span> <span class="k">update</span> <span class="n">params</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div>

<p>Note:
    If you also override the :meth:<code>~pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad</code>
    model hook don't forget to add the call to it before <code>optimizer.zero_grad()</code> yourself.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">optimizer_step</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="n">self</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="nl">epoch</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="nl">batch_idx</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="nl">optimizer</span><span class="p">:</span><span class="w"> </span><span class="n">Optimizer</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="nl">optimizer_idx</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="nl">second_order_closure</span><span class="p">:</span><span class="w"> </span><span class="n">Optional</span><span class="o">[</span><span class="n">Callable</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="nl">on_tpu</span><span class="p">:</span><span class="w"> </span><span class="n">bool</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">False</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="nl">using_native_amp</span><span class="p">:</span><span class="w"> </span><span class="n">bool</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">False</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="nl">using_lbfgs</span><span class="p">:</span><span class="w"> </span><span class="n">bool</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">False</span><span class="p">,</span><span class="w"></span>

<span class="w">    </span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="n">r</span><span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Override this method to adjust the default way the</span>

<span class="ss">        :class:`~pytorch_lightning.trainer.trainer.Trainer` calls each optimizer.</span>

<span class="ss">        By default, Lightning calls ``step()`` and ``zero_grad()`` as shown in the example</span>

<span class="ss">        once per optimizer.</span>

<span class="ss">        Args:</span>

<span class="ss">            epoch: Current epoch</span>

<span class="ss">            batch_idx: Index of current batch</span>

<span class="ss">            optimizer: A PyTorch optimizer</span>

<span class="ss">            optimizer_idx: If you used multiple optimizers this indexes into that list.</span>

<span class="ss">            second_order_closure: closure for second order methods</span>

<span class="ss">            on_tpu: true if TPU backward is required</span>

<span class="ss">            using_native_amp: True if using native amp</span>

<span class="ss">            using_lbfgs: True if the matching optimizer is lbfgs</span>

<span class="ss">        Examples:</span>

<span class="ss">            .. code-block:: python</span>

<span class="ss">                # DEFAULT</span>

<span class="ss">                def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx,</span>

<span class="ss">                                   second_order_closure, on_tpu, using_native_amp, using_lbfgs):</span>

<span class="ss">                    optimizer.step()</span>

<span class="ss">                # Alternating schedule for optimizer steps (i.e.: GANs)</span>

<span class="ss">                def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx,</span>

<span class="ss">                                   second_order_closure, on_tpu, using_native_amp, using_lbfgs):</span>

<span class="ss">                    # update generator opt every 2 steps</span>

<span class="ss">                    if optimizer_idx == 0:</span>

<span class="ss">                        if batch_idx % 2 == 0 :</span>

<span class="ss">                            optimizer.step()</span>

<span class="ss">                            optimizer.zero_grad()</span>

<span class="ss">                    # update discriminator opt every 4 steps</span>

<span class="ss">                    if optimizer_idx == 1:</span>

<span class="ss">                        if batch_idx % 4 == 0 :</span>

<span class="ss">                            optimizer.step()</span>

<span class="ss">                            optimizer.zero_grad()</span>

<span class="ss">                    # ...</span>

<span class="ss">                    # add as many optimizers as you want</span>

<span class="ss">            Here&#39;s another example showing how to use this for more advanced things such as</span>

<span class="ss">            learning rate warm-up:</span>

<span class="ss">            .. code-block:: python</span>

<span class="ss">                # learning rate warm-up</span>

<span class="ss">                def optimizer_step(self, current_epoch, batch_idx, optimizer,</span>

<span class="ss">                                    optimizer_idx, second_order_closure, on_tpu, using_native_amp, using_lbfgs):</span>

<span class="ss">                    # warm up lr</span>

<span class="ss">                    if self.trainer.global_step &lt; 500:</span>

<span class="ss">                        lr_scale = min(1., float(self.trainer.global_step + 1) / 500.)</span>

<span class="ss">                        for pg in optimizer.param_groups:</span>

<span class="ss">                            pg[&#39;lr&#39;] = lr_scale * self.learning_rate</span>

<span class="ss">                    # update params</span>

<span class="ss">                    optimizer.step()</span>

<span class="ss">                    optimizer.zero_grad()</span>

<span class="ss">        Note:</span>

<span class="ss">            If you also override the :meth:`~pytorch_lightning.core.hooks.ModelHooks.on_before_zero_grad`</span>

<span class="ss">            model hook don&#39;t forget to add the call to it before ``optimizer.zero_grad()`` yourself.</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nl">on_tpu</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">xm</span><span class="p">.</span><span class="n">optimizer_step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="nl">using_native_amp</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">self</span><span class="p">.</span><span class="n">trainer</span><span class="p">.</span><span class="n">scaler</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="nl">using_lbfgs</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">second_order_closure</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">else</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span><span class="w"></span>
</code></pre></div>

</details>
<h5 id="optimizer_zero_grad">optimizer_zero_grad</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">epoch</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span>
    <span class="n">batch_idx</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
    <span class="n">optimizer_idx</span><span class="p">:</span><span class="nb">int</span>
<span class="p">)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">optimizer_zero_grad</span><span class="p">(</span><span class="k">self</span><span class="p">,</span>

                            <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>

                            <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>

                            <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>

                            <span class="n">optimizer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>

        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div>

</details>
<h5 id="parameters">parameters</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">recurse</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]</span>
</code></pre></div>

<p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<p>Args:
    recurse (bool): if True, then yields parameters of this module
        and all submodules. Otherwise, yields only parameters that
        are direct members of this module.</p>
<p>Yields:
    Parameter: module parameter</p>
<p>Example::</p>
<div class="codehilite"><pre><span></span><code><span class="err">&gt;&gt;&gt; for param in model.parameters():</span>
<span class="err">&gt;&gt;&gt;     print(type(param), param.size())</span>
<span class="err">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="err">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="k">parameters</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">recurse</span><span class="p">:</span><span class="w"> </span><span class="n">bool</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">True</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Iterator</span><span class="o">[</span><span class="n">Parameter</span><span class="o">]</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="n">r</span><span class="ss">&quot;&quot;&quot;Returns an iterator over module parameters.</span>

<span class="ss">        This is typically passed to an optimizer.</span>

<span class="ss">        Args:</span>

<span class="ss">            recurse (bool): if True, then yields parameters of this module</span>

<span class="ss">                and all submodules. Otherwise, yields only parameters that</span>

<span class="ss">                are direct members of this module.</span>

<span class="ss">        Yields:</span>

<span class="ss">            Parameter: module parameter</span>

<span class="ss">        Example::</span>

<span class="ss">            &gt;&gt;&gt; for param in model.parameters():</span>

<span class="ss">            &gt;&gt;&gt;     print(type(param), param.size())</span>

<span class="ss">            &lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>

<span class="ss">            &lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="n">param</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="n">recurse</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">yield</span><span class="w"> </span><span class="n">param</span><span class="w"></span>
</code></pre></div>

</details>
<h5 id="prepare_data">prepare_data</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Use this to download and prepare data.</p>
<p>.. warning:: DO NOT set state to the model (use <code>setup</code> instead)
    since this is NOT called on every GPU in DDP/TPU</p>
<p>Example::</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span> <span class="n">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># good</span>
    <span class="n">download_data</span><span class="p">()</span>
    <span class="n">tokenize</span><span class="p">()</span>
    <span class="n">etc</span><span class="p">()</span>

    <span class="c1"># bad</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="n">data_split</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">some_state</span> <span class="o">=</span> <span class="n">some_other_state</span><span class="p">()</span>
</code></pre></div>

<p>In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)):</p>
<ol>
<li>Once per node. This is the default and is only called on LOCAL_RANK=0.</li>
<li>Once in total. Only called on GLOBAL_RANK=0.</li>
</ol>
<p>Example::</p>
<div class="codehilite"><pre><span></span><code><span class="o">#</span> <span class="k">DEFAULT</span>
<span class="o">#</span> <span class="k">called</span> <span class="n">once</span> <span class="n">per</span> <span class="n">node</span> <span class="k">on</span> <span class="n">LOCAL_RANK</span><span class="o">=</span><span class="mi">0</span> <span class="k">of</span> <span class="n">that</span> <span class="n">node</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">prepare_data_per_node</span><span class="o">=</span><span class="k">True</span><span class="p">)</span>

<span class="o">#</span> <span class="k">call</span> <span class="k">on</span> <span class="n">GLOBAL_RANK</span><span class="o">=</span><span class="mi">0</span> <span class="p">(</span><span class="n">great</span> <span class="k">for</span> <span class="n">shared</span> <span class="n">file</span> <span class="n">systems</span><span class="p">)</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">prepare_data_per_node</span><span class="o">=</span><span class="k">False</span><span class="p">)</span>
</code></pre></div>

<p>This is called before requesting the dataloaders:</p>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code><span class="n">model</span><span class="o">.</span><span class="n">prepare_data</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">ddp</span><span class="o">/</span><span class="n">tpu</span><span class="p">:</span> <span class="n">init</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">val_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">test_dataloader</span><span class="p">()</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">prepare_data</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">None</span><span class="o">:</span>

        <span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2"></span>

<span class="s2">        Use this to download and prepare data.</span>

<span class="s2">        .. warning:: DO NOT set state to the model (use `setup` instead)</span>

<span class="s2">            since this is NOT called on every GPU in DDP/TPU</span>

<span class="s2">        Example::</span>

<span class="s2">            def prepare_data(self):</span>

<span class="s2">                # good</span>

<span class="s2">                download_data()</span>

<span class="s2">                tokenize()</span>

<span class="s2">                etc()</span>

<span class="s2">                # bad</span>

<span class="s2">                self.split = data_split</span>

<span class="s2">                self.some_state = some_other_state()</span>

<span class="s2">        In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)):</span>

<span class="s2">        1. Once per node. This is the default and is only called on LOCAL_RANK=0.</span>

<span class="s2">        2. Once in total. Only called on GLOBAL_RANK=0.</span>

<span class="s2">        Example::</span>

<span class="s2">            # DEFAULT</span>

<span class="s2">            # called once per node on LOCAL_RANK=0 of that node</span>

<span class="s2">            Trainer(prepare_data_per_node=True)</span>

<span class="s2">            # call on GLOBAL_RANK=0 (great for shared file systems)</span>

<span class="s2">            Trainer(prepare_data_per_node=False)</span>

<span class="s2">        This is called before requesting the dataloaders:</span>

<span class="s2">        .. code-block:: python</span>

<span class="s2">            model.prepare_data()</span>

<span class="s2">                if ddp/tpu: init()</span>

<span class="s2">            model.setup(stage)</span>

<span class="s2">            model.train_dataloader()</span>

<span class="s2">            model.val_dataloader()</span>

<span class="s2">            model.test_dataloader()</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>
</code></pre></div>

</details>
<h5 id="print">print</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">print</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Prints only from process 0. Use this in any distributed mode to log only once.</p>
<p>Args:
    <em>args: The thing to print. Will be passed to Python's built-in print function.
    </em>*kwargs: Will be passed to Python's built-in print function.</p>
<p>Example:</p>
<div class="codehilite"><pre><span></span><code><span class="p">..</span> <span class="ow">code-block</span><span class="p">::</span> <span class="k">python</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;in forward&#39;</span><span class="p">)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">print</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">None</span><span class="p">:</span>

        <span class="n">r</span><span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Prints only from process 0. Use this in any distributed mode to log only once.</span>

<span class="ss">        Args:</span>

<span class="ss">            *args: The thing to print. Will be passed to Python&#39;s built-in print function.</span>

<span class="ss">            **kwargs: Will be passed to Python&#39;s built-in print function.</span>

<span class="ss">        Example:</span>

<span class="ss">            .. code-block:: python</span>

<span class="ss">                def forward(self, x):</span>

<span class="ss">                    self.print(x, &#39;in forward&#39;)</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="k">self</span><span class="p">.</span><span class="n">trainer</span><span class="p">.</span><span class="n">is_global_zero</span><span class="p">:</span>

            <span class="n">print</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>

</details>
<h5 id="register_backward_hook">register_backward_hook</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">register_backward_hook</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hook</span><span class="p">:</span><span class="n">Callable</span><span class="p">[[</span><span class="n">_ForwardRef</span><span class="p">(</span><span class="s1">&#39;Module&#39;</span><span class="p">),</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">Union</span><span class="p">[</span><span class="n">NoneType</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span>
</code></pre></div>

<p>Registers a backward hook on the module.</p>
<p>.. warning ::</p>
<div class="codehilite"><pre><span></span><code><span class="n">The</span> <span class="k">current</span> <span class="n">implementation</span> <span class="n">will</span> <span class="k">not</span> <span class="n">have</span> <span class="n">the</span> <span class="n">presented</span> <span class="n">behavior</span>
<span class="k">for</span> <span class="n">complex</span> <span class="o">:</span><span class="n">class</span><span class="o">:</span><span class="n">`Module`</span> <span class="n">that</span> <span class="n">perform</span> <span class="n">many</span> <span class="n">operations</span><span class="p">.</span>
<span class="k">In</span> <span class="k">some</span> <span class="n">failure</span> <span class="n">cases</span><span class="p">,</span> <span class="o">:</span><span class="n">attr</span><span class="o">:</span><span class="n">`grad_input`</span> <span class="k">and</span> <span class="o">:</span><span class="n">attr</span><span class="o">:</span><span class="n">`grad_output`</span> <span class="n">will</span> <span class="k">only</span>
<span class="n">contain</span> <span class="n">the</span> <span class="n">gradients</span> <span class="k">for</span> <span class="n">a</span> <span class="n">subset</span> <span class="k">of</span> <span class="n">the</span> <span class="n">inputs</span> <span class="k">and</span> <span class="n">outputs</span><span class="p">.</span>
<span class="k">For</span> <span class="n">such</span> <span class="o">:</span><span class="n">class</span><span class="o">:</span><span class="n">`Module`</span><span class="p">,</span> <span class="n">you</span> <span class="n">should</span> <span class="k">use</span> <span class="o">:</span><span class="n">func</span><span class="o">:</span><span class="n">`torch.Tensor.register_hook`</span>
<span class="n">directly</span> <span class="k">on</span> <span class="n">a</span> <span class="k">specific</span> <span class="n">input</span> <span class="k">or</span> <span class="n">output</span> <span class="k">to</span> <span class="k">get</span> <span class="n">the</span> <span class="n">required</span> <span class="n">gradients</span><span class="p">.</span>
</code></pre></div>

<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature::</p>
<div class="codehilite"><pre><span></span><code><span class="err">hook(module, grad_input, grad_output) -&gt; Tensor or None</span>
</code></pre></div>

<p>The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> may be tuples if the
module has multiple inputs or outputs. The hook should not modify its
arguments, but it can optionally return a new gradient with respect to
input that will be used in place of :attr:<code>grad_input</code> in subsequent
computations. :attr:<code>grad_input</code> will only correspond to the inputs given
as positional arguments.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">register_backward_hook</span><span class="p">(</span>

        <span class="n">self</span><span class="p">,</span> <span class="n">hook</span><span class="o">:</span> <span class="n">Callable</span><span class="err">[[</span><span class="s1">&#39;Module&#39;</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">,</span> <span class="n">_grad_t</span><span class="err">]</span><span class="p">,</span> <span class="k">Union</span><span class="err">[</span><span class="k">None</span><span class="p">,</span> <span class="n">Tensor</span><span class="err">]]</span>

    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="o">:</span>

        <span class="n">r</span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Registers a backward hook on the module.</span>

<span class="s2">        .. warning ::</span>

<span class="s2">            The current implementation will not have the presented behavior</span>

<span class="s2">            for complex :class:`Module` that perform many operations.</span>

<span class="s2">            In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only</span>

<span class="s2">            contain the gradients for a subset of the inputs and outputs.</span>

<span class="s2">            For such :class:`Module`, you should use :func:`torch.Tensor.register_hook`</span>

<span class="s2">            directly on a specific input or output to get the required gradients.</span>

<span class="s2">        The hook will be called every time the gradients with respect to module</span>

<span class="s2">        inputs are computed. The hook should have the following signature::</span>

<span class="s2">            hook(module, grad_input, grad_output) -&gt; Tensor or None</span>

<span class="s2">        The :attr:`grad_input` and :attr:`grad_output` may be tuples if the</span>

<span class="s2">        module has multiple inputs or outputs. The hook should not modify its</span>

<span class="s2">        arguments, but it can optionally return a new gradient with respect to</span>

<span class="s2">        input that will be used in place of :attr:`grad_input` in subsequent</span>

<span class="s2">        computations. :attr:`grad_input` will only correspond to the inputs given</span>

<span class="s2">        as positional arguments.</span>

<span class="s2">        Returns:</span>

<span class="s2">            :class:`torch.utils.hooks.RemovableHandle`:</span>

<span class="s2">                a handle that can be used to remove the added hook by calling</span>

<span class="s2">                ``handle.remove()``</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="p">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_backward_hooks</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">_backward_hooks</span><span class="err">[</span><span class="n">handle</span><span class="p">.</span><span class="n">id</span><span class="err">]</span> <span class="o">=</span> <span class="n">hook</span>

        <span class="k">return</span> <span class="n">handle</span>
</code></pre></div>

</details>
<h5 id="register_buffer">register_buffer</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">register_buffer</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span>
    <span class="n">tensor</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">persistent</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Adds a buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm's <code>running_mean</code>
is not a parameter, but is part of the module's state. Buffers, by
default, are persistent and will be saved alongside parameters. This
behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The
only difference between a persistent buffer and a non-persistent buffer
is that the latter will not be a part of this module's
:attr:<code>state_dict</code>.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<p>Args:
    name (string): name of the buffer. The buffer can be accessed
        from this module using the given name
    tensor (Tensor): buffer to be registered.
    persistent (bool): whether the buffer is part of this module's
        :attr:<code>state_dict</code>.</p>
<p>Example::</p>
<div class="codehilite"><pre><span></span><code><span class="err">&gt;&gt;&gt; self.register_buffer(&#39;running_mean&#39;, torch.zeros(num_features))</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">register_buffer</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="k">name</span><span class="o">:</span> <span class="n">str</span><span class="p">,</span> <span class="n">tensor</span><span class="o">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">persistent</span><span class="o">:</span> <span class="kt">bool</span> <span class="o">=</span> <span class="no">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">None</span><span class="o">:</span>

        <span class="n">r</span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Adds a buffer to the module.</span>

<span class="s2">        This is typically used to register a buffer that should not to be</span>

<span class="s2">        considered a model parameter. For example, BatchNorm&#39;s ``running_mean``</span>

<span class="s2">        is not a parameter, but is part of the module&#39;s state. Buffers, by</span>

<span class="s2">        default, are persistent and will be saved alongside parameters. This</span>

<span class="s2">        behavior can be changed by setting :attr:`persistent` to ``False``. The</span>

<span class="s2">        only difference between a persistent buffer and a non-persistent buffer</span>

<span class="s2">        is that the latter will not be a part of this module&#39;s</span>

<span class="s2">        :attr:`state_dict`.</span>

<span class="s2">        Buffers can be accessed as attributes using given names.</span>

<span class="s2">        Args:</span>

<span class="s2">            name (string): name of the buffer. The buffer can be accessed</span>

<span class="s2">                from this module using the given name</span>

<span class="s2">            tensor (Tensor): buffer to be registered.</span>

<span class="s2">            persistent (bool): whether the buffer is part of this module&#39;s</span>

<span class="s2">                :attr:`state_dict`.</span>

<span class="s2">        Example::</span>

<span class="s2">            &gt;&gt;&gt; self.register_buffer(&#39;running_mean&#39;, torch.zeros(num_features))</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="k">if</span> <span class="n">persistent</span> <span class="k">is</span> <span class="no">False</span> <span class="k">and</span> <span class="n">isinstance</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">ScriptModule</span><span class="p">)</span><span class="o">:</span>

            <span class="n">raise</span> <span class="n">RuntimeError</span><span class="p">(</span><span class="s2">&quot;ScriptModule does not support non-persistent buffers&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s1">&#39;_buffers&#39;</span> <span class="k">not</span> <span class="k">in</span> <span class="n">self</span><span class="p">.</span><span class="n">__dict__</span><span class="o">:</span>

            <span class="n">raise</span> <span class="n">AttributeError</span><span class="p">(</span>

                <span class="s2">&quot;cannot assign buffer before Module.__init__() call&quot;</span><span class="p">)</span>

        <span class="n">elif</span> <span class="k">not</span> <span class="n">isinstance</span><span class="p">(</span><span class="k">name</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">_six</span><span class="p">.</span><span class="n">string_classes</span><span class="p">)</span><span class="o">:</span>

            <span class="n">raise</span> <span class="n">TypeError</span><span class="p">(</span><span class="s2">&quot;buffer name should be a string. &quot;</span>

                            <span class="s2">&quot;Got {}&quot;</span><span class="p">.</span><span class="k">format</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">typename</span><span class="p">(</span><span class="k">name</span><span class="p">)))</span>

        <span class="n">elif</span> <span class="s1">&#39;.&#39;</span> <span class="k">in</span> <span class="k">name</span><span class="o">:</span>

            <span class="n">raise</span> <span class="n">KeyError</span><span class="p">(</span><span class="s2">&quot;buffer name can&#39;t contain </span><span class="se">\&quot;</span><span class="s2">.</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">elif</span> <span class="k">name</span> <span class="o">==</span> <span class="s1">&#39;&#39;</span><span class="o">:</span>

            <span class="n">raise</span> <span class="n">KeyError</span><span class="p">(</span><span class="s2">&quot;buffer name can&#39;t be empty string </span><span class="se">\&quot;\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">elif</span> <span class="n">hasattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="k">name</span><span class="p">)</span> <span class="k">and</span> <span class="k">name</span> <span class="k">not</span> <span class="k">in</span> <span class="n">self</span><span class="p">.</span><span class="n">_buffers</span><span class="o">:</span>

            <span class="n">raise</span> <span class="n">KeyError</span><span class="p">(</span><span class="s2">&quot;attribute &#39;{}&#39; already exists&quot;</span><span class="p">.</span><span class="k">format</span><span class="p">(</span><span class="k">name</span><span class="p">))</span>

        <span class="n">elif</span> <span class="n">tensor</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span> <span class="k">and</span> <span class="k">not</span> <span class="n">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span><span class="o">:</span>

            <span class="n">raise</span> <span class="n">TypeError</span><span class="p">(</span><span class="s2">&quot;cannot assign &#39;{}&#39; object to buffer &#39;{}&#39; &quot;</span>

                            <span class="s2">&quot;(torch Tensor or None required)&quot;</span>

                            <span class="p">.</span><span class="k">format</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">typename</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span> <span class="k">name</span><span class="p">))</span>

        <span class="k">else</span><span class="o">:</span>

            <span class="n">self</span><span class="p">.</span><span class="n">_buffers</span><span class="err">[</span><span class="k">name</span><span class="err">]</span> <span class="o">=</span> <span class="n">tensor</span>

            <span class="k">if</span> <span class="n">persistent</span><span class="o">:</span>

                <span class="n">self</span><span class="p">.</span><span class="n">_non_persistent_buffers_set</span><span class="p">.</span><span class="k">discard</span><span class="p">(</span><span class="k">name</span><span class="p">)</span>

            <span class="k">else</span><span class="o">:</span>

                <span class="n">self</span><span class="p">.</span><span class="n">_non_persistent_buffers_set</span><span class="p">.</span><span class="k">add</span><span class="p">(</span><span class="k">name</span><span class="p">)</span>
</code></pre></div>

</details>
<h5 id="register_forward_hook">register_forward_hook</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">register_forward_hook</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hook</span><span class="p">:</span><span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span>
</code></pre></div>

<p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after :func:<code>forward</code> has computed an output.
It should have the following signature::</p>
<div class="codehilite"><pre><span></span><code><span class="err">hook(module, input, output) -&gt; None or modified output</span>
</code></pre></div>

<p>The input contains only the positional arguments given to the module.
Keyword arguments won't be passed to the hooks and only to the <code>forward</code>.
The hook can modify the output. It can modify the input inplace but
it will not have effect on forward since this is called after
:func:<code>forward</code> is called.</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">register_forward_hook</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hook</span><span class="o">:</span> <span class="n">Callable</span><span class="err">[</span><span class="p">...,</span> <span class="k">None</span><span class="err">]</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="o">:</span>

        <span class="n">r</span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Registers a forward hook on the module.</span>

<span class="s2">        The hook will be called every time after :func:`forward` has computed an output.</span>

<span class="s2">        It should have the following signature::</span>

<span class="s2">            hook(module, input, output) -&gt; None or modified output</span>

<span class="s2">        The input contains only the positional arguments given to the module.</span>

<span class="s2">        Keyword arguments won&#39;t be passed to the hooks and only to the ``forward``.</span>

<span class="s2">        The hook can modify the output. It can modify the input inplace but</span>

<span class="s2">        it will not have effect on forward since this is called after</span>

<span class="s2">        :func:`forward` is called.</span>

<span class="s2">        Returns:</span>

<span class="s2">            :class:`torch.utils.hooks.RemovableHandle`:</span>

<span class="s2">                a handle that can be used to remove the added hook by calling</span>

<span class="s2">                ``handle.remove()``</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="p">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_forward_hooks</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">_forward_hooks</span><span class="err">[</span><span class="n">handle</span><span class="p">.</span><span class="n">id</span><span class="err">]</span> <span class="o">=</span> <span class="n">hook</span>

        <span class="k">return</span> <span class="n">handle</span>
</code></pre></div>

</details>
<h5 id="register_forward_pre_hook">register_forward_pre_hook</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">register_forward_pre_hook</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hook</span><span class="p">:</span><span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span>
</code></pre></div>

<p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before :func:<code>forward</code> is invoked.
It should have the following signature::</p>
<div class="codehilite"><pre><span></span><code><span class="err">hook(module, input) -&gt; None or modified input</span>
</code></pre></div>

<p>The input contains only the positional arguments given to the module.
Keyword arguments won't be passed to the hooks and only to the <code>forward</code>.
The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).</p>
<p>Returns:
    :class:<code>torch.utils.hooks.RemovableHandle</code>:
        a handle that can be used to remove the added hook by calling
        <code>handle.remove()</code></p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hook</span><span class="o">:</span> <span class="n">Callable</span><span class="err">[</span><span class="p">...,</span> <span class="k">None</span><span class="err">]</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="o">:</span>

        <span class="n">r</span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Registers a forward pre-hook on the module.</span>

<span class="s2">        The hook will be called every time before :func:`forward` is invoked.</span>

<span class="s2">        It should have the following signature::</span>

<span class="s2">            hook(module, input) -&gt; None or modified input</span>

<span class="s2">        The input contains only the positional arguments given to the module.</span>

<span class="s2">        Keyword arguments won&#39;t be passed to the hooks and only to the ``forward``.</span>

<span class="s2">        The hook can modify the input. User can either return a tuple or a</span>

<span class="s2">        single modified value in the hook. We will wrap the value into a tuple</span>

<span class="s2">        if a single value is returned(unless that value is already a tuple).</span>

<span class="s2">        Returns:</span>

<span class="s2">            :class:`torch.utils.hooks.RemovableHandle`:</span>

<span class="s2">                a handle that can be used to remove the added hook by calling</span>

<span class="s2">                ``handle.remove()``</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="p">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_forward_pre_hooks</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">_forward_pre_hooks</span><span class="err">[</span><span class="n">handle</span><span class="p">.</span><span class="n">id</span><span class="err">]</span> <span class="o">=</span> <span class="n">hook</span>

        <span class="k">return</span> <span class="n">handle</span>
</code></pre></div>

</details>
<h5 id="register_parameter">register_parameter</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">register_parameter</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span>
    <span class="n">param</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">Parameter</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<p>Args:
    name (string): name of the parameter. The parameter can be accessed
        from this module using the given name
    param (Parameter): parameter to be added to the module.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">register_parameter</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span><span class="p">,</span><span class="w"> </span><span class="nl">param</span><span class="p">:</span><span class="w"> </span><span class="k">Parameter</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="n">r</span><span class="ss">&quot;&quot;&quot;Adds a parameter to the module.</span>

<span class="ss">        The parameter can be accessed as an attribute using given name.</span>

<span class="ss">        Args:</span>

<span class="ss">            name (string): name of the parameter. The parameter can be accessed</span>

<span class="ss">                from this module using the given name</span>

<span class="ss">            param (Parameter): parameter to be added to the module.</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="s1">&#39;_parameters&#39;</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="nl">__dict__</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">AttributeError</span><span class="p">(</span><span class="w"></span>

<span class="w">                </span><span class="ss">&quot;cannot assign parameter before Module.__init__() call&quot;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="n">_six</span><span class="p">.</span><span class="n">string_classes</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span><span class="ss">&quot;parameter name should be a string. &quot;</span><span class="w"></span>

<span class="w">                            </span><span class="ss">&quot;Got {}&quot;</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">typename</span><span class="p">(</span><span class="n">name</span><span class="p">)))</span><span class="w"></span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="s1">&#39;.&#39;</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nl">name</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">KeyError</span><span class="p">(</span><span class="ss">&quot;parameter name can&#39;t contain \&quot;</span><span class="p">.</span><span class="err">\</span><span class="ss">&quot;&quot;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s1">&#39;&#39;</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">KeyError</span><span class="p">(</span><span class="ss">&quot;parameter name can&#39;t be empty string \&quot;</span><span class="err">\</span><span class="ss">&quot;&quot;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="n">hasattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="nl">_parameters</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">KeyError</span><span class="p">(</span><span class="ss">&quot;attribute &#39;{}&#39; already exists&quot;</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">param</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">self</span><span class="p">.</span><span class="n">_parameters</span><span class="o">[</span><span class="n">name</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="w"></span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span><span class="w"> </span><span class="k">Parameter</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span><span class="ss">&quot;cannot assign &#39;{}&#39; object to parameter &#39;{}&#39; &quot;</span><span class="w"></span>

<span class="w">                            </span><span class="ss">&quot;(torch.nn.Parameter or None required)&quot;</span><span class="w"></span>

<span class="w">                            </span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">typename</span><span class="p">(</span><span class="n">param</span><span class="p">),</span><span class="w"> </span><span class="n">name</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="n">param</span><span class="p">.</span><span class="nl">grad_fn</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="w"></span>

<span class="w">                </span><span class="ss">&quot;Cannot assign non-leaf Tensor to parameter &#39;{0}&#39;. Model &quot;</span><span class="w"></span>

<span class="w">                </span><span class="ss">&quot;parameters must be created explicitly. To express &#39;{0}&#39; &quot;</span><span class="w"></span>

<span class="w">                </span><span class="ss">&quot;as a function of another Tensor, compute the value in &quot;</span><span class="w"></span>

<span class="w">                </span><span class="ss">&quot;the forward() method.&quot;</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="k">else</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">self</span><span class="p">.</span><span class="n">_parameters</span><span class="o">[</span><span class="n">name</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">param</span><span class="w"></span>
</code></pre></div>

</details>
<h5 id="requires_grad_">requires_grad_</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">requires_grad_</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">:</span><span class="o">~</span><span class="n">T</span><span class="p">,</span>
    <span class="n">requires_grad</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="o">~</span><span class="n">T</span>
</code></pre></div>

<p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters' :attr:<code>requires_grad</code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<p>Args:
    requires_grad (bool): whether autograd should record operations on
                          parameters in this module. Default: <code>True</code>.</p>
<p>Returns:
    Module: self</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">requires_grad_</span><span class="p">(</span><span class="n">self</span><span class="o">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">:</span> <span class="kt">bool</span> <span class="o">=</span> <span class="no">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="o">:</span>

        <span class="n">r</span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Change if autograd should record operations on parameters in this</span>

<span class="s2">        module.</span>

<span class="s2">        This method sets the parameters&#39; :attr:`requires_grad` attributes</span>

<span class="s2">        in-place.</span>

<span class="s2">        This method is helpful for freezing part of the module for finetuning</span>

<span class="s2">        or training parts of a model individually (e.g., GAN training).</span>

<span class="s2">        Args:</span>

<span class="s2">            requires_grad (bool): whether autograd should record operations on</span>

<span class="s2">                                  parameters in this module. Default: ``True``.</span>

<span class="s2">        Returns:</span>

<span class="s2">            Module: self</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="k">for</span> <span class="n">p</span> <span class="k">in</span> <span class="n">self</span><span class="p">.</span><span class="n">parameters</span><span class="p">()</span><span class="o">:</span>

            <span class="n">p</span><span class="p">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">self</span>
</code></pre></div>

</details>
<h5 id="save_hyperparameters">save_hyperparameters</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">save_hyperparameters</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="n">frame</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Save all model arguments.</p>
<p>Args:
    args: single object of <code>dict</code>, <code>NameSpace</code> or <code>OmegaConf</code>
     or string names or argumenst from class <code>__init__</code></p>
<blockquote>
<blockquote>
<blockquote>
<p>from collections import OrderedDict
class ManuallyArgsModel(LightningModule):
...     def <strong>init</strong>(self, arg1, arg2, arg3):
...         super().<strong>init</strong>()
...         # manually assine arguments
...         self.save_hyperparameters('arg1', 'arg3')
...     def forward(self, <em>args, </em>*kwargs):
...         ...
model = ManuallyArgsModel(1, 'abc', 3.14)
model.hparams
"arg1": 1
"arg3": 3.14</p>
<p>class AutomaticArgsModel(LightningModule):
...     def <strong>init</strong>(self, arg1, arg2, arg3):
...         super().<strong>init</strong>()
...         # equivalent automatic
...         self.save_hyperparameters()
...     def forward(self, <em>args, </em>*kwargs):
...         ...
model = AutomaticArgsModel(1, 'abc', 3.14)
model.hparams
"arg1": 1
"arg2": abc
"arg3": 3.14</p>
<p>class SingleArgModel(LightningModule):
...     def <strong>init</strong>(self, params):
...         super().<strong>init</strong>()
...         # manually assign single argument
...         self.save_hyperparameters(params)
...     def forward(self, <em>args, </em>*kwargs):
...         ...
model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14))
model.hparams
"p1": 1
"p2": abc
"p3": 3.14</p>
</blockquote>
</blockquote>
</blockquote>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="k">def</span> <span class="nf">save_hyperparameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">frame</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>

        <span class="sd">&quot;&quot;&quot;Save all model arguments.</span>

<span class="sd">        Args:</span>

<span class="sd">            args: single object of `dict`, `NameSpace` or `OmegaConf`</span>

<span class="sd">             or string names or argumenst from class `__init__`</span>

<span class="sd">        &gt;&gt;&gt; from collections import OrderedDict</span>

<span class="sd">        &gt;&gt;&gt; class ManuallyArgsModel(LightningModule):</span>

<span class="sd">        ...     def __init__(self, arg1, arg2, arg3):</span>

<span class="sd">        ...         super().__init__()</span>

<span class="sd">        ...         # manually assine arguments</span>

<span class="sd">        ...         self.save_hyperparameters(&#39;arg1&#39;, &#39;arg3&#39;)</span>

<span class="sd">        ...     def forward(self, *args, **kwargs):</span>

<span class="sd">        ...         ...</span>

<span class="sd">        &gt;&gt;&gt; model = ManuallyArgsModel(1, &#39;abc&#39;, 3.14)</span>

<span class="sd">        &gt;&gt;&gt; model.hparams</span>

<span class="sd">        &quot;arg1&quot;: 1</span>

<span class="sd">        &quot;arg3&quot;: 3.14</span>

<span class="sd">        &gt;&gt;&gt; class AutomaticArgsModel(LightningModule):</span>

<span class="sd">        ...     def __init__(self, arg1, arg2, arg3):</span>

<span class="sd">        ...         super().__init__()</span>

<span class="sd">        ...         # equivalent automatic</span>

<span class="sd">        ...         self.save_hyperparameters()</span>

<span class="sd">        ...     def forward(self, *args, **kwargs):</span>

<span class="sd">        ...         ...</span>

<span class="sd">        &gt;&gt;&gt; model = AutomaticArgsModel(1, &#39;abc&#39;, 3.14)</span>

<span class="sd">        &gt;&gt;&gt; model.hparams</span>

<span class="sd">        &quot;arg1&quot;: 1</span>

<span class="sd">        &quot;arg2&quot;: abc</span>

<span class="sd">        &quot;arg3&quot;: 3.14</span>

<span class="sd">        &gt;&gt;&gt; class SingleArgModel(LightningModule):</span>

<span class="sd">        ...     def __init__(self, params):</span>

<span class="sd">        ...         super().__init__()</span>

<span class="sd">        ...         # manually assign single argument</span>

<span class="sd">        ...         self.save_hyperparameters(params)</span>

<span class="sd">        ...     def forward(self, *args, **kwargs):</span>

<span class="sd">        ...         ...</span>

<span class="sd">        &gt;&gt;&gt; model = SingleArgModel(Namespace(p1=1, p2=&#39;abc&#39;, p3=3.14))</span>

<span class="sd">        &gt;&gt;&gt; model.hparams</span>

<span class="sd">        &quot;p1&quot;: 1</span>

<span class="sd">        &quot;p2&quot;: abc</span>

<span class="sd">        &quot;p3&quot;: 3.14</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">frame</span><span class="p">:</span>

            <span class="n">frame</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">currentframe</span><span class="p">()</span><span class="o">.</span><span class="n">f_back</span>

        <span class="n">init_args</span> <span class="o">=</span> <span class="n">get_init_args</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">init_args</span><span class="p">,</span> <span class="s1">&#39;failed to inspect the self init&#39;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="p">:</span>

            <span class="n">hp</span> <span class="o">=</span> <span class="n">init_args</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_hparams_name</span> <span class="o">=</span> <span class="s1">&#39;kwargs&#39;</span> <span class="k">if</span> <span class="n">hp</span> <span class="k">else</span> <span class="bp">None</span>

        <span class="k">else</span><span class="p">:</span>

            <span class="n">isx_non_str</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">arg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="nb">str</span><span class="p">)]</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">isx_non_str</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>

                <span class="n">hp</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="n">isx_non_str</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

                <span class="n">cand_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">init_args</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span> <span class="o">==</span> <span class="n">hp</span><span class="p">]</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">_hparams_name</span> <span class="o">=</span> <span class="n">cand_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">cand_names</span> <span class="k">else</span> <span class="bp">None</span>

            <span class="k">else</span><span class="p">:</span>

                <span class="n">hp</span> <span class="o">=</span> <span class="p">{</span><span class="n">arg</span><span class="p">:</span> <span class="n">init_args</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="nb">str</span><span class="p">)}</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">_hparams_name</span> <span class="o">=</span> <span class="s1">&#39;kwargs&#39;</span>

        <span class="c1"># `hparams` are expected here</span>

        <span class="k">if</span> <span class="n">hp</span><span class="p">:</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_set_hparams</span><span class="p">(</span><span class="n">hp</span><span class="p">)</span>
</code></pre></div>

</details>
<h5 id="setup">setup</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">setup</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">stage</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">None</span><span class="p">:</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_rpn</span><span class="p">:</span>  <span class="c1"># step 2</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span>

                <span class="n">Classification</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_mfn_ckpt</span><span class="p">,</span> <span class="s2">&quot;model.mfn&quot;</span>

            <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span> <span class="o">=</span> <span class="n">RPN</span><span class="p">()</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_roi</span><span class="p">:</span>  <span class="c1"># step 3</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span>

                <span class="n">Classification</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;mfn&quot;</span>

            <span class="p">)</span>

            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RPN</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;rpn&quot;</span><span class="p">)</span>

            <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">roi</span> <span class="o">=</span> <span class="n">RoI</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetune_rpn</span><span class="p">:</span>  <span class="c1"># step 4 or extra finetune rpn</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_rpn_ckpt</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_roi_ckpt</span><span class="p">:</span>  <span class="c1"># extra finetune rpn</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span>

                    <span class="n">Classification</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;mfn&quot;</span>

                <span class="p">)</span>

                <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RPN</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;rpn&quot;</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">roi</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RoI</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_roi_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;roi&quot;</span><span class="p">)</span>

                <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">roi</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Detection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">roi</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span>

                    <span class="n">Classification</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;mfn&quot;</span>

                <span class="p">)</span>

                <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RPN</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;rpn&quot;</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">roi</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RoI</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_roi_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;roi&quot;</span><span class="p">)</span>

                <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">roi</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Detection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">roi</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetune_roi</span><span class="p">:</span>  <span class="c1"># step 5 or extra finetune roi</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_rpn_ckpt</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_roi_ckpt</span><span class="p">:</span>  <span class="c1"># extra finetune roi</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span>

                    <span class="n">Classification</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;mfn&quot;</span>

                <span class="p">)</span>

                <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RPN</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;rpn&quot;</span><span class="p">)</span>

                <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">roi</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RoI</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_roi_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;roi&quot;</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Detection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">roi</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span>

                    <span class="n">Classification</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;mfn&quot;</span>

                <span class="p">)</span>

                <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RPN</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_rpn_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;rpn&quot;</span><span class="p">)</span>

                <span class="n">freeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">roi</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">RoI</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained_roi_ckpt</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;roi&quot;</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Detection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">roi</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>  <span class="c1"># step 6: final/joint model</span>

            <span class="n">load_checkpoint_fn</span> <span class="o">=</span> <span class="n">load_checkpoint</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_roi_ckpt</span> <span class="k">is</span> <span class="ow">not</span> <span class="n">None</span><span class="p">:</span>

                <span class="n">ckpt_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetuned_rpn_ckpt</span>

            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">resume_sagemaker_from_checkpoint</span> <span class="k">is</span> <span class="ow">not</span> <span class="n">None</span><span class="p">:</span>

                <span class="n">ckpt_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resume_sagemaker_from_checkpoint</span>

            <span class="k">else</span><span class="p">:</span>

                <span class="n">ckpt_path</span> <span class="o">=</span> <span class="n">None</span>

                <span class="c1"># ignore load_checkpoint</span>

                <span class="n">load_checkpoint_fn</span> <span class="o">=</span> <span class="n">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">mfn</span> <span class="o">=</span> <span class="n">load_checkpoint_fn</span><span class="p">(</span><span class="n">Classification</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="n">ckpt_path</span><span class="p">,</span> <span class="s2">&quot;mfn&quot;</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span> <span class="o">=</span> <span class="n">load_checkpoint_fn</span><span class="p">(</span><span class="n">RPN</span><span class="p">(),</span> <span class="n">ckpt_path</span><span class="p">,</span> <span class="s2">&quot;rpn&quot;</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">roi</span> <span class="o">=</span> <span class="n">load_checkpoint_fn</span><span class="p">(</span><span class="n">RoI</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">),</span> <span class="n">ckpt_path</span><span class="p">,</span> <span class="s2">&quot;roi&quot;</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Detection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mfn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">roi</span><span class="p">)</span>

        <span class="k">return</span>
</code></pre></div>

</details>
<h5 id="share_memory">share_memory</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">share_memory</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">:</span><span class="o">~</span><span class="n">T</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="o">~</span><span class="n">T</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">share_memory</span><span class="p">(</span><span class="k">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>

        <span class="k">return</span> <span class="k">self</span><span class="p">.</span><span class="n">_apply</span><span class="p">(</span><span class="n">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="p">.</span><span class="n">share_memory_</span><span class="p">())</span>
</code></pre></div>

</details>
<h5 id="state_dict">state_dict</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">destination</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span>
    <span class="n">keep_vars</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>

<p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.</p>
<p>Returns:
    dict:
        a dictionary containing a whole state of the module</p>
<p>Example::</p>
<div class="codehilite"><pre><span></span><code><span class="err">&gt;&gt;&gt; module.state_dict().keys()</span>
<span class="err">[&#39;bias&#39;, &#39;weight&#39;]</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="s s-Atom">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="s s-Atom">self</span><span class="p">,</span> <span class="s s-Atom">destination</span><span class="o">=</span><span class="nv">None</span><span class="p">,</span> <span class="s s-Atom">prefix=&#39;&#39;</span><span class="p">,</span> <span class="s s-Atom">keep_vars</span><span class="o">=</span><span class="nv">False</span><span class="p">)</span><span class="s s-Atom">:</span>

        <span class="s s-Atom">r</span><span class="s2">&quot;&quot;&quot;Returns a dictionary containing a whole state of the module.</span>

<span class="s2">        Both parameters and persistent buffers (e.g. running averages) are</span>

<span class="s2">        included. Keys are corresponding parameter and buffer names.</span>

<span class="s2">        Returns:</span>

<span class="s2">            dict:</span>

<span class="s2">                a dictionary containing a whole state of the module</span>

<span class="s2">        Example::</span>

<span class="s2">            &gt;&gt;&gt; module.state_dict().keys()</span>

<span class="s2">            [&#39;bias&#39;, &#39;weight&#39;]</span>

<span class="s2">        &quot;&quot;&quot;</span>

        <span class="s s-Atom">if</span> <span class="s s-Atom">destination</span> <span class="o">is</span> <span class="nv">None</span><span class="s s-Atom">:</span>

            <span class="s s-Atom">destination</span> <span class="o">=</span> <span class="nv">OrderedDict</span><span class="p">()</span>

            <span class="s s-Atom">destination</span><span class="p">.</span><span class="k">_</span><span class="s s-Atom">metadata</span> <span class="o">=</span> <span class="nv">OrderedDict</span><span class="p">()</span>

        <span class="s s-Atom">destination</span><span class="p">.</span><span class="k">_</span><span class="s s-Atom">metadata</span><span class="p">[</span><span class="s s-Atom">prefix</span><span class="p">[:-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="s s-Atom">local_metadata</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span><span class="s s-Atom">version</span><span class="o">=</span><span class="s s-Atom">self</span><span class="p">.</span><span class="k">_</span><span class="s s-Atom">version</span><span class="p">)</span>

        <span class="s s-Atom">self</span><span class="p">.</span><span class="k">_</span><span class="nf">save_to_state_dict</span><span class="p">(</span><span class="s s-Atom">destination</span><span class="p">,</span> <span class="s s-Atom">prefix</span><span class="p">,</span> <span class="s s-Atom">keep_vars</span><span class="p">)</span>

        <span class="s s-Atom">for</span> <span class="s s-Atom">name</span><span class="p">,</span> <span class="s s-Atom">module</span> <span class="s s-Atom">in</span> <span class="s s-Atom">self</span><span class="p">.</span><span class="k">_</span><span class="s s-Atom">modules</span><span class="p">.</span><span class="nf">items</span><span class="p">()</span><span class="s s-Atom">:</span>

            <span class="s s-Atom">if</span> <span class="s s-Atom">module</span> <span class="o">is</span> <span class="o">not</span> <span class="nv">None</span><span class="s s-Atom">:</span>

                <span class="s s-Atom">module</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(</span><span class="s s-Atom">destination</span><span class="p">,</span> <span class="s s-Atom">prefix</span> <span class="o">+</span> <span class="s s-Atom">name</span> <span class="o">+</span> <span class="s s-Atom">&#39;.&#39;</span><span class="p">,</span> <span class="s s-Atom">keep_vars</span><span class="o">=</span><span class="s s-Atom">keep_vars</span><span class="p">)</span>

        <span class="s s-Atom">for</span> <span class="s s-Atom">hook</span> <span class="s s-Atom">in</span> <span class="s s-Atom">self</span><span class="p">.</span><span class="k">_</span><span class="s s-Atom">state_dict_hooks</span><span class="p">.</span><span class="nf">values</span><span class="p">()</span><span class="s s-Atom">:</span>

            <span class="s s-Atom">hook_result</span> <span class="o">=</span> <span class="nf">hook</span><span class="p">(</span><span class="s s-Atom">self</span><span class="p">,</span> <span class="s s-Atom">destination</span><span class="p">,</span> <span class="s s-Atom">prefix</span><span class="p">,</span> <span class="s s-Atom">local_metadata</span><span class="p">)</span>

            <span class="s s-Atom">if</span> <span class="s s-Atom">hook_result</span> <span class="o">is</span> <span class="o">not</span> <span class="nv">None</span><span class="s s-Atom">:</span>

                <span class="s s-Atom">destination</span> <span class="o">=</span> <span class="s s-Atom">hook_result</span>

        <span class="s s-Atom">return</span> <span class="s s-Atom">destination</span>
</code></pre></div>

</details>
<h5 id="summarize">summarize</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">summarize</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">mode</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="s1">&#39;top&#39;</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pytorch_lightning</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">ModelSummary</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">summarize</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="k">mode</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="n">ModelSummary</span><span class="p">.</span><span class="n">MODE_DEFAULT</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ModelSummary</span><span class="p">:</span>

        <span class="n">model_summary</span> <span class="o">=</span> <span class="n">ModelSummary</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="k">mode</span><span class="o">=</span><span class="k">mode</span><span class="p">)</span>

        <span class="n">log</span><span class="p">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;\n&#39;</span> <span class="o">+</span> <span class="n">str</span><span class="p">(</span><span class="n">model_summary</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">model_summary</span>
</code></pre></div>

</details>
<h5 id="tbptt_split_batch">tbptt_split_batch</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">tbptt_split_batch</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">batch</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">split_size</span><span class="p">:</span><span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span>
</code></pre></div>

<p>When using truncated backpropagation through time, each batch must be split along the
time dimension. Lightning handles this by default, but for custom behavior override
this function.</p>
<p>Args:
    batch: Current batch
    split_size: The size of the split</p>
<p>Return:
    List of batch splits. Each split will be passed to :meth:<code>training_step</code> to enable truncated
    back propagation through time. The default implementation splits root level Tensors and
    Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length.</p>
<p>Examples:
    .. code-block:: python</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">tbptt_split_batch</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="n">split_size</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">      </span><span class="n">splits</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span><span class="w"></span>
<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">time_dims</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">split_size</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">          </span><span class="n">batch_split</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span><span class="w"></span>
<span class="w">          </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">              </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">                  </span><span class="n">split_x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="o">[</span><span class="n">:, t:t + split_size</span><span class="o">]</span><span class="w"></span>
<span class="w">              </span><span class="n">elif</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">collections</span><span class="p">.</span><span class="k">Sequence</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">                  </span><span class="n">split_x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">None</span><span class="o">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"></span>
<span class="w">                  </span><span class="k">for</span><span class="w"> </span><span class="n">batch_idx</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="err">:</span><span class="w"></span>
<span class="w">                      </span><span class="n">split_x</span><span class="o">[</span><span class="n">batch_idx</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="o">[</span><span class="n">batch_idx</span><span class="o">][</span><span class="n">t:t + split_size</span><span class="o">]</span><span class="w"></span>

<span class="w">              </span><span class="n">batch_split</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">split_x</span><span class="p">)</span><span class="w"></span>

<span class="w">          </span><span class="n">splits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_split</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">splits</span><span class="w"></span>
</code></pre></div>

<p>Note:
    Called in the training loop after
    :meth:<code>~pytorch_lightning.callbacks.base.Callback.on_batch_start</code>
    if :paramref:<code>~pytorch_lightning.trainer.Trainer.truncated_bptt_steps</code> &gt; 0.
    Each returned batch split is passed separately to :meth:<code>training_step</code>.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">tbptt_split_batch</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">batch</span><span class="p">:</span><span class="w"> </span><span class="n">Tensor</span><span class="p">,</span><span class="w"> </span><span class="nl">split_size</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nl">list</span><span class="p">:</span><span class="w"></span>

<span class="w">        </span><span class="n">r</span><span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        When using truncated backpropagation through time, each batch must be split along the</span>

<span class="ss">        time dimension. Lightning handles this by default, but for custom behavior override</span>

<span class="ss">        this function.</span>

<span class="ss">        Args:</span>

<span class="ss">            batch: Current batch</span>

<span class="ss">            split_size: The size of the split</span>

<span class="ss">        Return:</span>

<span class="ss">            List of batch splits. Each split will be passed to :meth:`training_step` to enable truncated</span>

<span class="ss">            back propagation through time. The default implementation splits root level Tensors and</span>

<span class="ss">            Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length.</span>

<span class="ss">        Examples:</span>

<span class="ss">            .. code-block:: python</span>

<span class="ss">                def tbptt_split_batch(self, batch, split_size):</span>

<span class="ss">                  splits = []</span>

<span class="ss">                  for t in range(0, time_dims[0], split_size):</span>

<span class="ss">                      batch_split = []</span>

<span class="ss">                      for i, x in enumerate(batch):</span>

<span class="ss">                          if isinstance(x, torch.Tensor):</span>

<span class="ss">                              split_x = x[:, t:t + split_size]</span>

<span class="ss">                          elif isinstance(x, collections.Sequence):</span>

<span class="ss">                              split_x = [None] * len(x)</span>

<span class="ss">                              for batch_idx in range(len(x)):</span>

<span class="ss">                                  split_x[batch_idx] = x[batch_idx][t:t + split_size]</span>

<span class="ss">                          batch_split.append(split_x)</span>

<span class="ss">                      splits.append(batch_split)</span>

<span class="ss">                  return splits</span>

<span class="ss">        Note:</span>

<span class="ss">            Called in the training loop after</span>

<span class="ss">            :meth:`~pytorch_lightning.callbacks.base.Callback.on_batch_start`</span>

<span class="ss">            if :paramref:`~pytorch_lightning.trainer.Trainer.truncated_bptt_steps` &gt; 0.</span>

<span class="ss">            Each returned batch split is passed separately to :meth:`training_step`.</span>

<span class="ss">        &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">time_dims</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">len(x[0</span><span class="o">]</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span><span class="w"> </span><span class="n">collections</span><span class="p">.</span><span class="k">Sequence</span><span class="p">))</span><span class="err">]</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">time_dims</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;Unable to determine batch time dimension&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="ow">all</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">time_dims</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">time_dims</span><span class="p">),</span><span class="w"> </span><span class="ss">&quot;Batch time dimension length is ambiguous&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">splits</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span><span class="w"></span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">time_dims</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">split_size</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">batch_split</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span><span class="w"></span>

<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">                    </span><span class="n">split_x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="o">[</span><span class="n">:, t:t + split_size</span><span class="o">]</span><span class="w"></span>

<span class="w">                </span><span class="n">elif</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">collections</span><span class="p">.</span><span class="k">Sequence</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">                    </span><span class="n">split_x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">None</span><span class="o">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"></span>

<span class="w">                    </span><span class="k">for</span><span class="w"> </span><span class="n">batch_idx</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="err">:</span><span class="w"></span>

<span class="w">                        </span><span class="n">split_x</span><span class="o">[</span><span class="n">batch_idx</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="o">[</span><span class="n">batch_idx</span><span class="o">][</span><span class="n">t:t + split_size</span><span class="o">]</span><span class="w"></span>

<span class="w">                </span><span class="n">batch_split</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">split_x</span><span class="p">)</span><span class="w"></span>

<span class="w">            </span><span class="n">splits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_split</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">splits</span><span class="w"></span>
</code></pre></div>

</details>
<h5 id="teardown">teardown</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">teardown</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">stage</span><span class="p">:</span><span class="nb">str</span>
<span class="p">)</span>
</code></pre></div>

<p>Called at the end of fit and test.</p>
<p>Args:
    stage: either 'fit' or 'test'</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">teardown</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">:</span> <span class="n">str</span><span class="p">):</span>

        <span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Called at the end of fit and test.</span>

<span class="ss">        Args:</span>

<span class="ss">            stage: either &#39;fit&#39; or &#39;test&#39;</span>

<span class="ss">        &quot;&quot;&quot;</span>
</code></pre></div>

</details>
<h5 id="test_dataloader">test_dataloader</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">]]</span>
</code></pre></div>

<p>Implement one or multiple PyTorch DataLoaders for testing.</p>
<p>The dataloader you return will not be called every epoch unless you set
:paramref:<code>~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch</code> to <code>True</code>.</p>
<p>For data processing use the following pattern:</p>
<div class="codehilite"><pre><span></span><code><span class="o">-</span> <span class="n">download</span> <span class="k">in</span> <span class="o">:</span><span class="n">meth</span><span class="o">:</span><span class="n">`prepare_data`</span>
<span class="o">-</span> <span class="k">process</span> <span class="k">and</span> <span class="n">split</span> <span class="k">in</span> <span class="o">:</span><span class="n">meth</span><span class="o">:</span><span class="n">`setup`</span>
</code></pre></div>

<p>However, the above are only necessary for distributed processing.</p>
<p>.. warning:: do not assign state in prepare_data</p>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.Trainer.fit</code></li>
<li>...</li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
<li>:meth:<code>train_dataloader</code></li>
<li>:meth:<code>val_dataloader</code></li>
<li>:meth:<code>test_dataloader</code></li>
</ul>
<p>Note:
    Lightning adds the correct sampler for distributed and arbitrary hardware.
    There is no need to set it yourself.</p>
<p>Return:
    Single or multiple PyTorch DataLoaders.</p>
<p>Example:
    .. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                        <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                        <span class="n">download</span><span class="o">=</span><span class="n">True</span><span class="p">)</span>
        <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="n">False</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">loader</span>
</code></pre></div>

<p>Note:
    If you don't need a test dataset and a :meth:<code>test_step</code>, you don't need to implement
    this method.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">test_dataloader</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">Union</span><span class="err">[</span><span class="n">DataLoader</span><span class="p">,</span> <span class="k">List</span><span class="err">[</span><span class="n">DataLoader</span><span class="err">]]</span><span class="o">:</span>

        <span class="n">r</span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2"></span>

<span class="s2">        Implement one or multiple PyTorch DataLoaders for testing.</span>

<span class="s2">        The dataloader you return will not be called every epoch unless you set</span>

<span class="s2">        :paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch` to ``True``.</span>

<span class="s2">        For data processing use the following pattern:</span>

<span class="s2">            - download in :meth:`prepare_data`</span>

<span class="s2">            - process and split in :meth:`setup`</span>

<span class="s2">        However, the above are only necessary for distributed processing.</span>

<span class="s2">        .. warning:: do not assign state in prepare_data</span>

<span class="s2">        - :meth:`~pytorch_lightning.trainer.Trainer.fit`</span>

<span class="s2">        - ...</span>

<span class="s2">        - :meth:`prepare_data`</span>

<span class="s2">        - :meth:`setup`</span>

<span class="s2">        - :meth:`train_dataloader`</span>

<span class="s2">        - :meth:`val_dataloader`</span>

<span class="s2">        - :meth:`test_dataloader`</span>

<span class="s2">        Note:</span>

<span class="s2">            Lightning adds the correct sampler for distributed and arbitrary hardware.</span>

<span class="s2">            There is no need to set it yourself.</span>

<span class="s2">        Return:</span>

<span class="s2">            Single or multiple PyTorch DataLoaders.</span>

<span class="s2">        Example:</span>

<span class="s2">            .. code-block:: python</span>

<span class="s2">                def test_dataloader(self):</span>

<span class="s2">                    transform = transforms.Compose([transforms.ToTensor(),</span>

<span class="s2">                                                    transforms.Normalize((0.5,), (1.0,))])</span>

<span class="s2">                    dataset = MNIST(root=&#39;/path/to/mnist/&#39;, train=False, transform=transform,</span>

<span class="s2">                                    download=True)</span>

<span class="s2">                    loader = torch.utils.data.DataLoader(</span>

<span class="s2">                        dataset=dataset,</span>

<span class="s2">                        batch_size=self.batch_size,</span>

<span class="s2">                        shuffle=False</span>

<span class="s2">                    )</span>

<span class="s2">                    return loader</span>

<span class="s2">        Note:</span>

<span class="s2">            If you don&#39;t need a test dataset and a :meth:`test_step`, you don&#39;t need to implement</span>

<span class="s2">            this method.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>
</code></pre></div>

</details>
<h5 id="test_end">test_end</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">test_end</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">outputs</span>
<span class="p">)</span>
</code></pre></div>

<p>Warnings:
Deprecated in v0.7.0. Use :meth:<code>test_epoch_end</code> instead.
Will be removed in 1.0.0.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">test_end</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span><span class="o">:</span>

        <span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2"></span>

<span class="s2">        Warnings:</span>

<span class="s2">             Deprecated in v0.7.0. Use :meth:`test_epoch_end` instead.</span>

<span class="s2">             Will be removed in 1.0.0.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>
</code></pre></div>

</details>
<h5 id="test_epoch_end">test_epoch_end</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">outputs</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
</code></pre></div>

<p>Called at the end of a test epoch with the output of all test steps.</p>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code><span class="err"># the pseudocode for these calls</span>
<span class="err">test_outs = []</span>
<span class="err">for test_batch in test_data:</span>
<span class="err">    out = test_step(test_batch)</span>
<span class="err">    test_outs.append(out)</span>
<span class="err">test_epoch_end(test_outs)</span>
</code></pre></div>

<p>Args:
    outputs: List of outputs you defined in :meth:<code>test_step_end</code>, or if there
        are multiple dataloaders, a list containing a list of outputs for each dataloader</p>
<p>Return:
    Dict or OrderedDict: Dict has the following optional keys:</p>
<div class="codehilite"><pre><span></span><code><span class="err">- progress_bar -&gt; Dict for progress bar display. Must have only tensors.</span>
<span class="err">- log -&gt; Dict of metrics to add to logger. Must have only tensors (no images, etc).</span>
</code></pre></div>

<p>Note:
    If you didn't define a :meth:<code>test_step</code>, this won't be called.</p>
<ul>
<li>The outputs here are strictly for logging or progress bar.</li>
<li>If you don't need to display anything, don't return anything.</li>
<li>If you want to manually set current step, specify it with the 'step' key in the 'log' Dict</li>
</ul>
<p>Examples:
    With a single dataloader:</p>
<div class="codehilite"><pre><span></span><code><span class="p">..</span> <span class="k">code</span><span class="o">-</span><span class="k">block</span><span class="o">::</span> <span class="n">python</span>

    <span class="n">def</span> <span class="n">test_epoch_end</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span><span class="o">:</span>
        <span class="n">test_acc_mean</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">output</span> <span class="k">in</span> <span class="n">outputs</span><span class="o">:</span>
            <span class="n">test_acc_mean</span> <span class="o">+=</span> <span class="n">output</span><span class="err">[</span><span class="s1">&#39;test_acc&#39;</span><span class="err">]</span>

        <span class="n">test_acc_mean</span> <span class="o">/=</span> <span class="n">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="n">tqdm_dict</span> <span class="o">=</span> <span class="err">{</span><span class="s1">&#39;test_acc&#39;</span><span class="o">:</span> <span class="n">test_acc_mean</span><span class="p">.</span><span class="n">item</span><span class="p">()</span><span class="err">}</span>

        <span class="c1"># show test_loss and test_acc in progress bar but only log test_loss</span>
        <span class="n">results</span> <span class="o">=</span> <span class="err">{</span>
            <span class="s1">&#39;progress_bar&#39;</span><span class="o">:</span> <span class="n">tqdm_dict</span><span class="p">,</span>
            <span class="s1">&#39;log&#39;</span><span class="o">:</span> <span class="err">{</span><span class="s1">&#39;test_acc&#39;</span><span class="o">:</span> <span class="n">test_acc_mean</span><span class="p">.</span><span class="n">item</span><span class="p">()</span><span class="err">}</span>
        <span class="err">}</span>
        <span class="k">return</span> <span class="n">results</span>

<span class="k">With</span> <span class="n">multiple</span> <span class="n">dataloaders</span><span class="p">,</span> <span class="n">`outputs`</span> <span class="n">will</span> <span class="n">be</span> <span class="n">a</span> <span class="k">list</span> <span class="k">of</span> <span class="n">lists</span><span class="p">.</span> <span class="n">The</span> <span class="k">outer</span> <span class="k">list</span> <span class="k">contains</span>
<span class="k">one</span> <span class="n">entry</span> <span class="n">per</span> <span class="n">dataloader</span><span class="p">,</span> <span class="k">while</span> <span class="n">the</span> <span class="k">inner</span> <span class="k">list</span> <span class="k">contains</span> <span class="n">the</span> <span class="n">individual</span> <span class="n">outputs</span> <span class="k">of</span>
<span class="k">each</span> <span class="n">test</span> <span class="n">step</span> <span class="k">for</span> <span class="n">that</span> <span class="n">dataloader</span><span class="p">.</span>

<span class="p">..</span> <span class="k">code</span><span class="o">-</span><span class="k">block</span><span class="o">::</span> <span class="n">python</span>

    <span class="n">def</span> <span class="n">test_epoch_end</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span><span class="o">:</span>
        <span class="n">test_acc_mean</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">dataloader_outputs</span> <span class="k">in</span> <span class="n">outputs</span><span class="o">:</span>
            <span class="k">for</span> <span class="n">output</span> <span class="k">in</span> <span class="n">dataloader_outputs</span><span class="o">:</span>
                <span class="n">test_acc_mean</span> <span class="o">+=</span> <span class="n">output</span><span class="err">[</span><span class="s1">&#39;test_acc&#39;</span><span class="err">]</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">test_acc_mean</span> <span class="o">/=</span> <span class="n">i</span>
        <span class="n">tqdm_dict</span> <span class="o">=</span> <span class="err">{</span><span class="s1">&#39;test_acc&#39;</span><span class="o">:</span> <span class="n">test_acc_mean</span><span class="p">.</span><span class="n">item</span><span class="p">()</span><span class="err">}</span>

        <span class="c1"># show test_loss and test_acc in progress bar but only log test_loss</span>
        <span class="n">results</span> <span class="o">=</span> <span class="err">{</span>
            <span class="s1">&#39;progress_bar&#39;</span><span class="o">:</span> <span class="n">tqdm_dict</span><span class="p">,</span>
            <span class="s1">&#39;log&#39;</span><span class="o">:</span> <span class="err">{</span><span class="s1">&#39;test_acc&#39;</span><span class="o">:</span> <span class="n">test_acc_mean</span><span class="p">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">&#39;step&#39;</span><span class="o">:</span> <span class="n">self</span><span class="p">.</span><span class="n">current_epoch</span><span class="err">}</span>
        <span class="err">}</span>
        <span class="k">return</span> <span class="n">results</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">test_epoch_end</span><span class="p">(</span>

            <span class="n">self</span><span class="p">,</span>

            <span class="n">outputs</span><span class="o">:</span> <span class="k">Union</span><span class="err">[</span><span class="k">List</span><span class="err">[</span><span class="n">Dict</span><span class="err">[</span><span class="n">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="err">]]</span><span class="p">,</span> <span class="k">List</span><span class="err">[</span><span class="k">List</span><span class="err">[</span><span class="n">Dict</span><span class="err">[</span><span class="n">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="err">]]]]</span>

    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="err">[</span><span class="n">str</span><span class="p">,</span> <span class="n">Dict</span><span class="err">[</span><span class="n">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="err">]]</span><span class="o">:</span>

        <span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2"></span>

<span class="s2">        Called at the end of a test epoch with the output of all test steps.</span>

<span class="s2">        .. code-block:: python</span>

<span class="s2">            # the pseudocode for these calls</span>

<span class="s2">            test_outs = []</span>

<span class="s2">            for test_batch in test_data:</span>

<span class="s2">                out = test_step(test_batch)</span>

<span class="s2">                test_outs.append(out)</span>

<span class="s2">            test_epoch_end(test_outs)</span>

<span class="s2">        Args:</span>

<span class="s2">            outputs: List of outputs you defined in :meth:`test_step_end`, or if there</span>

<span class="s2">                are multiple dataloaders, a list containing a list of outputs for each dataloader</span>

<span class="s2">        Return:</span>

<span class="s2">            Dict or OrderedDict: Dict has the following optional keys:</span>

<span class="s2">            - progress_bar -&gt; Dict for progress bar display. Must have only tensors.</span>

<span class="s2">            - log -&gt; Dict of metrics to add to logger. Must have only tensors (no images, etc).</span>

<span class="s2">        Note:</span>

<span class="s2">            If you didn&#39;t define a :meth:`test_step`, this won&#39;t be called.</span>

<span class="s2">        - The outputs here are strictly for logging or progress bar.</span>

<span class="s2">        - If you don&#39;t need to display anything, don&#39;t return anything.</span>

<span class="s2">        - If you want to manually set current step, specify it with the &#39;step&#39; key in the &#39;log&#39; Dict</span>

<span class="s2">        Examples:</span>

<span class="s2">            With a single dataloader:</span>

<span class="s2">            .. code-block:: python</span>

<span class="s2">                def test_epoch_end(self, outputs):</span>

<span class="s2">                    test_acc_mean = 0</span>

<span class="s2">                    for output in outputs:</span>

<span class="s2">                        test_acc_mean += output[&#39;test_acc&#39;]</span>

<span class="s2">                    test_acc_mean /= len(outputs)</span>

<span class="s2">                    tqdm_dict = {&#39;test_acc&#39;: test_acc_mean.item()}</span>

<span class="s2">                    # show test_loss and test_acc in progress bar but only log test_loss</span>

<span class="s2">                    results = {</span>

<span class="s2">                        &#39;progress_bar&#39;: tqdm_dict,</span>

<span class="s2">                        &#39;log&#39;: {&#39;test_acc&#39;: test_acc_mean.item()}</span>

<span class="s2">                    }</span>

<span class="s2">                    return results</span>

<span class="s2">            With multiple dataloaders, `outputs` will be a list of lists. The outer list contains</span>

<span class="s2">            one entry per dataloader, while the inner list contains the individual outputs of</span>

<span class="s2">            each test step for that dataloader.</span>

<span class="s2">            .. code-block:: python</span>

<span class="s2">                def test_epoch_end(self, outputs):</span>

<span class="s2">                    test_acc_mean = 0</span>

<span class="s2">                    i = 0</span>

<span class="s2">                    for dataloader_outputs in outputs:</span>

<span class="s2">                        for output in dataloader_outputs:</span>

<span class="s2">                            test_acc_mean += output[&#39;test_acc&#39;]</span>

<span class="s2">                            i += 1</span>

<span class="s2">                    test_acc_mean /= i</span>

<span class="s2">                    tqdm_dict = {&#39;test_acc&#39;: test_acc_mean.item()}</span>

<span class="s2">                    # show test_loss and test_acc in progress bar but only log test_loss</span>

<span class="s2">                    results = {</span>

<span class="s2">                        &#39;progress_bar&#39;: tqdm_dict,</span>

<span class="s2">                        &#39;log&#39;: {&#39;test_acc&#39;: test_acc_mean.item(), &#39;step&#39;: self.current_epoch}</span>

<span class="s2">                    }</span>

<span class="s2">                    return results</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>
</code></pre></div>

</details>
<h5 id="test_step">test_step</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
</code></pre></div>

<p>Operates on a single batch of data from the test set.
In this step you'd normally generate examples or calculate anything of interest
such as accuracy.</p>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code><span class="err"># the pseudocode for these calls</span>
<span class="err">test_outs = []</span>
<span class="err">for test_batch in test_data:</span>
<span class="err">    out = test_step(test_batch)</span>
<span class="err">    test_outs.append(out)</span>
<span class="err">test_epoch_end(test_outs)</span>
</code></pre></div>

<p>Args:
    batch (:class:<code>~torch.Tensor</code> | (:class:<code>~torch.Tensor</code>, ...) | [:class:<code>~torch.Tensor</code>, ...]):
        The output of your :class:<code>~torch.utils.data.DataLoader</code>. A tensor, tuple or list.
    batch_idx (int): The index of this batch.
    dataloader_idx (int): The index of the dataloader that produced this batch
        (only if multiple test datasets used).</p>
<p>Return:
    Dict or OrderedDict - passed to the :meth:<code>test_epoch_end</code> method.
    If you defined :meth:<code>test_step_end</code> it will go to that first.</p>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># if you have one test dataloader:</span>
<span class="n">def</span> <span class="n">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

<span class="c1"># if you have multiple test dataloaders:</span>
<span class="n">def</span> <span class="n">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
</code></pre></div>

<p>Examples:
    .. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>    <span class="c1"># CASE 1: A single test dataset</span>
    <span class="n">def</span> <span class="n">test_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span><span class="o">:</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="c1"># implement your own</span>
        <span class="k">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">loss</span><span class="p">(</span><span class="k">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="c1"># log 6 example images</span>
        <span class="c1"># or generated text... or whatever</span>
        <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="err">[</span><span class="o">:</span><span class="mi">6</span><span class="err">]</span>
        <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">logger</span><span class="p">.</span><span class="n">experiment</span><span class="p">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># calculate acc</span>
        <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="k">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">val_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">).</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

        <span class="c1"># all optional...</span>
        <span class="c1"># return whatever you need for the collation function test_epoch_end</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">(</span><span class="err">{</span>
            <span class="s1">&#39;val_loss&#39;</span><span class="o">:</span> <span class="n">loss_val</span><span class="p">,</span>
            <span class="s1">&#39;val_acc&#39;</span><span class="o">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">val_acc</span><span class="p">),</span> <span class="c1"># everything must be a tensor</span>
        <span class="err">}</span><span class="p">)</span>

        <span class="c1"># return an optional dict</span>
        <span class="k">return</span> <span class="n">output</span>

<span class="k">If</span> <span class="n">you</span> <span class="n">pass</span> <span class="k">in</span> <span class="n">multiple</span> <span class="k">validation</span> <span class="n">datasets</span><span class="p">,</span> <span class="o">:</span><span class="n">meth</span><span class="o">:</span><span class="n">`test_step`</span> <span class="n">will</span> <span class="n">have</span> <span class="n">an</span> <span class="n">additional</span>
<span class="n">argument</span><span class="p">.</span>

<span class="p">..</span> <span class="k">code</span><span class="o">-</span><span class="k">block</span><span class="o">::</span> <span class="n">python</span>

    <span class="c1"># CASE 2: multiple test datasets</span>
    <span class="n">def</span> <span class="n">test_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataset_idx</span><span class="p">)</span><span class="o">:</span>
        <span class="c1"># dataset_idx tells you which dataset this is.</span>
</code></pre></div>

<p>Note:
    If you don't need to validate you don't need to implement this method.</p>
<p>Note:
    When the :meth:<code>test_step</code> is called, the model has been put in eval mode and
    PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
    to training mode and gradients are enabled.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">test_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="err">[</span><span class="n">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="err">]</span><span class="o">:</span>

        <span class="n">r</span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2"></span>

<span class="s2">        Operates on a single batch of data from the test set.</span>

<span class="s2">        In this step you&#39;d normally generate examples or calculate anything of interest</span>

<span class="s2">        such as accuracy.</span>

<span class="s2">        .. code-block:: python</span>

<span class="s2">            # the pseudocode for these calls</span>

<span class="s2">            test_outs = []</span>

<span class="s2">            for test_batch in test_data:</span>

<span class="s2">                out = test_step(test_batch)</span>

<span class="s2">                test_outs.append(out)</span>

<span class="s2">            test_epoch_end(test_outs)</span>

<span class="s2">        Args:</span>

<span class="s2">            batch (:class:`~torch.Tensor` | (:class:`~torch.Tensor`, ...) | [:class:`~torch.Tensor`, ...]):</span>

<span class="s2">                The output of your :class:`~torch.utils.data.DataLoader`. A tensor, tuple or list.</span>

<span class="s2">            batch_idx (int): The index of this batch.</span>

<span class="s2">            dataloader_idx (int): The index of the dataloader that produced this batch</span>

<span class="s2">                (only if multiple test datasets used).</span>

<span class="s2">        Return:</span>

<span class="s2">            Dict or OrderedDict - passed to the :meth:`test_epoch_end` method.</span>

<span class="s2">            If you defined :meth:`test_step_end` it will go to that first.</span>

<span class="s2">        .. code-block:: python</span>

<span class="s2">            # if you have one test dataloader:</span>

<span class="s2">            def test_step(self, batch, batch_idx)</span>

<span class="s2">            # if you have multiple test dataloaders:</span>

<span class="s2">            def test_step(self, batch, batch_idx, dataloader_idx)</span>

<span class="s2">        Examples:</span>

<span class="s2">            .. code-block:: python</span>

<span class="s2">                # CASE 1: A single test dataset</span>

<span class="s2">                def test_step(self, batch, batch_idx):</span>

<span class="s2">                    x, y = batch</span>

<span class="s2">                    # implement your own</span>

<span class="s2">                    out = self(x)</span>

<span class="s2">                    loss = self.loss(out, y)</span>

<span class="s2">                    # log 6 example images</span>

<span class="s2">                    # or generated text... or whatever</span>

<span class="s2">                    sample_imgs = x[:6]</span>

<span class="s2">                    grid = torchvision.utils.make_grid(sample_imgs)</span>

<span class="s2">                    self.logger.experiment.add_image(&#39;example_images&#39;, grid, 0)</span>

<span class="s2">                    # calculate acc</span>

<span class="s2">                    labels_hat = torch.argmax(out, dim=1)</span>

<span class="s2">                    val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)</span>

<span class="s2">                    # all optional...</span>

<span class="s2">                    # return whatever you need for the collation function test_epoch_end</span>

<span class="s2">                    output = OrderedDict({</span>

<span class="s2">                        &#39;val_loss&#39;: loss_val,</span>

<span class="s2">                        &#39;val_acc&#39;: torch.tensor(val_acc), # everything must be a tensor</span>

<span class="s2">                    })</span>

<span class="s2">                    # return an optional dict</span>

<span class="s2">                    return output</span>

<span class="s2">            If you pass in multiple validation datasets, :meth:`test_step` will have an additional</span>

<span class="s2">            argument.</span>

<span class="s2">            .. code-block:: python</span>

<span class="s2">                # CASE 2: multiple test datasets</span>

<span class="s2">                def test_step(self, batch, batch_idx, dataset_idx):</span>

<span class="s2">                    # dataset_idx tells you which dataset this is.</span>

<span class="s2">        Note:</span>

<span class="s2">            If you don&#39;t need to validate you don&#39;t need to implement this method.</span>

<span class="s2">        Note:</span>

<span class="s2">            When the :meth:`test_step` is called, the model has been put in eval mode and</span>

<span class="s2">            PyTorch gradients have been disabled. At the end of the test epoch, the model goes back</span>

<span class="s2">            to training mode and gradients are enabled.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>
</code></pre></div>

</details>
<h5 id="test_step_end">test_step_end</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">test_step_end</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
</code></pre></div>

<p>Use this when testing with dp or ddp2 because :meth:<code>test_step</code> will operate
on only part of the batch. However, this is still optional
and only needed for things like softmax or NCE loss.</p>
<p>Note:
    If you later switch to ddp or some other mode, this will still be called
    so that you don't have to change your code.</p>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code><span class="err"># pseudocode</span>
<span class="err">sub_batches = split_batches_for_dp(batch)</span>
<span class="err">batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches]</span>
<span class="err">test_step_end(batch_parts_outputs)</span>
</code></pre></div>

<p>Args:
    batch_parts_outputs: What you return in :meth:<code>test_step</code> for each batch part.</p>
<p>Return:
     Dict or OrderedDict - passed to the :meth:<code>test_epoch_end</code>.</p>
<p>Examples:
    .. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>    <span class="o">#</span> <span class="k">WITHOUT</span> <span class="n">test_step_end</span>
    <span class="o">#</span> <span class="k">if</span> <span class="n">used</span> <span class="k">in</span> <span class="n">DP</span> <span class="k">or</span> <span class="n">DDP2</span><span class="p">,</span> <span class="n">this</span> <span class="n">batch</span> <span class="k">is</span> <span class="mi">1</span><span class="o">/</span><span class="n">num_gpus</span> <span class="k">large</span>
    <span class="n">def</span> <span class="n">test_step</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="o">#</span> <span class="n">batch</span> <span class="k">is</span> <span class="mi">1</span><span class="o">/</span><span class="n">num_gpus</span> <span class="n">big</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="k">out</span> <span class="o">=</span> <span class="k">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="k">out</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="err">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="err">}</span>

    <span class="o">#</span> <span class="c1">--------------</span>
    <span class="o">#</span> <span class="k">with</span> <span class="n">test_step_end</span> <span class="k">to</span> <span class="k">do</span> <span class="n">softmax</span> <span class="n">over</span> <span class="n">the</span> <span class="k">full</span> <span class="n">batch</span>
    <span class="n">def</span> <span class="n">test_step</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="o">#</span> <span class="n">batch</span> <span class="k">is</span> <span class="mi">1</span><span class="o">/</span><span class="n">num_gpus</span> <span class="n">big</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="k">out</span> <span class="o">=</span> <span class="k">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="err">{</span><span class="s1">&#39;out&#39;</span><span class="p">:</span> <span class="k">out</span><span class="err">}</span>

    <span class="n">def</span> <span class="n">test_step_end</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="o">#</span> <span class="n">this</span> <span class="k">out</span> <span class="k">is</span> <span class="n">now</span> <span class="n">the</span> <span class="k">full</span> <span class="k">size</span> <span class="k">of</span> <span class="n">the</span> <span class="n">batch</span>
        <span class="k">out</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;out&#39;</span><span class="p">]</span>

        <span class="o">#</span> <span class="n">this</span> <span class="n">softmax</span> <span class="n">now</span> <span class="n">uses</span> <span class="n">the</span> <span class="k">full</span> <span class="n">batch</span> <span class="k">size</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="err">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="err">}</span>
</code></pre></div>

<p>See Also:
    See the :ref:<code>multi-gpu-training</code> guide for more details.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">test_step_end</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="err">[</span><span class="n">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="err">]</span><span class="o">:</span>

        <span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2"></span>

<span class="s2">        Use this when testing with dp or ddp2 because :meth:`test_step` will operate</span>

<span class="s2">        on only part of the batch. However, this is still optional</span>

<span class="s2">        and only needed for things like softmax or NCE loss.</span>

<span class="s2">        Note:</span>

<span class="s2">            If you later switch to ddp or some other mode, this will still be called</span>

<span class="s2">            so that you don&#39;t have to change your code.</span>

<span class="s2">        .. code-block:: python</span>

<span class="s2">            # pseudocode</span>

<span class="s2">            sub_batches = split_batches_for_dp(batch)</span>

<span class="s2">            batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches]</span>

<span class="s2">            test_step_end(batch_parts_outputs)</span>

<span class="s2">        Args:</span>

<span class="s2">            batch_parts_outputs: What you return in :meth:`test_step` for each batch part.</span>

<span class="s2">        Return:</span>

<span class="s2">             Dict or OrderedDict - passed to the :meth:`test_epoch_end`.</span>

<span class="s2">        Examples:</span>

<span class="s2">            .. code-block:: python</span>

<span class="s2">                # WITHOUT test_step_end</span>

<span class="s2">                # if used in DP or DDP2, this batch is 1/num_gpus large</span>

<span class="s2">                def test_step(self, batch, batch_idx):</span>

<span class="s2">                    # batch is 1/num_gpus big</span>

<span class="s2">                    x, y = batch</span>

<span class="s2">                    out = self(x)</span>

<span class="s2">                    loss = self.softmax(out)</span>

<span class="s2">                    loss = nce_loss(loss)</span>

<span class="s2">                    return {&#39;loss&#39;: loss}</span>

<span class="s2">                # --------------</span>

<span class="s2">                # with test_step_end to do softmax over the full batch</span>

<span class="s2">                def test_step(self, batch, batch_idx):</span>

<span class="s2">                    # batch is 1/num_gpus big</span>

<span class="s2">                    x, y = batch</span>

<span class="s2">                    out = self(x)</span>

<span class="s2">                    return {&#39;out&#39;: out}</span>

<span class="s2">                def test_step_end(self, outputs):</span>

<span class="s2">                    # this out is now the full size of the batch</span>

<span class="s2">                    out = outputs[&#39;out&#39;]</span>

<span class="s2">                    # this softmax now uses the full batch size</span>

<span class="s2">                    loss = nce_loss(loss)</span>

<span class="s2">                    return {&#39;loss&#39;: loss}</span>

<span class="s2">        See Also:</span>

<span class="s2">            See the :ref:`multi-gpu-training` guide for more details.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>
</code></pre></div>

</details>
<h5 id="tng_dataloader">tng_dataloader</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">tng_dataloader</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>

<p>Warnings:
Deprecated in v0.5.0. Use :meth:<code>train_dataloader</code> instead. Will be removed in 1.0.0.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">tng_dataloader</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="o">:</span>  <span class="c1"># todo: remove in v1.0.0</span>

        <span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2"></span>

<span class="s2">        Warnings:</span>

<span class="s2">            Deprecated in v0.5.0. Use :meth:`train_dataloader` instead. Will be removed in 1.0.0.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">train_dataloader</span><span class="p">()</span>

        <span class="n">rank_zero_warn</span><span class="p">(</span><span class="s2">&quot;`tng_dataloader` has been renamed to `train_dataloader` since v0.5.0.&quot;</span>

                       <span class="s2">&quot; and this method will be removed in v1.0.0&quot;</span><span class="p">,</span> <span class="n">DeprecationWarning</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>
</code></pre></div>

</details>
<h5 id="to">to</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">to</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span>
</code></pre></div>

<p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as
.. function:: to(device=None, dtype=None, non_blocking=False)
.. function:: to(dtype, non_blocking=False)
.. function:: to(tensor, non_blocking=False)
Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts
floating point desired :attr:<code>dtype</code> s. In addition, this method will
only cast the floating point parameters and buffers to :attr:<code>dtype</code>
(if given). The integral parameters and buffers will be moved
:attr:<code>device</code>, if that is given, but with dtypes unchanged. When
:attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.
See below for examples.</p>
<p>Note:
    This method modifies the module in-place.</p>
<p>Args:
    device: the desired device of the parameters
        and buffers in this module
    dtype: the desired floating point type of
        the floating point parameters and buffers in this module
    tensor: Tensor whose dtype and device are the desired
        dtype and device for all parameters and buffers in this module</p>
<p>Returns:
    Module: self</p>
<p>Example::
    &gt;&gt;&gt; class ExampleModule(DeviceDtypeModuleMixin):
    ...     def <strong>init</strong>(self, weight: torch.Tensor):
    ...         super().<strong>init</strong>()
    ...         self.register_buffer('weight', weight)
    &gt;&gt;&gt; _ = torch.manual_seed(0)
    &gt;&gt;&gt; module = ExampleModule(torch.rand(3, 4))
    &gt;&gt;&gt; module.weight #doctest: +ELLIPSIS
    tensor([[...]])
    &gt;&gt;&gt; module.to(torch.double)
    ExampleModule()
    &gt;&gt;&gt; module.weight #doctest: +ELLIPSIS
    tensor([[...]], dtype=torch.float64)
    &gt;&gt;&gt; cpu = torch.device('cpu')
    &gt;&gt;&gt; module.to(cpu, dtype=torch.half, non_blocking=True)
    ExampleModule()
    &gt;&gt;&gt; module.weight #doctest: +ELLIPSIS
    tensor([[...]], dtype=torch.float16)
    &gt;&gt;&gt; module.to(cpu)
    ExampleModule()
    &gt;&gt;&gt; module.weight #doctest: +ELLIPSIS
    tensor([[...]], dtype=torch.float16)</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="k">to</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Module</span><span class="o">:</span>

        <span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Moves and/or casts the parameters and buffers.</span>

<span class="s2">        This can be called as</span>

<span class="s2">        .. function:: to(device=None, dtype=None, non_blocking=False)</span>

<span class="s2">        .. function:: to(dtype, non_blocking=False)</span>

<span class="s2">        .. function:: to(tensor, non_blocking=False)</span>

<span class="s2">        Its signature is similar to :meth:`torch.Tensor.to`, but only accepts</span>

<span class="s2">        floating point desired :attr:`dtype` s. In addition, this method will</span>

<span class="s2">        only cast the floating point parameters and buffers to :attr:`dtype`</span>

<span class="s2">        (if given). The integral parameters and buffers will be moved</span>

<span class="s2">        :attr:`device`, if that is given, but with dtypes unchanged. When</span>

<span class="s2">        :attr:`non_blocking` is set, it tries to convert/move asynchronously</span>

<span class="s2">        with respect to the host if possible, e.g., moving CPU Tensors with</span>

<span class="s2">        pinned memory to CUDA devices.</span>

<span class="s2">        See below for examples.</span>

<span class="s2">        Note:</span>

<span class="s2">            This method modifies the module in-place.</span>

<span class="s2">        Args:</span>

<span class="s2">            device: the desired device of the parameters</span>

<span class="s2">                and buffers in this module</span>

<span class="s2">            dtype: the desired floating point type of</span>

<span class="s2">                the floating point parameters and buffers in this module</span>

<span class="s2">            tensor: Tensor whose dtype and device are the desired</span>

<span class="s2">                dtype and device for all parameters and buffers in this module</span>

<span class="s2">        Returns:</span>

<span class="s2">            Module: self</span>

<span class="s2">        Example::</span>

<span class="s2">            &gt;&gt;&gt; class ExampleModule(DeviceDtypeModuleMixin):</span>

<span class="s2">            ...     def __init__(self, weight: torch.Tensor):</span>

<span class="s2">            ...         super().__init__()</span>

<span class="s2">            ...         self.register_buffer(&#39;weight&#39;, weight)</span>

<span class="s2">            &gt;&gt;&gt; _ = torch.manual_seed(0)</span>

<span class="s2">            &gt;&gt;&gt; module = ExampleModule(torch.rand(3, 4))</span>

<span class="s2">            &gt;&gt;&gt; module.weight #doctest: +ELLIPSIS</span>

<span class="s2">            tensor([[...]])</span>

<span class="s2">            &gt;&gt;&gt; module.to(torch.double)</span>

<span class="s2">            ExampleModule()</span>

<span class="s2">            &gt;&gt;&gt; module.weight #doctest: +ELLIPSIS</span>

<span class="s2">            tensor([[...]], dtype=torch.float64)</span>

<span class="s2">            &gt;&gt;&gt; cpu = torch.device(&#39;cpu&#39;)</span>

<span class="s2">            &gt;&gt;&gt; module.to(cpu, dtype=torch.half, non_blocking=True)</span>

<span class="s2">            ExampleModule()</span>

<span class="s2">            &gt;&gt;&gt; module.weight #doctest: +ELLIPSIS</span>

<span class="s2">            tensor([[...]], dtype=torch.float16)</span>

<span class="s2">            &gt;&gt;&gt; module.to(cpu)</span>

<span class="s2">            ExampleModule()</span>

<span class="s2">            &gt;&gt;&gt; module.weight #doctest: +ELLIPSIS</span>

<span class="s2">            tensor([[...]], dtype=torch.float16)</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="c1"># there is diff nb vars in PT 1.5</span>

        <span class="k">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">_C</span><span class="p">.</span><span class="n">_nn</span><span class="p">.</span><span class="n">_parse_to</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">device</span> <span class="o">=</span> <span class="k">out</span><span class="err">[</span><span class="mi">0</span><span class="err">]</span>

        <span class="n">dtype</span> <span class="o">=</span> <span class="k">out</span><span class="err">[</span><span class="mi">1</span><span class="err">]</span>

        <span class="k">if</span> <span class="n">device</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="o">:</span>

            <span class="n">self</span><span class="p">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>

        <span class="k">if</span> <span class="n">dtype</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="o">:</span>

            <span class="n">self</span><span class="p">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtype</span>

        <span class="k">return</span> <span class="k">super</span><span class="p">().</span><span class="k">to</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>

</details>
<h5 id="train">train</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">:</span><span class="o">~</span><span class="n">T</span><span class="p">,</span>
    <span class="n">mode</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="o">~</span><span class="n">T</span>
</code></pre></div>

<p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>,
etc.</p>
<p>Args:
    mode (bool): whether to set training mode (<code>True</code>) or evaluation
                 mode (<code>False</code>). Default: <code>True</code>.</p>
<p>Returns:
    Module: self</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">train</span><span class="p">(</span><span class="n">self</span><span class="o">:</span> <span class="n">T</span><span class="p">,</span> <span class="k">mode</span><span class="o">:</span> <span class="kt">bool</span> <span class="o">=</span> <span class="no">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="o">:</span>

        <span class="n">r</span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Sets the module in training mode.</span>

<span class="s2">        This has any effect only on certain modules. See documentations of</span>

<span class="s2">        particular modules for details of their behaviors in training/evaluation</span>

<span class="s2">        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,</span>

<span class="s2">        etc.</span>

<span class="s2">        Args:</span>

<span class="s2">            mode (bool): whether to set training mode (``True``) or evaluation</span>

<span class="s2">                         mode (``False``). Default: ``True``.</span>

<span class="s2">        Returns:</span>

<span class="s2">            Module: self</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="n">self</span><span class="p">.</span><span class="n">training</span> <span class="o">=</span> <span class="k">mode</span>

        <span class="k">for</span> <span class="n">module</span> <span class="k">in</span> <span class="n">self</span><span class="p">.</span><span class="n">children</span><span class="p">()</span><span class="o">:</span>

            <span class="n">module</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="k">mode</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">self</span>
</code></pre></div>

</details>
<h5 id="train_dataloader">train_dataloader</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>

            <span class="n">dataset</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">,</span>

            <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>

            <span class="n">collate_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">,</span>

            <span class="n">shuffle</span><span class="o">=</span><span class="n">True</span><span class="p">,</span>

            <span class="n">num_workers</span><span class="o">=</span><span class="n">cpu_count</span><span class="p">(),</span>

        <span class="p">)</span>

        <span class="k">return</span> <span class="n">train_loader</span>
</code></pre></div>

</details>
<h5 id="training_end">training_end</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">training_end</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Warnings:
Deprecated in v0.7.0. Use  :meth:<code>training_step_end</code> instead.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">training_end</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">:</span>

        <span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2"></span>

<span class="s2">        Warnings:</span>

<span class="s2">            Deprecated in v0.7.0. Use  :meth:`training_step_end` instead.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>
</code></pre></div>

</details>
<h5 id="training_epoch_end">training_epoch_end</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">training_epoch_end</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">outputs</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
</code></pre></div>

<p>Called at the end of the training epoch with the outputs of all training steps.</p>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code><span class="err"># the pseudocode for these calls</span>
<span class="err">train_outs = []</span>
<span class="err">for train_batch in train_data:</span>
<span class="err">    out = training_step(train_batch)</span>
<span class="err">    train_outs.append(out)</span>
<span class="err">training_epoch_end(train_outs)</span>
</code></pre></div>

<p>Args:
    outputs: List of outputs you defined in :meth:<code>training_step</code>, or if there are
        multiple dataloaders, a list containing a list of outputs for each dataloader.</p>
<p>Return:
    Dict or OrderedDict.
    May contain the following optional keys:</p>
<div class="codehilite"><pre><span></span><code><span class="err">- log (metrics to be added to the logger; only tensors)</span>
<span class="err">- progress_bar (dict for progress bar display)</span>
<span class="err">- any metric used in a callback (e.g. early stopping).</span>
</code></pre></div>

<p>Note:
    If this method is not overridden, this won't be called.</p>
<ul>
<li>The outputs here are strictly for logging or progress bar.</li>
<li>If you don't need to display anything, don't return anything.</li>
<li>If you want to manually set current step, you can specify the 'step' key in the 'log' dict.</li>
</ul>
<p>Examples:
    With a single dataloader:</p>
<div class="codehilite"><pre><span></span><code><span class="p">..</span> <span class="k">code</span><span class="o">-</span><span class="k">block</span><span class="o">::</span> <span class="n">python</span>

    <span class="n">def</span> <span class="n">training_epoch_end</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span><span class="o">:</span>
        <span class="n">train_acc_mean</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">output</span> <span class="k">in</span> <span class="n">outputs</span><span class="o">:</span>
            <span class="n">train_acc_mean</span> <span class="o">+=</span> <span class="n">output</span><span class="err">[</span><span class="s1">&#39;train_acc&#39;</span><span class="err">]</span>

        <span class="n">train_acc_mean</span> <span class="o">/=</span> <span class="n">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>

        <span class="c1"># log training accuracy at the end of an epoch</span>
        <span class="n">results</span> <span class="o">=</span> <span class="err">{</span>
            <span class="s1">&#39;log&#39;</span><span class="o">:</span> <span class="err">{</span><span class="s1">&#39;train_acc&#39;</span><span class="o">:</span> <span class="n">train_acc_mean</span><span class="p">.</span><span class="n">item</span><span class="p">()</span><span class="err">}</span><span class="p">,</span>
            <span class="s1">&#39;progress_bar&#39;</span><span class="o">:</span> <span class="err">{</span><span class="s1">&#39;train_acc&#39;</span><span class="o">:</span> <span class="n">train_acc_mean</span><span class="err">}</span><span class="p">,</span>
        <span class="err">}</span>
        <span class="k">return</span> <span class="n">results</span>

<span class="k">With</span> <span class="n">multiple</span> <span class="n">dataloaders</span><span class="p">,</span> <span class="n">``outputs``</span> <span class="n">will</span> <span class="n">be</span> <span class="n">a</span> <span class="k">list</span> <span class="k">of</span> <span class="n">lists</span><span class="p">.</span> <span class="n">The</span> <span class="k">outer</span> <span class="k">list</span> <span class="k">contains</span>
<span class="k">one</span> <span class="n">entry</span> <span class="n">per</span> <span class="n">dataloader</span><span class="p">,</span> <span class="k">while</span> <span class="n">the</span> <span class="k">inner</span> <span class="k">list</span> <span class="k">contains</span> <span class="n">the</span> <span class="n">individual</span> <span class="n">outputs</span> <span class="k">of</span>
<span class="k">each</span> <span class="n">training</span> <span class="n">step</span> <span class="k">for</span> <span class="n">that</span> <span class="n">dataloader</span><span class="p">.</span>

<span class="p">..</span> <span class="k">code</span><span class="o">-</span><span class="k">block</span><span class="o">::</span> <span class="n">python</span>

    <span class="n">def</span> <span class="n">training_epoch_end</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span><span class="o">:</span>
        <span class="n">train_acc_mean</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">dataloader_outputs</span> <span class="k">in</span> <span class="n">outputs</span><span class="o">:</span>
            <span class="k">for</span> <span class="n">output</span> <span class="k">in</span> <span class="n">dataloader_outputs</span><span class="o">:</span>
                <span class="n">train_acc_mean</span> <span class="o">+=</span> <span class="n">output</span><span class="err">[</span><span class="s1">&#39;train_acc&#39;</span><span class="err">]</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">train_acc_mean</span> <span class="o">/=</span> <span class="n">i</span>

        <span class="c1"># log training accuracy at the end of an epoch</span>
        <span class="n">results</span> <span class="o">=</span> <span class="err">{</span>
            <span class="s1">&#39;log&#39;</span><span class="o">:</span> <span class="err">{</span><span class="s1">&#39;train_acc&#39;</span><span class="o">:</span> <span class="n">train_acc_mean</span><span class="p">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">&#39;step&#39;</span><span class="o">:</span> <span class="n">self</span><span class="p">.</span><span class="n">current_epoch</span><span class="err">}</span>
            <span class="s1">&#39;progress_bar&#39;</span><span class="o">:</span> <span class="err">{</span><span class="s1">&#39;train_acc&#39;</span><span class="o">:</span> <span class="n">train_acc_mean</span><span class="err">}</span><span class="p">,</span>
        <span class="err">}</span>
        <span class="k">return</span> <span class="n">results</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">training_epoch_end</span><span class="p">(</span>

            <span class="n">self</span><span class="p">,</span>

            <span class="n">outputs</span><span class="o">:</span> <span class="k">Union</span><span class="err">[</span><span class="k">List</span><span class="err">[</span><span class="n">Dict</span><span class="err">[</span><span class="n">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="err">]]</span><span class="p">,</span> <span class="k">List</span><span class="err">[</span><span class="k">List</span><span class="err">[</span><span class="n">Dict</span><span class="err">[</span><span class="n">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="err">]]]]</span>

    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="err">[</span><span class="n">str</span><span class="p">,</span> <span class="n">Dict</span><span class="err">[</span><span class="n">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="err">]]</span><span class="o">:</span>

        <span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Called at the end of the training epoch with the outputs of all training steps.</span>

<span class="s2">        .. code-block:: python</span>

<span class="s2">            # the pseudocode for these calls</span>

<span class="s2">            train_outs = []</span>

<span class="s2">            for train_batch in train_data:</span>

<span class="s2">                out = training_step(train_batch)</span>

<span class="s2">                train_outs.append(out)</span>

<span class="s2">            training_epoch_end(train_outs)</span>

<span class="s2">        Args:</span>

<span class="s2">            outputs: List of outputs you defined in :meth:`training_step`, or if there are</span>

<span class="s2">                multiple dataloaders, a list containing a list of outputs for each dataloader.</span>

<span class="s2">        Return:</span>

<span class="s2">            Dict or OrderedDict.</span>

<span class="s2">            May contain the following optional keys:</span>

<span class="s2">            - log (metrics to be added to the logger; only tensors)</span>

<span class="s2">            - progress_bar (dict for progress bar display)</span>

<span class="s2">            - any metric used in a callback (e.g. early stopping).</span>

<span class="s2">        Note:</span>

<span class="s2">            If this method is not overridden, this won&#39;t be called.</span>

<span class="s2">        - The outputs here are strictly for logging or progress bar.</span>

<span class="s2">        - If you don&#39;t need to display anything, don&#39;t return anything.</span>

<span class="s2">        - If you want to manually set current step, you can specify the &#39;step&#39; key in the &#39;log&#39; dict.</span>

<span class="s2">        Examples:</span>

<span class="s2">            With a single dataloader:</span>

<span class="s2">            .. code-block:: python</span>

<span class="s2">                def training_epoch_end(self, outputs):</span>

<span class="s2">                    train_acc_mean = 0</span>

<span class="s2">                    for output in outputs:</span>

<span class="s2">                        train_acc_mean += output[&#39;train_acc&#39;]</span>

<span class="s2">                    train_acc_mean /= len(outputs)</span>

<span class="s2">                    # log training accuracy at the end of an epoch</span>

<span class="s2">                    results = {</span>

<span class="s2">                        &#39;log&#39;: {&#39;train_acc&#39;: train_acc_mean.item()},</span>

<span class="s2">                        &#39;progress_bar&#39;: {&#39;train_acc&#39;: train_acc_mean},</span>

<span class="s2">                    }</span>

<span class="s2">                    return results</span>

<span class="s2">            With multiple dataloaders, ``outputs`` will be a list of lists. The outer list contains</span>

<span class="s2">            one entry per dataloader, while the inner list contains the individual outputs of</span>

<span class="s2">            each training step for that dataloader.</span>

<span class="s2">            .. code-block:: python</span>

<span class="s2">                def training_epoch_end(self, outputs):</span>

<span class="s2">                    train_acc_mean = 0</span>

<span class="s2">                    i = 0</span>

<span class="s2">                    for dataloader_outputs in outputs:</span>

<span class="s2">                        for output in dataloader_outputs:</span>

<span class="s2">                            train_acc_mean += output[&#39;train_acc&#39;]</span>

<span class="s2">                            i += 1</span>

<span class="s2">                    train_acc_mean /= i</span>

<span class="s2">                    # log training accuracy at the end of an epoch</span>

<span class="s2">                    results = {</span>

<span class="s2">                        &#39;log&#39;: {&#39;train_acc&#39;: train_acc_mean.item(), &#39;step&#39;: self.current_epoch}</span>

<span class="s2">                        &#39;progress_bar&#39;: {&#39;train_acc&#39;: train_acc_mean},</span>

<span class="s2">                    }</span>

<span class="s2">                    return results</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>
</code></pre></div>

</details>
<h5 id="training_step">training_step</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">batch</span><span class="p">,</span>
    <span class="n">batch_idx</span>
<span class="p">)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">training_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span><span class="o">:</span>

        <span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="nl">train_rpn:</span>

            <span class="n">targets</span> <span class="o">=</span> <span class="p">[{</span><span class="s">&quot;boxes&quot;</span><span class="o">:</span> <span class="n">t</span><span class="p">[</span><span class="s">&quot;boxes&quot;</span><span class="p">]}</span> <span class="k">for</span> <span class="n">t</span> <span class="n">in</span> <span class="n">targets</span><span class="p">]</span>

            <span class="n">_</span><span class="p">,</span> <span class="n">loss_dict</span> <span class="o">=</span> <span class="n">self</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="n">sum</span><span class="p">(</span><span class="n">loss</span> <span class="k">for</span> <span class="n">loss</span> <span class="n">in</span> <span class="n">loss_dict</span><span class="p">.</span><span class="n">values</span><span class="p">())</span>

            <span class="k">return</span> <span class="n">OrderedDict</span><span class="p">({</span><span class="s">&quot;loss&quot;</span><span class="o">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s">&quot;progress_bar&quot;</span><span class="o">:</span> <span class="n">loss_dict</span><span class="p">,</span> <span class="s">&quot;log&quot;</span><span class="o">:</span> <span class="n">loss_dict</span><span class="p">})</span>

        <span class="n">elif</span> <span class="n">self</span><span class="p">.</span><span class="nl">train_roi:</span>

            <span class="n">_</span><span class="p">,</span> <span class="n">loss_dict</span> <span class="o">=</span> <span class="n">self</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="n">sum</span><span class="p">(</span><span class="n">loss</span> <span class="k">for</span> <span class="n">loss</span> <span class="n">in</span> <span class="n">loss_dict</span><span class="p">.</span><span class="n">values</span><span class="p">())</span>

            <span class="k">return</span> <span class="n">OrderedDict</span><span class="p">({</span><span class="s">&quot;loss&quot;</span><span class="o">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s">&quot;progress_bar&quot;</span><span class="o">:</span> <span class="n">loss_dict</span><span class="p">,</span> <span class="s">&quot;log&quot;</span><span class="o">:</span> <span class="n">loss_dict</span><span class="p">})</span>

        <span class="k">else</span><span class="o">:</span>

            <span class="n">images</span> <span class="o">=</span> <span class="n">list</span><span class="p">(</span><span class="n">image</span> <span class="k">for</span> <span class="n">image</span> <span class="n">in</span> <span class="n">images</span><span class="p">)</span>

            <span class="n">targets</span> <span class="o">=</span> <span class="p">[{</span><span class="nl">k:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="n">in</span> <span class="n">t</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span> <span class="k">for</span> <span class="n">t</span> <span class="n">in</span> <span class="n">targets</span><span class="p">]</span>

            <span class="n">loss_dict</span> <span class="o">=</span> <span class="n">self</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>

            <span class="p">#</span> <span class="n">loss</span> <span class="nl">keys:</span> <span class="p">[&#39;</span><span class="n">loss_classifier</span><span class="p">&#39;,</span> <span class="p">&#39;</span><span class="n">loss_box_reg</span><span class="p">&#39;,</span> <span class="p">&#39;</span><span class="n">loss_objectness</span><span class="p">&#39;,</span> <span class="p">&#39;</span><span class="n">loss_rpn_box_reg</span><span class="p">&#39;]</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="n">sum</span><span class="p">(</span><span class="n">loss</span> <span class="k">for</span> <span class="n">loss</span> <span class="n">in</span> <span class="n">loss_dict</span><span class="p">.</span><span class="n">values</span><span class="p">())</span>

            <span class="k">if</span> <span class="k">not</span> <span class="n">math</span><span class="p">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">())</span><span class="o">:</span>

                <span class="n">sys</span><span class="p">.</span><span class="n">exit</span><span class="p">(</span><span class="mh">1</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">OrderedDict</span><span class="p">({</span><span class="s">&quot;loss&quot;</span><span class="o">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s">&quot;progress_bar&quot;</span><span class="o">:</span> <span class="n">loss_dict</span><span class="p">,</span> <span class="s">&quot;log&quot;</span><span class="o">:</span> <span class="n">loss_dict</span><span class="p">})</span>
</code></pre></div>

</details>
<h5 id="training_step_end">training_step_end</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">training_step_end</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span>
</code></pre></div>

<p>Use this when training with dp or ddp2 because :meth:<code>training_step</code>
will operate on only part of the batch. However, this is still optional
and only needed for things like softmax or NCE loss.</p>
<p>Note:
    If you later switch to ddp or some other mode, this will still be called
    so that you don't have to change your code</p>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code><span class="err"># pseudocode</span>
<span class="err">sub_batches = split_batches_for_dp(batch)</span>
<span class="err">batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches]</span>
<span class="err">training_step_end(batch_parts_outputs)</span>
</code></pre></div>

<p>Args:
    batch_parts_outputs: What you return in <code>training_step</code> for each batch part.</p>
<p>Return:
    Dict with loss key and optional log or progress bar keys.</p>
<div class="codehilite"><pre><span></span><code><span class="err">- loss -&gt; tensor scalar **REQUIRED**</span>
<span class="err">- progress_bar -&gt; Dict for progress bar display. Must have only tensors</span>
<span class="err">- log -&gt; Dict of metrics to add to logger. Must have only tensors (no images, etc)</span>
</code></pre></div>

<p>Examples:
    .. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>    <span class="o">#</span> <span class="k">WITHOUT</span> <span class="n">training_step_end</span>
    <span class="o">#</span> <span class="k">if</span> <span class="n">used</span> <span class="k">in</span> <span class="n">DP</span> <span class="k">or</span> <span class="n">DDP2</span><span class="p">,</span> <span class="n">this</span> <span class="n">batch</span> <span class="k">is</span> <span class="mi">1</span><span class="o">/</span><span class="n">num_gpus</span> <span class="k">large</span>
    <span class="n">def</span> <span class="n">training_step</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="o">#</span> <span class="n">batch</span> <span class="k">is</span> <span class="mi">1</span><span class="o">/</span><span class="n">num_gpus</span> <span class="n">big</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="k">out</span> <span class="o">=</span> <span class="k">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="k">out</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="err">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="err">}</span>

    <span class="o">#</span> <span class="c1">--------------</span>
    <span class="o">#</span> <span class="k">with</span> <span class="n">training_step_end</span> <span class="k">to</span> <span class="k">do</span> <span class="n">softmax</span> <span class="n">over</span> <span class="n">the</span> <span class="k">full</span> <span class="n">batch</span>
    <span class="n">def</span> <span class="n">training_step</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="o">#</span> <span class="n">batch</span> <span class="k">is</span> <span class="mi">1</span><span class="o">/</span><span class="n">num_gpus</span> <span class="n">big</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="k">out</span> <span class="o">=</span> <span class="k">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="err">{</span><span class="s1">&#39;out&#39;</span><span class="p">:</span> <span class="k">out</span><span class="err">}</span>

    <span class="n">def</span> <span class="n">training_step_end</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="o">#</span> <span class="n">this</span> <span class="k">out</span> <span class="k">is</span> <span class="n">now</span> <span class="n">the</span> <span class="k">full</span> <span class="k">size</span> <span class="k">of</span> <span class="n">the</span> <span class="n">batch</span>
        <span class="k">out</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;out&#39;</span><span class="p">]</span>

        <span class="o">#</span> <span class="n">this</span> <span class="n">softmax</span> <span class="n">now</span> <span class="n">uses</span> <span class="n">the</span> <span class="k">full</span> <span class="n">batch</span> <span class="k">size</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="err">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="err">}</span>
</code></pre></div>

<p>See Also:
    See the :ref:<code>multi-gpu-training</code> guide for more details.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">training_step_end</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="err">[</span>

        <span class="n">str</span><span class="p">,</span> <span class="k">Union</span><span class="err">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="err">[</span><span class="n">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="err">]]</span>

    <span class="err">]</span><span class="o">:</span>

        <span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2"></span>

<span class="s2">        Use this when training with dp or ddp2 because :meth:`training_step`</span>

<span class="s2">        will operate on only part of the batch. However, this is still optional</span>

<span class="s2">        and only needed for things like softmax or NCE loss.</span>

<span class="s2">        Note:</span>

<span class="s2">            If you later switch to ddp or some other mode, this will still be called</span>

<span class="s2">            so that you don&#39;t have to change your code</span>

<span class="s2">        .. code-block:: python</span>

<span class="s2">            # pseudocode</span>

<span class="s2">            sub_batches = split_batches_for_dp(batch)</span>

<span class="s2">            batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches]</span>

<span class="s2">            training_step_end(batch_parts_outputs)</span>

<span class="s2">        Args:</span>

<span class="s2">            batch_parts_outputs: What you return in `training_step` for each batch part.</span>

<span class="s2">        Return:</span>

<span class="s2">            Dict with loss key and optional log or progress bar keys.</span>

<span class="s2">            - loss -&gt; tensor scalar **REQUIRED**</span>

<span class="s2">            - progress_bar -&gt; Dict for progress bar display. Must have only tensors</span>

<span class="s2">            - log -&gt; Dict of metrics to add to logger. Must have only tensors (no images, etc)</span>

<span class="s2">        Examples:</span>

<span class="s2">            .. code-block:: python</span>

<span class="s2">                # WITHOUT training_step_end</span>

<span class="s2">                # if used in DP or DDP2, this batch is 1/num_gpus large</span>

<span class="s2">                def training_step(self, batch, batch_idx):</span>

<span class="s2">                    # batch is 1/num_gpus big</span>

<span class="s2">                    x, y = batch</span>

<span class="s2">                    out = self(x)</span>

<span class="s2">                    loss = self.softmax(out)</span>

<span class="s2">                    loss = nce_loss(loss)</span>

<span class="s2">                    return {&#39;loss&#39;: loss}</span>

<span class="s2">                # --------------</span>

<span class="s2">                # with training_step_end to do softmax over the full batch</span>

<span class="s2">                def training_step(self, batch, batch_idx):</span>

<span class="s2">                    # batch is 1/num_gpus big</span>

<span class="s2">                    x, y = batch</span>

<span class="s2">                    out = self(x)</span>

<span class="s2">                    return {&#39;out&#39;: out}</span>

<span class="s2">                def training_step_end(self, outputs):</span>

<span class="s2">                    # this out is now the full size of the batch</span>

<span class="s2">                    out = outputs[&#39;out&#39;]</span>

<span class="s2">                    # this softmax now uses the full batch size</span>

<span class="s2">                    loss = nce_loss(loss)</span>

<span class="s2">                    return {&#39;loss&#39;: loss}</span>

<span class="s2">        See Also:</span>

<span class="s2">            See the :ref:`multi-gpu-training` guide for more details.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>
</code></pre></div>

</details>
<h5 id="transfer_batch_to_device">transfer_batch_to_device</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">transfer_batch_to_device</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">batch</span><span class="p">:</span><span class="n">Any</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span>
</code></pre></div>

<p>Override this hook if your :class:<code>~torch.utils.data.DataLoader</code> returns tensors
wrapped in a custom data structure.</p>
<p>The data types listed below (and any arbitrary nesting of them) are supported out of the box:</p>
<ul>
<li>:class:<code>torch.Tensor</code> or anything that implements <code>.to(...)</code></li>
<li>:class:<code>list</code></li>
<li>:class:<code>dict</code></li>
<li>:class:<code>tuple</code></li>
<li>:class:<code>torchtext.data.batch.Batch</code></li>
</ul>
<p>For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...).</p>
<p>Example::</p>
<div class="codehilite"><pre><span></span><code><span class="err">def transfer_batch_to_device(self, batch, device)</span>
<span class="err">    if isinstance(batch, CustomBatch):</span>
<span class="err">        # move all tensors in your custom data structure to the device</span>
<span class="err">        batch.samples = batch.samples.to(device)</span>
<span class="err">        batch.targets = batch.targets.to(device)</span>
<span class="err">    else:</span>
<span class="err">        batch = super().transfer_batch_to_device(data, device)</span>
<span class="err">    return batch</span>
</code></pre></div>

<p>Args:
    batch: A batch of data that needs to be transferred to a new device.
    device: The target device as defined in PyTorch.</p>
<p>Returns:
    A reference to the data on the new device.</p>
<p>Note:
    This hook should only transfer the data and not modify it, nor should it move the data to
    any other device than the one passed in as argument (unless you know what you are doing).
    The :class:<code>~pytorch_lightning.trainer.trainer.Trainer</code> already takes care of splitting the
    batch and determines the target devices.</p>
<p>See Also:
    - :func:<code>~pytorch_lightning.utilities.apply_func.move_data_to_device</code>
    - :func:<code>~pytorch_lightning.utilities.apply_func.apply_to_collection</code></p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">transfer_batch_to_device</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch</span><span class="o">:</span> <span class="k">Any</span><span class="p">,</span> <span class="n">device</span><span class="o">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">Any</span><span class="o">:</span>

        <span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2"></span>

<span class="s2">        Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors</span>

<span class="s2">        wrapped in a custom data structure.</span>

<span class="s2">        The data types listed below (and any arbitrary nesting of them) are supported out of the box:</span>

<span class="s2">        - :class:`torch.Tensor` or anything that implements `.to(...)`</span>

<span class="s2">        - :class:`list`</span>

<span class="s2">        - :class:`dict`</span>

<span class="s2">        - :class:`tuple`</span>

<span class="s2">        - :class:`torchtext.data.batch.Batch`</span>

<span class="s2">        For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...).</span>

<span class="s2">        Example::</span>

<span class="s2">            def transfer_batch_to_device(self, batch, device)</span>

<span class="s2">                if isinstance(batch, CustomBatch):</span>

<span class="s2">                    # move all tensors in your custom data structure to the device</span>

<span class="s2">                    batch.samples = batch.samples.to(device)</span>

<span class="s2">                    batch.targets = batch.targets.to(device)</span>

<span class="s2">                else:</span>

<span class="s2">                    batch = super().transfer_batch_to_device(data, device)</span>

<span class="s2">                return batch</span>

<span class="s2">        Args:</span>

<span class="s2">            batch: A batch of data that needs to be transferred to a new device.</span>

<span class="s2">            device: The target device as defined in PyTorch.</span>

<span class="s2">        Returns:</span>

<span class="s2">            A reference to the data on the new device.</span>

<span class="s2">        Note:</span>

<span class="s2">            This hook should only transfer the data and not modify it, nor should it move the data to</span>

<span class="s2">            any other device than the one passed in as argument (unless you know what you are doing).</span>

<span class="s2">            The :class:`~pytorch_lightning.trainer.trainer.Trainer` already takes care of splitting the</span>

<span class="s2">            batch and determines the target devices.</span>

<span class="s2">        See Also:</span>

<span class="s2">            - :func:`~pytorch_lightning.utilities.apply_func.move_data_to_device`</span>

<span class="s2">            - :func:`~pytorch_lightning.utilities.apply_func.apply_to_collection`</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="k">return</span> <span class="n">move_data_to_device</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</code></pre></div>

</details>
<h5 id="type">type</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">type</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">dst_type</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span>
</code></pre></div>

<p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p>
<p>Arguments:
    dst_type (type or string): the desired type</p>
<p>Returns:
    Module: self</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="k">type</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dst_type</span><span class="o">:</span> <span class="k">Union</span><span class="err">[</span><span class="n">str</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">dtype</span><span class="err">]</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Module</span><span class="o">:</span>

        <span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Casts all parameters and buffers to :attr:`dst_type`.</span>

<span class="s2">        Arguments:</span>

<span class="s2">            dst_type (type or string): the desired type</span>

<span class="s2">        Returns:</span>

<span class="s2">            Module: self</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>

        <span class="n">self</span><span class="p">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dst_type</span>

        <span class="k">return</span> <span class="k">super</span><span class="p">().</span><span class="k">type</span><span class="p">(</span><span class="n">dst_type</span><span class="o">=</span><span class="n">dst_type</span><span class="p">)</span>
</code></pre></div>

</details>
<h5 id="unfreeze">unfreeze</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">unfreeze</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Unfreeze all parameters for training.</p>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code><span class="err">model = MyLightningModule(...)</span>
<span class="err">model.unfreeze()</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">unfreeze</span><span class="p">(</span><span class="k">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">None</span><span class="p">:</span>

        <span class="ss">&quot;&quot;&quot;</span>

<span class="ss">        Unfreeze all parameters for training.</span>

<span class="ss">        .. code-block:: python</span>

<span class="ss">            model = MyLightningModule(...)</span>

<span class="ss">            model.unfreeze()</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="k">for</span> <span class="n">param</span> <span class="k">in</span> <span class="k">self</span><span class="p">.</span><span class="k">parameters</span><span class="p">():</span>

            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="k">True</span>

        <span class="k">self</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div>

</details>
<h5 id="val_dataloader">val_dataloader</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="n">val_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">val_dataset</span><span class="p">,</span>

            <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>

            <span class="n">collate_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">val_dataset</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">,</span>

            <span class="n">shuffle</span><span class="o">=</span><span class="n">False</span><span class="p">,</span>

            <span class="n">num_workers</span><span class="o">=</span><span class="n">cpu_count</span><span class="p">()</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span>

        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">coco_evaluator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_evaluator</span><span class="p">(</span><span class="n">val_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">val_loader</span>
</code></pre></div>

</details>
<h5 id="validation_end">validation_end</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">validation_end</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">outputs</span>
<span class="p">)</span>
</code></pre></div>

<p>Warnings:
Deprecated in v0.7.0. Use :meth:<code>validation_epoch_end</code> instead.
Will be removed in 1.0.0.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">validation_end</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span><span class="o">:</span>

        <span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2"></span>

<span class="s2">        Warnings:</span>

<span class="s2">            Deprecated in v0.7.0. Use :meth:`validation_epoch_end` instead.</span>

<span class="s2">            Will be removed in 1.0.0.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>
</code></pre></div>

</details>
<h5 id="validation_epoch_end">validation_epoch_end</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">outputs</span>
<span class="p">)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@auto_move_data</span><span class="w"></span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">outputs</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="nl">train_rpn</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="err">{}</span><span class="w"></span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="nl">train_roi</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="nl">TODO</span><span class="p">:</span><span class="w"> </span><span class="n">above</span><span class="w"></span>

<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="err">{}</span><span class="w"></span>

<span class="w">        </span><span class="k">else</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">self</span><span class="p">.</span><span class="n">coco_evaluator</span><span class="p">.</span><span class="n">synchronize_between_processes</span><span class="p">()</span><span class="w"></span>

<span class="w">            </span><span class="n">self</span><span class="p">.</span><span class="n">coco_evaluator</span><span class="p">.</span><span class="n">accumulate</span><span class="p">()</span><span class="w"></span>

<span class="w">            </span><span class="n">self</span><span class="p">.</span><span class="n">coco_evaluator</span><span class="p">.</span><span class="n">summarize</span><span class="p">()</span><span class="w"></span>

<span class="w">            </span><span class="n">metric</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">coco_evaluator</span><span class="p">.</span><span class="n">coco_eval</span><span class="o">[</span><span class="n">&quot;bbox&quot;</span><span class="o">]</span><span class="p">.</span><span class="n">stats</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="w"></span>

<span class="w">            </span><span class="n">metric</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">metric</span><span class="p">)</span><span class="w"></span>

<span class="w">            </span><span class="n">tensorboard_logs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">{</span><span class="ss">&quot;main_score&quot;</span><span class="err">:</span><span class="w"> </span><span class="n">metric</span><span class="err">}</span><span class="w"></span>

<span class="w">            </span><span class="n">self</span><span class="p">.</span><span class="n">coco_evaluator</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_evaluator</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">val_dataset</span><span class="p">)</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">need</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">update</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">evaluation</span><span class="w"></span>

<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="err">{</span><span class="ss">&quot;val_loss&quot;</span><span class="err">:</span><span class="w"> </span><span class="n">metric</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;log&quot;</span><span class="err">:</span><span class="w"> </span><span class="n">tensorboard_logs</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;progress_bar&quot;</span><span class="err">:</span><span class="w"> </span><span class="n">tensorboard_logs</span><span class="err">}</span><span class="w"></span>
</code></pre></div>

</details>
<h5 id="validation_step">validation_step</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">batch</span><span class="p">,</span>
    <span class="n">batch_idx</span>
<span class="p">)</span>
</code></pre></div>

<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@auto_move_data</span><span class="w"></span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">validation_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="n">batch_idx</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="n">images</span><span class="p">,</span><span class="w"> </span><span class="n">targets</span><span class="p">,</span><span class="w"> </span><span class="n">_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">batch</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="nl">train_rpn</span><span class="p">:</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">rpn</span><span class="w"> </span><span class="n">doesn</span><span class="err">&#39;</span><span class="n">t</span><span class="w"> </span><span class="k">compute</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">val</span><span class="w"></span>

<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="err">{}</span><span class="w"></span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="nl">train_roi</span><span class="p">:</span><span class="w"></span>

<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="nl">TODO</span><span class="p">:</span><span class="w"> </span><span class="n">scores</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">predictions</span><span class="w"> </span><span class="n">scores</span><span class="p">,</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">metric</span><span class="err">!</span><span class="w"> </span><span class="n">iou</span><span class="vm">?</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">acc</span><span class="vm">?</span><span class="w"></span>

<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="err">{}</span><span class="w"></span>

<span class="w">        </span><span class="k">else</span><span class="err">:</span><span class="w"></span>

<span class="w">            </span><span class="n">images</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">list</span><span class="p">(</span><span class="nc">image</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nc">image</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">images</span><span class="p">)</span><span class="w"></span>

<span class="w">            </span><span class="n">targets</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">{k: v for k, v in t.items()} for t in targets</span><span class="o">]</span><span class="w"></span>

<span class="w">            </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">(</span><span class="n">images</span><span class="p">,</span><span class="w"> </span><span class="n">targets</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span><span class="w"></span>

<span class="w">            </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">{</span><span class="n">target</span><span class="o">[</span><span class="n">&quot;image_id&quot;</span><span class="o">]</span><span class="p">.</span><span class="n">item</span><span class="p">()</span><span class="err">:</span><span class="w"> </span><span class="k">output</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">target</span><span class="p">,</span><span class="w"> </span><span class="k">output</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">zip</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span><span class="w"> </span><span class="n">outputs</span><span class="p">)</span><span class="err">}</span><span class="w"></span>

<span class="w">            </span><span class="n">self</span><span class="p">.</span><span class="n">coco_evaluator</span><span class="p">.</span><span class="k">update</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span><span class="w"></span>

<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="err">{}</span><span class="w"></span>
</code></pre></div>

</details>
<h5 id="validation_step_end">validation_step_end</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">validation_step_end</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
</code></pre></div>

<p>Use this when validating with dp or ddp2 because :meth:<code>validation_step</code>
will operate on only part of the batch. However, this is still optional
and only needed for things like softmax or NCE loss.</p>
<p>Note:
    If you later switch to ddp or some other mode, this will still be called
    so that you don't have to change your code.</p>
<p>.. code-block:: python</p>
<div class="codehilite"><pre><span></span><code><span class="err"># pseudocode</span>
<span class="err">sub_batches = split_batches_for_dp(batch)</span>
<span class="err">batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches]</span>
<span class="err">validation_step_end(batch_parts_outputs)</span>
</code></pre></div>

<p>Args:
    batch_parts_outputs: What you return in :meth:<code>validation_step</code>
        for each batch part.</p>
<p>Return:
   Dict or OrderedDict - passed to the :meth:<code>validation_epoch_end</code> method.</p>
<p>Examples:
    .. code-block:: python</p>
<div class="codehilite"><pre><span></span><code>    <span class="o">#</span> <span class="k">WITHOUT</span> <span class="n">validation_step_end</span>
    <span class="o">#</span> <span class="k">if</span> <span class="n">used</span> <span class="k">in</span> <span class="n">DP</span> <span class="k">or</span> <span class="n">DDP2</span><span class="p">,</span> <span class="n">this</span> <span class="n">batch</span> <span class="k">is</span> <span class="mi">1</span><span class="o">/</span><span class="n">num_gpus</span> <span class="k">large</span>
    <span class="n">def</span> <span class="n">validation_step</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="o">#</span> <span class="n">batch</span> <span class="k">is</span> <span class="mi">1</span><span class="o">/</span><span class="n">num_gpus</span> <span class="n">big</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="k">out</span> <span class="o">=</span> <span class="k">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="k">out</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="err">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="err">}</span>

    <span class="o">#</span> <span class="c1">--------------</span>
    <span class="o">#</span> <span class="k">with</span> <span class="n">validation_step_end</span> <span class="k">to</span> <span class="k">do</span> <span class="n">softmax</span> <span class="n">over</span> <span class="n">the</span> <span class="k">full</span> <span class="n">batch</span>
    <span class="n">def</span> <span class="n">validation_step</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="o">#</span> <span class="n">batch</span> <span class="k">is</span> <span class="mi">1</span><span class="o">/</span><span class="n">num_gpus</span> <span class="n">big</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="k">out</span> <span class="o">=</span> <span class="k">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="err">{</span><span class="s1">&#39;out&#39;</span><span class="p">:</span> <span class="k">out</span><span class="err">}</span>

    <span class="n">def</span> <span class="n">validation_epoch_end</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="o">#</span> <span class="n">this</span> <span class="k">out</span> <span class="k">is</span> <span class="n">now</span> <span class="n">the</span> <span class="k">full</span> <span class="k">size</span> <span class="k">of</span> <span class="n">the</span> <span class="n">batch</span>
        <span class="k">out</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;out&#39;</span><span class="p">]</span>

        <span class="o">#</span> <span class="n">this</span> <span class="n">softmax</span> <span class="n">now</span> <span class="n">uses</span> <span class="n">the</span> <span class="k">full</span> <span class="n">batch</span> <span class="k">size</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="err">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="err">}</span>
</code></pre></div>

<p>See Also:
    See the :ref:<code>multi-gpu-training</code> guide for more details.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">validation_step_end</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="err">[</span><span class="n">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="err">]</span><span class="o">:</span>

        <span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2"></span>

<span class="s2">        Use this when validating with dp or ddp2 because :meth:`validation_step`</span>

<span class="s2">        will operate on only part of the batch. However, this is still optional</span>

<span class="s2">        and only needed for things like softmax or NCE loss.</span>

<span class="s2">        Note:</span>

<span class="s2">            If you later switch to ddp or some other mode, this will still be called</span>

<span class="s2">            so that you don&#39;t have to change your code.</span>

<span class="s2">        .. code-block:: python</span>

<span class="s2">            # pseudocode</span>

<span class="s2">            sub_batches = split_batches_for_dp(batch)</span>

<span class="s2">            batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches]</span>

<span class="s2">            validation_step_end(batch_parts_outputs)</span>

<span class="s2">        Args:</span>

<span class="s2">            batch_parts_outputs: What you return in :meth:`validation_step`</span>

<span class="s2">                for each batch part.</span>

<span class="s2">        Return:</span>

<span class="s2">           Dict or OrderedDict - passed to the :meth:`validation_epoch_end` method.</span>

<span class="s2">        Examples:</span>

<span class="s2">            .. code-block:: python</span>

<span class="s2">                # WITHOUT validation_step_end</span>

<span class="s2">                # if used in DP or DDP2, this batch is 1/num_gpus large</span>

<span class="s2">                def validation_step(self, batch, batch_idx):</span>

<span class="s2">                    # batch is 1/num_gpus big</span>

<span class="s2">                    x, y = batch</span>

<span class="s2">                    out = self(x)</span>

<span class="s2">                    loss = self.softmax(out)</span>

<span class="s2">                    loss = nce_loss(loss)</span>

<span class="s2">                    return {&#39;loss&#39;: loss}</span>

<span class="s2">                # --------------</span>

<span class="s2">                # with validation_step_end to do softmax over the full batch</span>

<span class="s2">                def validation_step(self, batch, batch_idx):</span>

<span class="s2">                    # batch is 1/num_gpus big</span>

<span class="s2">                    x, y = batch</span>

<span class="s2">                    out = self(x)</span>

<span class="s2">                    return {&#39;out&#39;: out}</span>

<span class="s2">                def validation_epoch_end(self, outputs):</span>

<span class="s2">                    # this out is now the full size of the batch</span>

<span class="s2">                    out = outputs[&#39;out&#39;]</span>

<span class="s2">                    # this softmax now uses the full batch size</span>

<span class="s2">                    loss = nce_loss(loss)</span>

<span class="s2">                    return {&#39;loss&#39;: loss}</span>

<span class="s2">        See Also:</span>

<span class="s2">            See the :ref:`multi-gpu-training` guide for more details.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span>
</code></pre></div>

</details>
<h5 id="zero_grad">zero_grad</h5>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

<p>Sets gradients of all model parameters to zero.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><code>    <span class="n">def</span> <span class="n">zero_grad</span><span class="p">(</span><span class="k">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">None</span><span class="p">:</span>

        <span class="n">r</span><span class="ss">&quot;&quot;&quot;Sets gradients of all model parameters to zero.&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">getattr</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="s1">&#39;_is_replica&#39;</span><span class="p">,</span> <span class="k">False</span><span class="p">):</span>

            <span class="n">warnings</span><span class="p">.</span><span class="n">warn</span><span class="p">(</span>

                <span class="ss">&quot;Calling .zero_grad() from a module created with nn.DataParallel() has no effect. &quot;</span>

                <span class="ss">&quot;The parameters are copied (in a differentiable manner) from the original module. &quot;</span>

                <span class="ss">&quot;This means they are not leaf nodes in autograd and so don&#39;t accumulate gradients. &quot;</span>

                <span class="ss">&quot;If you need gradients in your forward method, consider using autograd.grad instead.&quot;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">p</span> <span class="k">in</span> <span class="k">self</span><span class="p">.</span><span class="k">parameters</span><span class="p">():</span>

            <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="p">:</span>

                <span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">detach_</span><span class="p">()</span>

                <span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>
</code></pre></div>

</details>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../classifier/" title="Classifier" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Classifier
              </span>
            </div>
          </a>
        
        
          <a href="../" title="Index" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Index
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Powered by
        <a href="http://timothycrosley.github.io/portray">portray.</a>
        You too can
        <a href="http://timothycrosley.github.io/portray">
          portray</a>
        your Python project well using automatic documentation.
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../assets/javascripts/vendor.2d1db4bd.min.js"></script>
      <script src="../../../assets/javascripts/bundle.6627ddf3.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents"}</script>
      
      <script>
        app = initialize({
          base: "../../..",
          features: [],
          search: Object.assign({
            worker: "../../../assets/javascripts/worker/search.5eca75d3.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
    
  </body>
</html>